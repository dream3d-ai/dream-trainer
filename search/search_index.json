{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dream Trainer Documentation","text":"<p>Dream Trainer is a powerful, distributed training framework built exclusively around PyTorch's new DTensor abstractions. It provides a flexible, composable approach that makes it easy to adopt the latest PyTorch DTensor APIs.</p> <p>Dream Trainer was created to address these core issues:</p> <ul> <li>Boilerplate Overload: Each parallelism scheme (DDP, FSDP, tensor, pipeline, etc.) requires its own verbose, error-prone setup &amp; configuration that must be applied in the correct order.</li> <li>Legacy Trainer Limitations: Most trainers are tightly coupled to old DDP/FSDP APIs and \"zero-config\" abstractions, making debugging harder and preventing them from taking advantage of new DTensor-based distributed patterns. Being DTensor-native makes code simpler and easier to debug.</li> <li>Complexity in Real Workflows: Even simple training scripts become unwieldy when mixing advanced parallelism, due to scattered configuration and framework assumptions.</li> </ul>"},{"location":"#design-principles","title":"\ud83c\udfd7\ufe0f Design Principles","text":"<p>Dream Trainer is built on three core principles:</p> <ol> <li> <p>Native PyTorch First</p> </li> <li> <p>Designed exclusively around PyTorch's new DTensor abstractions for simple but powerful parallelism</p> </li> <li> <p>Direct integration with PyTorch's ecosystem (torchao, torchft, DCP, torchrun)</p> </li> <li> <p>Minimal Assumptions</p> </li> <li> <p>Let users make their own choices</p> </li> <li>No automatic model wrapping or hidden behaviors</li> <li> <p>Assume users know what they're doing with advanced parallelism</p> </li> <li> <p>Composable Architecture</p> </li> <li>Trainer is a composition of mixins</li> <li>Take what you need, drop the rest</li> <li>Write your own components when needed</li> <li>Callback system for drop-in modifications to the loop</li> </ol>"},{"location":"#key-features","title":"\ud83c\udf1f Key Features","text":""},{"location":"#parallelism-support","title":"Parallelism Support","text":"<p>Dream Trainer provides simple configuration for all PyTorch parallelism schemes:</p> <ul> <li>Data Parallelism: Basic multi-GPU training with PyTorch's <code>replicate()</code> API</li> <li>FSDP2: Second-generation Fully Sharded Data Parallel built on DTensor</li> <li>Tensor Parallelism (TP): Parameter-wise sharding via DTensor layouts; composable with FSDP2 for HSDP</li> <li>Context Parallelism (CP): Sequence parallelism for extremely long contexts</li> <li>Pipeline Parallelism (PP): Layer pipelining across GPUs / nodes with automatic schedule search</li> </ul> <p>Unlike monolithic frameworks, Dream Trainer uses mixins to let you pick exactly what you need:</p> <pre><code># Minimal trainer for research\nclass SimpleTrainer(BaseTrainer, SetupMixin):\n    def training_step(self, batch, batch_idx):\n        loss = self.model(batch)\n        self.backward(loss)\n        return {\"loss\": loss}\n\n# Production trainer with all the bells and whistles\nclass ProductionTrainer(BaseTrainer, SetupMixin, EvalMetricMixin, \n                       WandBLoggerMixin, QuantizeMixin):\n    # Same training_step, but now with metrics, logging, and quantization!\n</code></pre>"},{"location":"#other-features-via-callbakcs","title":"Other Features via Callbakcs","text":"<ul> <li>Checkpointing DCP-based checkpointing with async checkpoint support</li> <li>Built-in Fault Tolerance via torchft</li> <li>Native FP8 Quantization via torchao</li> <li>Custom Callbacks for extensibility</li> <li>Build-your-own-trainer by composing mixin primitives</li> </ul>"},{"location":"#why-dream-trainer-vs-other-frameworks","title":"\ud83e\udd14 Why Dream Trainer vs. Other Frameworks?","text":"<p>While PyTorch Lightning, Accelerate and DeepSpeed simplify distributed training, they revolve around classic DDP/FSDP wrappers and hide key details behind heavyweight base classes. Dream Trainer takes a different path:</p> <ul> <li>DTensor-native from day one\u2014every parameter is a <code>DTensor</code>, so new sharding layouts appear the moment they land in PyTorch nightly.</li> <li>Parallel schemes (FSDP2, TP, PP, CP) are first-class, composable primitives, not bolt-on \"plugins\".</li> <li>Mix-and-match \u2013 DreamTrainer is designed around mixins to maximize composability.</li> <li>Minimal magic \u2013 no metaclasses, no <code>LightningModule</code>; your model remains a plain <code>nn.Module</code>.</li> </ul>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":""},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>Getting Started - Installation and basic usage</li> <li>Configuration - Detailed configuration options</li> <li>Trainer Guide - Creating custom trainers</li> <li>Callbacks - Extending functionality with callbacks</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":"<ul> <li>Distributed Training - Multi-GPU and multi-node training</li> <li>Mixed Precision - FP16, BF16, and FP8 training</li> <li>Checkpointing - Model saving and loading</li> <li>Logging - Metrics and experiment tracking</li> </ul>"},{"location":"#examples-tutorials","title":"Examples &amp; Tutorials","text":"<ul> <li>Basic Examples - Simple training examples</li> <li>Advanced Examples - Complex use cases</li> <li>Best Practices - Training optimization tips</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Trainer API - Core trainer classes</li> <li>Config API - Configuration classes</li> <li>Callback API - Built-in callbacks</li> <li>Utils API - Utility functions</li> </ul>"},{"location":"#requirements","title":"\ud83d\udd27 Requirements","text":"<ul> <li>Python &gt;= 3.10</li> <li>PyTorch &gt;= 2.7.0</li> <li>CUDA-capable GPU (recommended)</li> </ul>"},{"location":"#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<ul> <li>Follow the Getting Started guide to install and set up Dream Trainer</li> <li>Check out the Examples for complete working code</li> <li>Read the Trainer Guide to create your own custom trainer</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api-reference-example/","title":"Example API Reference: BaseTrainer","text":"<p>This is an example of what the API reference documentation should look like for Dream Trainer. This example shows the documentation for <code>BaseTrainer</code>.</p>"},{"location":"api-reference-example/#dream_trainertrainerbasetrainer","title":"dream_trainer.trainer.BaseTrainer","text":"<pre><code>class dream_trainer.trainer.BaseTrainer(config: BaseTrainerConfig, *args, **kwargs)\n</code></pre> <p>An implementation of a basic training loop with support for gradient accumulation, validation, callbacks, and distributed training.</p> <p><code>BaseTrainer</code> provides the core training loop functionality and serves as the foundation for all trainers in Dream Trainer. It handles the training lifecycle, manages callbacks, and provides hooks for customization through abstract methods.</p>"},{"location":"api-reference-example/#parameters","title":"Parameters","text":"<ul> <li>config (BaseTrainerConfig) \u2013 Configuration object containing training parameters and callbacks</li> <li>***args** \u2013 Additional positional arguments passed to parent class</li> <li>****kwargs** \u2013 Additional keyword arguments passed to parent class</li> </ul>"},{"location":"api-reference-example/#attributes","title":"Attributes","text":"Attribute Type Description <code>config</code> <code>BaseTrainerConfig</code> Configuration object for the trainer <code>callbacks</code> <code>CallbackCollection</code> Collection of callbacks to execute during training <code>training</code> <code>bool</code> Whether the trainer is currently in training mode <code>global_step</code> <code>int</code> Number of optimizer steps taken across all epochs <code>local_batches</code> <code>int</code> Number of batches processed since program start <code>current_epoch</code> <code>int</code> Current epoch number (0-indexed)"},{"location":"api-reference-example/#abstract-methods","title":"Abstract Methods","text":"<p>These methods must be implemented by subclasses:</p>"},{"location":"api-reference-example/#training_step","title":"training_step","text":"<pre><code>abstract training_step(batch: dict[str, Any], batch_idx: int) \u2192 dict[str, Any]\n</code></pre> <p>Execute a single training step.</p> <p>This method should implement the forward pass, loss computation, and backward pass for a single batch of training data.</p> <p>Parameters: - batch (dict[str, Any]) \u2013 Dictionary containing the batch data, typically with keys like 'input', 'target', etc. - batch_idx (int) \u2013 Index of the current batch within the epoch</p> <p>Returns: - dict[str, Any] \u2013 Dictionary containing at minimum the computed loss and any other metrics or values to log</p> <p>Example: <pre><code>def training_step(self, batch, batch_idx):\n    inputs, targets = batch['inputs'], batch['targets']\n    outputs = self.model(inputs)\n    loss = F.cross_entropy(outputs, targets)\n\n    # Use self.backward() for proper gradient accumulation\n    self.backward(loss)\n\n    # Only step optimizer when not accumulating\n    if not self.is_accumulating_gradients:\n        grad_norm = self.step(self.model, self.optimizer)\n        return {\"loss\": loss, \"grad_norm\": grad_norm}\n\n    return {\"loss\": loss}\n</code></pre></p>"},{"location":"api-reference-example/#validation_step","title":"validation_step","text":"<pre><code>abstract validation_step(batch: dict[str, Any], batch_idx: int) \u2192 dict[str, Any]\n</code></pre> <p>Execute a single validation step.</p> <p>Similar to <code>training_step</code> but without gradient computation. Used for model evaluation during validation phases.</p> <p>Parameters: - batch (dict[str, Any]) \u2013 Dictionary containing the batch data - batch_idx (int) \u2013 Index of the current batch</p> <p>Returns: - dict[str, Any] \u2013 Dictionary containing validation metrics</p> <p>Example: <pre><code>def validation_step(self, batch, batch_idx):\n    inputs, targets = batch['inputs'], batch['targets']\n\n    with torch.no_grad():\n        outputs = self.model(inputs)\n        loss = F.cross_entropy(outputs, targets)\n        accuracy = (outputs.argmax(dim=1) == targets).float().mean()\n\n    return {\n        \"val_loss\": loss,\n        \"val_accuracy\": accuracy\n    }\n</code></pre></p>"},{"location":"api-reference-example/#core-methods","title":"Core Methods","text":""},{"location":"api-reference-example/#fit","title":"fit","text":"<pre><code>fit() \u2192 None\n</code></pre> <p>Execute the complete training pipeline.</p> <p>This is the main entry point for training. It handles the entire training lifecycle including setup, training loops, validation, and cleanup. The method ensures proper cleanup by destroying the distributed process group in the finally block, even if training is interrupted.</p> <p>Example: <pre><code>trainer = MyTrainer(config)\ntrainer.fit()  # Runs the complete training pipeline\n</code></pre></p>"},{"location":"api-reference-example/#backward","title":"backward","text":"<pre><code>backward(loss: torch.Tensor) \u2192 None\n</code></pre> <p>Perform backward pass with gradient accumulation support.</p> <p>This method handles gradient scaling for mixed precision training and gradient accumulation. It should be used instead of <code>loss.backward()</code> directly.</p> <p>Parameters: - loss (torch.Tensor) \u2013 The loss tensor to backpropagate</p> <p>Example: <pre><code>def training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    self.backward(loss)  # Handles accumulation and scaling\n    return {\"loss\": loss}\n</code></pre></p>"},{"location":"api-reference-example/#step","title":"step","text":"<pre><code>step(model: nn.Module, optimizer: Optimizer, scheduler: Optional[LRScheduler] = None) \u2192 float\n</code></pre> <p>Perform optimizer step with gradient clipping and scheduler update.</p> <p>Parameters: - model (nn.Module) \u2013 Model to compute gradient norm for - optimizer (Optimizer) \u2013 Optimizer to step - scheduler (Optional[LRScheduler]) \u2013 Learning rate scheduler to step</p> <p>Returns: - float \u2013 The gradient norm before clipping</p> <p>Example: <pre><code>if not self.is_accumulating_gradients:\n    grad_norm = self.step(self.model, self.optimizer, self.scheduler)\n    self.log(\"grad_norm\", grad_norm)\n</code></pre></p>"},{"location":"api-reference-example/#properties","title":"Properties","text":""},{"location":"api-reference-example/#is_accumulating_gradients","title":"is_accumulating_gradients","text":"<pre><code>property is_accumulating_gradients: bool\n</code></pre> <p>Whether the trainer is currently accumulating gradients.</p> <p>Returns <code>True</code> if the current step is not the last step in a gradient accumulation cycle.</p> <p>Example: <pre><code>def training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n    self.backward(loss)\n\n    if not self.is_accumulating_gradients:\n        # Only log when we actually update weights\n        self.log(\"learning_rate\", self.optimizer.param_groups[0][\"lr\"])\n\n    return {\"loss\": loss}\n</code></pre></p>"},{"location":"api-reference-example/#hooks-and-callbacks","title":"Hooks and Callbacks","text":"<p>The trainer provides numerous hooks throughout the training lifecycle:</p>"},{"location":"api-reference-example/#training-hooks","title":"Training Hooks","text":"<ul> <li><code>pre_launch</code> \u2013 Before distributed world launch</li> <li><code>pre_configure</code> \u2013 Before configuration</li> <li><code>post_configure</code> \u2013 After configuration</li> <li><code>pre_setup</code> \u2013 Before setup</li> <li><code>post_setup</code> \u2013 After setup</li> <li><code>pre_fit</code> \u2013 Before training starts</li> <li><code>pre_epoch</code> \u2013 Before each epoch</li> <li><code>post_epoch</code> \u2013 After each epoch</li> <li><code>post_fit</code> \u2013 After training completes</li> </ul>"},{"location":"api-reference-example/#batch-hooks","title":"Batch Hooks","text":"<ul> <li><code>pre_train_batch</code> \u2013 Before each training batch</li> <li><code>post_train_batch</code> \u2013 After each training batch</li> <li><code>pre_val_batch</code> \u2013 Before each validation batch</li> <li><code>post_val_batch</code> \u2013 After each validation batch</li> </ul>"},{"location":"api-reference-example/#complete-example","title":"Complete Example","text":"<pre><code>from dataclasses import dataclass\nimport torch\nimport torch.nn.functional as F\nfrom dream_trainer import BaseTrainer, BaseTrainerConfig\nfrom dream_trainer.trainer.mixins import SetupMixin, SetupConfigMixin\n\n@dataclass\nclass MyConfig(BaseTrainerConfig, SetupConfigMixin):\n    learning_rate: float = 1e-3\n    hidden_size: int = 128\n\nclass MyTrainer(BaseTrainer, SetupMixin):\n    config: MyConfig\n\n    def configure_models(self):\n        \"\"\"Create model on meta device\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(784, self.config.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.config.hidden_size, 10)\n        )\n\n    def init_weights(self):\n        \"\"\"Initialize weights after device placement\"\"\"\n        def init_fn(m):\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n        self.model.apply(init_fn)\n\n    def configure_optimizers(self):\n        \"\"\"Set up optimizer\"\"\"\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.config.learning_rate\n        )\n\n    def configure_dataloaders(self):\n        \"\"\"Return train and validation dataloaders\"\"\"\n        train_dataset = MyDataset(\"train\")\n        val_dataset = MyDataset(\"val\")\n\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.training_parameters.train_batch_size,\n            shuffle=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.training_parameters.val_batch_size,\n            shuffle=False\n        )\n\n        return train_loader, val_loader\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x.view(x.size(0), -1))\n        loss = F.cross_entropy(logits, y)\n\n        self.backward(loss)\n\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n            accuracy = (logits.argmax(dim=1) == y).float().mean()\n\n            return {\n                \"loss\": loss,\n                \"accuracy\": accuracy,\n                \"grad_norm\": grad_norm\n            }\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n\n        with torch.no_grad():\n            logits = self.model(x.view(x.size(0), -1))\n            loss = F.cross_entropy(logits, y)\n            accuracy = (logits.argmax(dim=1) == y).float().mean()\n\n        return {\n            \"val_loss\": loss,\n            \"val_accuracy\": accuracy\n        }\n\n# Usage\nconfig = MyConfig(\n    project=\"mnist-classification\",\n    group=\"baseline\",\n    experiment=\"run-001\",\n    training_parameters=TrainingParameters(\n        n_epochs=10,\n        train_batch_size=32,\n        gradient_accumulation_steps=4\n    )\n)\n\ntrainer = MyTrainer(config)\ntrainer.fit()\n</code></pre>"},{"location":"api-reference-example/#see-also","title":"See Also","text":"<ul> <li><code>AbstractTrainer</code> \u2013 Base interface for all trainers</li> <li><code>DreamTrainer</code> \u2013 Full-featured trainer with all mixins</li> <li>Training Guide \u2013 Detailed training concepts</li> <li>Configuration Guide \u2013 Configuration patterns</li> </ul>"},{"location":"api-reference-example/#notes","title":"Notes","text":"<ul> <li>The trainer automatically handles distributed training setup when launched with <code>torchrun</code></li> <li>Gradient accumulation is handled transparently - just call <code>self.backward(loss)</code></li> <li>Always use <code>self.step()</code> instead of calling <code>optimizer.step()</code> directly</li> <li>The trainer manages device placement - models are created on meta device and materialized during setup</li> </ul> <p>This example demonstrates the level of detail and organization expected for API reference documentation. Each class should have similar comprehensive documentation with examples, parameter descriptions, and cross-references. </p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"callbacks/","title":"Callbacks Guide","text":"<p>This guide explains how to use and create callbacks in Dream Trainer.</p>"},{"location":"callbacks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Usage</li> <li>Built-in Callbacks</li> <li>Creating Callbacks</li> <li>Callback Collection</li> <li>Best Practices</li> </ul>"},{"location":"callbacks/#basic-usage","title":"Basic Usage","text":"<p>Callbacks are a way to extend the trainer's functionality without modifying its code. They are called at specific points during training.</p>"},{"location":"callbacks/#adding-callbacks","title":"Adding Callbacks","text":"<p>Add callbacks to your trainer configuration:</p> <pre><code>from dream_trainer import DreamTrainerConfig\nfrom dream_trainer.callbacks import (\n    LoggerCallback,\n    ProgressBar,\n    CallbackCollection\n)\n\nconfig = DreamTrainerConfig(\n    # ... other settings ...\n    callbacks=CallbackCollection([\n        LoggerCallback(),  # Logs metrics to console/WandB\n        ProgressBar(),    # Shows training progress\n    ])\n)\n</code></pre>"},{"location":"callbacks/#callback-order","title":"Callback Order","text":"<p>Callbacks are executed in the order they are added. You can control the order:</p> <pre><code>callbacks = CallbackCollection([\n    LoggerCallback(),     # First: log metrics\n    ProgressBar(),       # Second: show progress\n    CheckpointCallback() # Third: save checkpoints\n])\n</code></pre>"},{"location":"callbacks/#built-in-callbacks","title":"Built-in Callbacks","text":""},{"location":"callbacks/#loggercallback","title":"LoggerCallback","text":"<p>Logs metrics to console and/or WandB:</p> <pre><code>from dream_trainer.callbacks import LoggerCallback\n\nlogger = LoggerCallback(\n    log_every_n_steps=100,  # Log every 100 steps\n    log_every_n_epochs=1,   # Log every epoch\n    log_metrics=True,       # Log metrics\n    log_gradients=False,    # Don't log gradients\n    log_parameters=False    # Don't log parameters\n)\n</code></pre>"},{"location":"callbacks/#progressbar","title":"ProgressBar","text":"<p>Shows training progress:</p> <pre><code>from dream_trainer.callbacks import ProgressBar\n\nprogress = ProgressBar(\n    refresh_rate=10,        # Update every 10 steps\n    show_epoch=True,        # Show epoch number\n    show_step=True,         # Show step number\n    show_metrics=True       # Show metrics\n)\n</code></pre>"},{"location":"callbacks/#checkpointcallback","title":"CheckpointCallback","text":"<p>Saves model checkpoints:</p> <pre><code>from dream_trainer.callbacks import CheckpointCallback\n\ncheckpoint = CheckpointCallback(\n    monitor=\"val_loss\",     # Metric to monitor\n    mode=\"min\",            # Minimize metric\n    save_top_k=3,          # Keep best 3 checkpoints\n    save_last=True,        # Always save latest\n    every_n_epochs=1       # Save every epoch\n)\n</code></pre>"},{"location":"callbacks/#earlystoppingcallback","title":"EarlyStoppingCallback","text":"<p>Stops training when metric stops improving:</p> <pre><code>from dream_trainer.callbacks import EarlyStoppingCallback\n\nearly_stopping = EarlyStoppingCallback(\n    monitor=\"val_loss\",     # Metric to monitor\n    mode=\"min\",            # Minimize metric\n    patience=5,            # Wait 5 epochs\n    min_delta=0.001        # Minimum change\n)\n</code></pre>"},{"location":"callbacks/#learningratemonitor","title":"LearningRateMonitor","text":"<p>Logs learning rate changes:</p> <pre><code>from dream_trainer.callbacks import LearningRateMonitor\n\nlr_monitor = LearningRateMonitor(\n    logging_interval=\"step\",  # Log every step\n    log_momentum=True        # Log momentum too\n)\n</code></pre>"},{"location":"callbacks/#creating-callbacks","title":"Creating Callbacks","text":""},{"location":"callbacks/#basic-callback","title":"Basic Callback","text":"<p>Create a custom callback by extending <code>Callback</code>:</p> <pre><code>from dream_trainer.callbacks import Callback\n\nclass MyCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        \"\"\"Called after each training batch\"\"\"\n        # Access trainer state\n        current_epoch = trainer.current_epoch\n        current_step = trainer.current_step\n\n        # Access outputs\n        loss = outputs[\"loss\"]\n\n        # Do something\n        if loss &gt; 10.0:\n            print(f\"High loss detected: {loss}\")\n</code></pre>"},{"location":"callbacks/#training-hooks","title":"Training Hooks","text":"<p>Available training hooks:</p> <pre><code>class MyCallback(Callback):\n    def on_train_start(self, trainer):\n        \"\"\"Called when training starts\"\"\"\n        pass\n\n    def on_train_epoch_start(self, trainer):\n        \"\"\"Called at the start of each training epoch\"\"\"\n        pass\n\n    def on_train_batch_start(self, trainer, batch, batch_idx):\n        \"\"\"Called before each training batch\"\"\"\n        pass\n\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        \"\"\"Called after each training batch\"\"\"\n        pass\n\n    def on_train_epoch_end(self, trainer):\n        \"\"\"Called at the end of each training epoch\"\"\"\n        pass\n\n    def on_train_end(self, trainer):\n        \"\"\"Called when training ends\"\"\"\n        pass\n</code></pre>"},{"location":"callbacks/#validation-hooks","title":"Validation Hooks","text":"<p>Available validation hooks:</p> <pre><code>class MyCallback(Callback):\n    def on_validation_start(self, trainer):\n        \"\"\"Called when validation starts\"\"\"\n        pass\n\n    def on_validation_epoch_start(self, trainer):\n        \"\"\"Called at the start of each validation epoch\"\"\"\n        pass\n\n    def on_validation_batch_start(self, trainer, batch, batch_idx):\n        \"\"\"Called before each validation batch\"\"\"\n        pass\n\n    def on_validation_batch_end(self, trainer, outputs, batch, batch_idx):\n        \"\"\"Called after each validation batch\"\"\"\n        pass\n\n    def on_validation_epoch_end(self, trainer):\n        \"\"\"Called at the end of each validation epoch\"\"\"\n        pass\n\n    def on_validation_end(self, trainer):\n        \"\"\"Called when validation ends\"\"\"\n        pass\n</code></pre>"},{"location":"callbacks/#state-management","title":"State Management","text":"<p>Callbacks can maintain their own state:</p> <pre><code>class StatefulCallback(Callback):\n    def __init__(self):\n        super().__init__()\n        self.best_metric = float('inf')\n        self.patience_counter = 0\n\n    def on_validation_epoch_end(self, trainer):\n        # Get current metric\n        current_metric = trainer.get_metric(\"val_loss\")\n\n        # Update state\n        if current_metric &lt; self.best_metric:\n            self.best_metric = current_metric\n            self.patience_counter = 0\n        else:\n            self.patience_counter += 1\n\n        # Check patience\n        if self.patience_counter &gt;= 5:\n            trainer.should_stop = True\n</code></pre>"},{"location":"callbacks/#accessing-trainer","title":"Accessing Trainer","text":"<p>Callbacks have access to the trainer instance:</p> <pre><code>class TrainerAwareCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        # Access trainer attributes\n        model = trainer.model\n        optimizer = trainer.optimizer\n        current_epoch = trainer.current_epoch\n\n        # Access trainer methods\n        trainer.log(\"custom_metric\", 42)\n        trainer.save_checkpoint(\"path/to/checkpoint.pt\")\n</code></pre>"},{"location":"callbacks/#callback-collection","title":"Callback Collection","text":""},{"location":"callbacks/#adding-callbacks_1","title":"Adding Callbacks","text":"<p>Add callbacks to a collection:</p> <pre><code>from dream_trainer.callbacks import CallbackCollection\n\ncallbacks = CallbackCollection([\n    LoggerCallback(),\n    ProgressBar(),\n    MyCustomCallback()\n])\n</code></pre>"},{"location":"callbacks/#removing-callbacks","title":"Removing Callbacks","text":"<p>Remove callbacks from a collection:</p> <pre><code># Remove by type\ncallbacks.remove(LoggerCallback)\n\n# Remove by instance\ncallbacks.remove(my_callback)\n</code></pre>"},{"location":"callbacks/#reordering-callbacks","title":"Reordering Callbacks","text":"<p>Change callback order:</p> <pre><code># Move to front\ncallbacks.move_to_front(my_callback)\n\n# Move to back\ncallbacks.move_to_back(my_callback)\n\n# Move to specific position\ncallbacks.move_to_position(my_callback, 2)\n</code></pre>"},{"location":"callbacks/#best-practices","title":"Best Practices","text":""},{"location":"callbacks/#1-keep-callbacks-focused","title":"1. Keep Callbacks Focused","text":"<p>Each callback should do one thing well:</p> <pre><code># Good: Single responsibility\nclass LossMonitor(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        if outputs[\"loss\"] &gt; 10.0:\n            print(\"High loss detected\")\n\n# Bad: Multiple responsibilities\nclass BadCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        # Monitoring\n        if outputs[\"loss\"] &gt; 10.0:\n            print(\"High loss detected\")\n        # Logging\n        trainer.log(\"custom_metric\", 42)\n        # Checkpointing\n        trainer.save_checkpoint(\"checkpoint.pt\")\n</code></pre>"},{"location":"callbacks/#2-use-type-hints","title":"2. Use Type Hints","text":"<p>Add type hints for better IDE support:</p> <pre><code>from typing import Dict, Any\nimport torch\n\nclass TypedCallback(Callback):\n    def on_train_batch_end(\n        self,\n        trainer: \"DreamTrainer\",\n        outputs: Dict[str, torch.Tensor],\n        batch: torch.Tensor,\n        batch_idx: int\n    ) -&gt; None:\n        pass\n</code></pre>"},{"location":"callbacks/#3-document-callbacks","title":"3. Document Callbacks","text":"<p>Add docstrings to explain functionality:</p> <pre><code>class DocumentedCallback(Callback):\n    \"\"\"Monitors training metrics and logs warnings.\n\n    This callback watches for:\n    - High loss values\n    - NaN gradients\n    - Learning rate spikes\n\n    Args:\n        loss_threshold: Threshold for high loss warning\n        lr_threshold: Threshold for learning rate warning\n    \"\"\"\n\n    def __init__(self, loss_threshold: float = 10.0, lr_threshold: float = 1e-2):\n        super().__init__()\n        self.loss_threshold = loss_threshold\n        self.lr_threshold = lr_threshold\n</code></pre>"},{"location":"callbacks/#4-handle-errors","title":"4. Handle Errors","text":"<p>Add proper error handling:</p> <pre><code>class ErrorHandlingCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        try:\n            # Risky operation\n            self.process_outputs(outputs)\n        except Exception as e:\n            # Log error but don't crash\n            trainer.log(\"callback_error\", str(e))\n</code></pre>"},{"location":"callbacks/#5-test-callbacks","title":"5. Test Callbacks","text":"<p>Write unit tests for your callbacks:</p> <pre><code>def test_my_callback():\n    # Create mock trainer\n    trainer = MockTrainer()\n\n    # Create callback\n    callback = MyCallback()\n\n    # Test hook\n    callback.on_train_batch_end(\n        trainer,\n        outputs={\"loss\": torch.tensor(5.0)},\n        batch=torch.randn(32, 10),\n        batch_idx=0\n    )\n\n    # Assert expected behavior\n    assert trainer.logged_metrics[\"custom_metric\"] == 42\n</code></pre>"},{"location":"callbacks/#6-use-callback-priority","title":"6. Use Callback Priority","text":"<p>Set callback priority for execution order:</p> <pre><code>class HighPriorityCallback(Callback):\n    priority = 100  # Higher number = earlier execution\n\nclass LowPriorityCallback(Callback):\n    priority = 0    # Lower number = later execution\n</code></pre>"},{"location":"callbacks/#7-avoid-side-effects","title":"7. Avoid Side Effects","text":"<p>Minimize side effects in callbacks:</p> <pre><code>class CleanCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        # Good: Only logging\n        trainer.log(\"metric\", outputs[\"loss\"])\n\n        # Bad: Modifying trainer state\n        trainer.model.requires_grad_(False)  # Don't do this\n</code></pre>"},{"location":"callbacks/#8-use-callback-groups","title":"8. Use Callback Groups","text":"<p>Group related callbacks:</p> <pre><code>class MonitoringGroup(Callback):\n    \"\"\"Group of monitoring callbacks\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.callbacks = [\n            LossMonitor(),\n            GradientMonitor(),\n            LearningRateMonitor()\n        ]\n\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        for callback in self.callbacks:\n            callback.on_train_batch_end(trainer, outputs, batch, batch_idx)\n</code></pre>"},{"location":"callbacks/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Examples to see callbacks in action</li> <li>Read about Distributed Training for multi-GPU callback considerations</li> <li>Check the API Reference for detailed callback documentation</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"configuration-mastery/","title":"Configuration Mastery","text":"<p>This guide dives deep into advanced configuration patterns for Dream Trainer, showcasing how to leverage Python's type system for powerful, maintainable, and production-ready configurations.</p>"},{"location":"configuration-mastery/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Configuration Patterns</li> <li>Factory Functions</li> <li>Conditional Configuration</li> <li>Environment-Based Configs</li> <li>Multi-Experiment Management</li> <li>Validation &amp; Constraints</li> <li>Custom Validators</li> <li>Type Safety Patterns</li> <li>Configuration Testing</li> <li>Real-World Examples</li> <li>Multi-Node Configurations</li> <li>A/B Testing Setups</li> <li>Hyperparameter Sweeps</li> </ul>"},{"location":"configuration-mastery/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"configuration-mastery/#factory-functions","title":"Factory Functions","text":"<p>Factory functions allow you to create complex configurations dynamically based on input parameters. This pattern is especially useful when configurations have interdependencies or need runtime computation.</p> <pre><code>from dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import Callable, Optional\nimport torch\nimport torch.nn as nn\nfrom dream_trainer import DreamTrainerConfig\nfrom dream_trainer.configs import DeviceParameters, TrainingParameters\n\n# Model factory function\ndef create_transformer_config(\n    model_size: str = \"base\",\n    vocab_size: int = 50257,\n    max_seq_len: int = 2048,\n    flash_attention: bool = True,\n) -&gt; Callable[[], nn.Module]:\n    \"\"\"Factory function for creating transformer models with size presets.\"\"\"\n\n    size_configs = {\n        \"tiny\": {\"dim\": 384, \"heads\": 6, \"layers\": 6, \"ffn_mult\": 4},\n        \"small\": {\"dim\": 768, \"heads\": 12, \"layers\": 12, \"ffn_mult\": 4},\n        \"base\": {\"dim\": 1024, \"heads\": 16, \"layers\": 24, \"ffn_mult\": 4},\n        \"large\": {\"dim\": 1280, \"heads\": 20, \"layers\": 36, \"ffn_mult\": 4},\n        \"xl\": {\"dim\": 1600, \"heads\": 25, \"layers\": 48, \"ffn_mult\": 4},\n    }\n\n    if model_size not in size_configs:\n        raise ValueError(f\"Unknown model size: {model_size}\")\n\n    config = size_configs[model_size]\n\n    # Return a partial function that creates the model\n    return partial(\n        TransformerModel,\n        vocab_size=vocab_size,\n        max_seq_len=max_seq_len,\n        dim=config[\"dim\"],\n        n_heads=config[\"heads\"],\n        n_layers=config[\"layers\"],\n        ffn_dim_multiplier=config[\"ffn_mult\"],\n        use_flash_attention=flash_attention,\n    )\n\n# Optimizer factory with model-size-aware settings\ndef create_optimizer_config(\n    model_size: str,\n    base_lr: float = 3e-4,\n    weight_decay: float = 0.1,\n    beta1: float = 0.9,\n    beta2: float = 0.95,\n) -&gt; Callable:\n    \"\"\"Create optimizer config with size-appropriate learning rates.\"\"\"\n\n    # Scale learning rate based on model size\n    lr_scale = {\n        \"tiny\": 2.0,\n        \"small\": 1.5, \n        \"base\": 1.0,\n        \"large\": 0.8,\n        \"xl\": 0.6,\n    }\n\n    scaled_lr = base_lr * lr_scale.get(model_size, 1.0)\n\n    return partial(\n        torch.optim.AdamW,\n        lr=scaled_lr,\n        betas=(beta1, beta2),\n        weight_decay=weight_decay,\n        eps=1e-8,\n    )\n\n# Scheduler factory with warmup calculation\ndef create_scheduler_config(\n    optimizer: torch.optim.Optimizer,\n    total_steps: int,\n    warmup_ratio: float = 0.05,\n    min_lr_ratio: float = 0.1,\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Create a cosine scheduler with linear warmup.\"\"\"\n\n    warmup_steps = int(total_steps * warmup_ratio)\n\n    def lr_lambda(step: int) -&gt; float:\n        if step &lt; warmup_steps:\n            return step / warmup_steps\n\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return min_lr_ratio + (1 - min_lr_ratio) * (1 + np.cos(np.pi * progress)) / 2\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# Usage example\n@dataclass\nclass ExperimentConfig(DreamTrainerConfig):\n    \"\"\"Config using factory functions for dynamic configuration.\"\"\"\n\n    # Model configuration\n    model_size: str = \"base\"\n    model: Callable = field(default_factory=lambda: create_transformer_config(\"base\"))\n\n    # Optimizer configuration \n    optimizer: Callable = field(default_factory=lambda: create_optimizer_config(\"base\"))\n\n    # Scheduler configuration (requires optimizer instance)\n    scheduler_factory: Callable = field(\n        default_factory=lambda: create_scheduler_config\n    )\n\n    def __post_init__(self):\n        \"\"\"Update factories based on model size.\"\"\"\n        self.model = create_transformer_config(self.model_size)\n        self.optimizer = create_optimizer_config(self.model_size)\n</code></pre>"},{"location":"configuration-mastery/#conditional-configuration","title":"Conditional Configuration","text":"<p>Conditional configuration allows you to adjust settings based on runtime conditions, hardware availability, or other factors.</p> <pre><code>import os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\n@dataclass\nclass ConditionalConfig(DreamTrainerConfig):\n    \"\"\"Configuration that adapts to runtime environment.\"\"\"\n\n    # Basic settings\n    experiment_name: str = \"adaptive_training\"\n    debug_mode: bool = field(default_factory=lambda: os.getenv(\"DEBUG\", \"0\") == \"1\")\n\n    # Conditional batch size based on GPU memory\n    @property\n    def batch_size(self) -&gt; int:\n        \"\"\"Dynamically set batch size based on available GPU memory.\"\"\"\n        if not torch.cuda.is_available():\n            return 4  # CPU debugging\n\n        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n\n        if self.debug_mode:\n            return 2\n        elif gpu_memory_gb &lt; 16:\n            return 8\n        elif gpu_memory_gb &lt; 40:\n            return 16\n        elif gpu_memory_gb &lt; 80:\n            return 32\n        else:\n            return 64\n\n    # Conditional gradient accumulation\n    @property\n    def gradient_accumulation_steps(self) -&gt; int:\n        \"\"\"Adjust gradient accumulation to maintain effective batch size.\"\"\"\n        target_batch_size = 256  # Target effective batch size\n        return max(1, target_batch_size // (self.batch_size * self.world_size))\n\n    # Conditional precision settings\n    @property\n    def mixed_precision(self) -&gt; Optional[str]:\n        \"\"\"Enable mixed precision based on GPU capability.\"\"\"\n        if not torch.cuda.is_available():\n            return None\n\n        capability = torch.cuda.get_device_capability()\n        if capability &gt;= (8, 0):  # Ampere or newer\n            return \"bf16\"\n        elif capability &gt;= (7, 0):  # Volta or newer\n            return \"fp16\"\n        else:\n            return None\n\n    # Conditional compilation\n    @property\n    def compile_model(self) -&gt; bool:\n        \"\"\"Enable compilation only in production mode.\"\"\"\n        return not self.debug_mode and torch.cuda.is_available()\n\n    # Conditional checkpointing\n    @property\n    def checkpoint_frequency(self) -&gt; Union[int, float]:\n        \"\"\"Checkpoint more frequently in debug mode.\"\"\"\n        return 0.1 if self.debug_mode else 1.0  # Every 10% vs every epoch\n\n    # Conditional data paths\n    @property\n    def data_root(self) -&gt; Path:\n        \"\"\"Select data path based on environment.\"\"\"\n        if cluster_path := os.getenv(\"CLUSTER_DATA_PATH\"):\n            return Path(cluster_path)\n        elif self.debug_mode:\n            return Path(\"./debug_data\")\n        else:\n            return Path(\"/data/datasets\")\n\n# Advanced conditional configuration with feature flags\n@dataclass \nclass FeatureFlagConfig(DreamTrainerConfig):\n    \"\"\"Configuration with feature flags for gradual rollout.\"\"\"\n\n    # Feature flags\n    enable_new_optimizer: bool = field(\n        default_factory=lambda: os.getenv(\"ENABLE_NEW_OPTIMIZER\", \"false\").lower() == \"true\"\n    )\n    enable_experimental_parallelism: bool = field(\n        default_factory=lambda: os.getenv(\"ENABLE_EXP_PARALLEL\", \"false\").lower() == \"true\"\n    )\n\n    def configure_optimizer(self) -&gt; Callable:\n        \"\"\"Select optimizer based on feature flag.\"\"\"\n        if self.enable_new_optimizer:\n            # New experimental optimizer\n            return partial(\n                torch.optim.AdamW,\n                lr=1e-4,\n                betas=(0.95, 0.98),\n                weight_decay=0.05,\n                fused=True,  # New feature\n            )\n        else:\n            # Stable optimizer\n            return partial(\n                torch.optim.AdamW,\n                lr=3e-4,\n                betas=(0.9, 0.95),\n                weight_decay=0.1,\n            )\n\n    def configure_device_parameters(self) -&gt; DeviceParameters:\n        \"\"\"Configure parallelism based on feature flags.\"\"\"\n        if self.enable_experimental_parallelism:\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                tensor_parallel=4,\n                context_parallel=2,\n                async_tensor_parallel=True,\n                loss_parallel=True,\n            )\n        else:\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                tensor_parallel=1,\n                compile_model=True,\n            )\n</code></pre>"},{"location":"configuration-mastery/#environment-based-configs","title":"Environment-Based Configs","text":"<p>Environment-based configurations allow you to maintain different settings for development, staging, and production environments.</p> <pre><code>import socket\nfrom enum import Enum, auto\nfrom typing import Dict, Any\n\nclass Environment(Enum):\n    \"\"\"Training environment types.\"\"\"\n    LOCAL = auto()\n    DEVELOPMENT = auto() \n    STAGING = auto()\n    PRODUCTION = auto()\n\ndef detect_environment() -&gt; Environment:\n    \"\"\"Automatically detect the current environment.\"\"\"\n    hostname = socket.gethostname()\n\n    if \"local\" in hostname or \"laptop\" in hostname:\n        return Environment.LOCAL\n    elif \"dev-\" in hostname:\n        return Environment.DEVELOPMENT\n    elif \"staging-\" in hostname:\n        return Environment.STAGING\n    elif \"prod-\" in hostname or \"cluster\" in hostname:\n        return Environment.PRODUCTION\n    else:\n        # Check environment variable\n        env_name = os.getenv(\"TRAINING_ENV\", \"local\").lower()\n        return Environment[env_name.upper()]\n\n@dataclass\nclass EnvironmentConfig(DreamTrainerConfig):\n    \"\"\"Configuration that adapts to different environments.\"\"\"\n\n    environment: Environment = field(default_factory=detect_environment)\n\n    # Environment-specific settings\n    ENV_SETTINGS: Dict[Environment, Dict[str, Any]] = field(default_factory=lambda: {\n        Environment.LOCAL: {\n            \"num_workers\": 0,  # Avoid multiprocessing issues\n            \"prefetch_factor\": 2,\n            \"pin_memory\": False,\n            \"persistent_workers\": False,\n            \"log_frequency\": 1,  # Log every step\n            \"checkpoint_frequency\": 0.1,  # Frequent checkpoints\n            \"enable_profiling\": True,\n            \"compile_model\": False,\n            \"use_wandb\": False,\n        },\n        Environment.DEVELOPMENT: {\n            \"num_workers\": 4,\n            \"prefetch_factor\": 2,\n            \"pin_memory\": True,\n            \"persistent_workers\": True,\n            \"log_frequency\": 10,\n            \"checkpoint_frequency\": 0.25,\n            \"enable_profiling\": True,\n            \"compile_model\": False,\n            \"use_wandb\": True,\n        },\n        Environment.STAGING: {\n            \"num_workers\": 8,\n            \"prefetch_factor\": 4,\n            \"pin_memory\": True,\n            \"persistent_workers\": True,\n            \"log_frequency\": 50,\n            \"checkpoint_frequency\": 1.0,\n            \"enable_profiling\": False,\n            \"compile_model\": True,\n            \"use_wandb\": True,\n        },\n        Environment.PRODUCTION: {\n            \"num_workers\": 16,\n            \"prefetch_factor\": 4,\n            \"pin_memory\": True,\n            \"persistent_workers\": True,\n            \"log_frequency\": 100,\n            \"checkpoint_frequency\": 1.0,\n            \"enable_profiling\": False,\n            \"compile_model\": True,\n            \"use_wandb\": True,\n        },\n    })\n\n    def __post_init__(self):\n        \"\"\"Apply environment-specific settings.\"\"\"\n        settings = self.ENV_SETTINGS[self.environment]\n\n        # Apply settings dynamically\n        for key, value in settings.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n\n    @property\n    def data_path(self) -&gt; Path:\n        \"\"\"Environment-specific data paths.\"\"\"\n        paths = {\n            Environment.LOCAL: Path(\"./data/sample\"),\n            Environment.DEVELOPMENT: Path(\"/mnt/dev-data\"),\n            Environment.STAGING: Path(\"/mnt/staging-data\"),\n            Environment.PRODUCTION: Path(\"/mnt/production-data\"),\n        }\n        return paths[self.environment]\n\n    @property\n    def checkpoint_dir(self) -&gt; Path:\n        \"\"\"Environment-specific checkpoint directories.\"\"\"\n        base_paths = {\n            Environment.LOCAL: Path(\"./checkpoints\"),\n            Environment.DEVELOPMENT: Path(\"/mnt/dev-checkpoints\"),\n            Environment.STAGING: Path(\"/mnt/staging-checkpoints\"),\n            Environment.PRODUCTION: Path(\"/mnt/production-checkpoints\"),\n        }\n        return base_paths[self.environment] / self.experiment\n\n# Cluster-aware configuration\n@dataclass\nclass ClusterConfig(EnvironmentConfig):\n    \"\"\"Configuration aware of specific cluster environments.\"\"\"\n\n    @property\n    def cluster_type(self) -&gt; str:\n        \"\"\"Detect cluster type from environment.\"\"\"\n        if slurm_cluster := os.getenv(\"SLURM_CLUSTER_NAME\"):\n            return f\"slurm_{slurm_cluster}\"\n        elif k8s_namespace := os.getenv(\"K8S_NAMESPACE\"):\n            return f\"k8s_{k8s_namespace}\"\n        elif aws_region := os.getenv(\"AWS_DEFAULT_REGION\"):\n            return f\"aws_{aws_region}\"\n        else:\n            return \"local\"\n\n    def configure_for_cluster(self) -&gt; DeviceParameters:\n        \"\"\"Configure device parameters based on cluster.\"\"\"\n        if \"slurm\" in self.cluster_type:\n            # SLURM cluster configuration\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                comm={\"backend\": \"nccl\", \"timeout\": 1800},\n                cpu_offload=False,\n            )\n        elif \"k8s\" in self.cluster_type:\n            # Kubernetes cluster configuration\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                comm={\"backend\": \"nccl\", \"init_method\": \"env://\"},\n                checkpoint_activations=True,  # Memory constrained\n            )\n        elif \"aws\" in self.cluster_type:\n            # AWS cluster configuration\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                comm={\"backend\": \"nccl\", \"timeout\": 3600},\n                enable_compiled_autograd=True,\n            )\n        else:\n            # Local configuration\n            return DeviceParameters(\n                dp_shard=1,\n                compile_model=False,\n            )\n</code></pre>"},{"location":"configuration-mastery/#multi-experiment-management","title":"Multi-Experiment Management","text":"<p>Managing multiple experiments with shared base configurations and experiment-specific overrides.</p> <pre><code>from typing import List, Dict, Type\nfrom datetime import datetime\n\n@dataclass\nclass BaseExperimentConfig(DreamTrainerConfig):\n    \"\"\"Base configuration shared across all experiments.\"\"\"\n\n    # Shared project settings\n    project: str = \"transformer_research\"\n    entity: str = \"my_team\"\n    tags: List[str] = field(default_factory=lambda: [\"baseline\", \"v2\"])\n\n    # Shared model architecture\n    vocab_size: int = 50257\n    max_seq_len: int = 2048\n    dropout: float = 0.1\n\n    # Shared training settings\n    gradient_clip_val: float = 1.0\n    weight_decay: float = 0.1\n    warmup_steps: int = 2000\n\n    # Shared data settings\n    data_root: Path = Path(\"/data/common_dataset\")\n\n    def experiment_name(self) -&gt; str:\n        \"\"\"Generate unique experiment name.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        return f\"{self.group}_{timestamp}\"\n\n# Experiment registry\nEXPERIMENTS: Dict[str, Type[BaseExperimentConfig]] = {}\n\ndef register_experiment(name: str):\n    \"\"\"Decorator to register experiments.\"\"\"\n    def decorator(cls):\n        EXPERIMENTS[name] = cls\n        return cls\n    return decorator\n\n@register_experiment(\"small_baseline\")\n@dataclass\nclass SmallBaselineConfig(BaseExperimentConfig):\n    \"\"\"Small model for quick iteration.\"\"\"\n\n    group: str = \"small_baseline\"\n    model_size: str = \"small\"\n    hidden_size: int = 768\n    num_heads: int = 12\n    num_layers: int = 12\n\n    # Small model specific settings\n    learning_rate: float = 6e-4\n    batch_size: int = 32\n    gradient_accumulation_steps: int = 1\n\n    # Frequent validation for debugging\n    val_frequency: float = 0.1\n\n    # More tags\n    tags: List[str] = field(\n        default_factory=lambda: BaseExperimentConfig.tags.__wrapped__() + [\"small\", \"debug\"]\n    )\n\n@register_experiment(\"large_scale\")\n@dataclass\nclass LargeScaleConfig(BaseExperimentConfig):\n    \"\"\"Large model for production training.\"\"\"\n\n    group: str = \"large_scale\"\n    model_size: str = \"large\"\n    hidden_size: int = 1280\n    num_heads: int = 20\n    num_layers: int = 36\n\n    # Large model specific settings\n    learning_rate: float = 2e-4\n    batch_size: int = 8  # Per GPU\n    gradient_accumulation_steps: int = 32  # Effective batch size 256\n\n    # Less frequent validation to save time\n    val_frequency: float = 1.0\n\n    # Enable optimizations\n    compile_model: bool = True\n    checkpoint_activations: bool = True\n\n    # Parallelism settings\n    device_parameters: DeviceParameters = field(\n        default_factory=lambda: DeviceParameters(\n            dp_shard=\"auto\",\n            tensor_parallel=4,\n            loss_parallel=True,\n        )\n    )\n\n@register_experiment(\"ablation_no_warmup\")\n@dataclass \nclass AblationNoWarmupConfig(BaseExperimentConfig):\n    \"\"\"Ablation study: no learning rate warmup.\"\"\"\n\n    group: str = \"ablation_studies\"\n    warmup_steps: int = 0  # Override base config\n\n    # Keep most settings from base\n    model_size: str = \"base\"\n    hidden_size: int = 1024\n    num_heads: int = 16\n    num_layers: int = 24\n\n    tags: List[str] = field(\n        default_factory=lambda: BaseExperimentConfig.tags.__wrapped__() + [\"ablation\", \"no_warmup\"]\n    )\n\n@register_experiment(\"hyperparameter_search\")\n@dataclass\nclass HyperparameterSearchConfig(BaseExperimentConfig):\n    \"\"\"Configuration for hyperparameter search.\"\"\"\n\n    group: str = \"hyperparam_search\"\n\n    # Hyperparameters to search\n    learning_rate: float = field(default_factory=lambda: np.random.choice([1e-4, 3e-4, 6e-4]))\n    dropout: float = field(default_factory=lambda: np.random.uniform(0.0, 0.3))\n    weight_decay: float = field(default_factory=lambda: np.random.choice([0.01, 0.05, 0.1]))\n\n    # Fixed model size for fair comparison\n    model_size: str = \"base\"\n    hidden_size: int = 1024\n    num_heads: int = 16\n    num_layers: int = 24\n\n    def __post_init__(self):\n        \"\"\"Add hyperparameter values to tags.\"\"\"\n        super().__post_init__()\n        self.tags.extend([\n            f\"lr_{self.learning_rate}\",\n            f\"dropout_{self.dropout:.2f}\",\n            f\"wd_{self.weight_decay}\",\n        ])\n\n# Experiment launcher\ndef launch_experiment(experiment_name: str, **overrides):\n    \"\"\"Launch an experiment by name with optional overrides.\"\"\"\n    if experiment_name not in EXPERIMENTS:\n        raise ValueError(f\"Unknown experiment: {experiment_name}\")\n\n    config_class = EXPERIMENTS[experiment_name]\n    config = config_class(**overrides)\n\n    # Create trainer and run\n    trainer = DreamTrainer(config)\n    trainer.fit()\n\n    return trainer\n\n# Multi-experiment runner\ndef run_all_experiments():\n    \"\"\"Run all registered experiments.\"\"\"\n    results = {}\n\n    for name, config_class in EXPERIMENTS.items():\n        print(f\"Running experiment: {name}\")\n        config = config_class()\n\n        trainer = DreamTrainer(config)\n        trainer.fit()\n\n        results[name] = {\n            \"config\": config,\n            \"metrics\": trainer.get_metrics(),\n        }\n\n    return results\n</code></pre>"},{"location":"configuration-mastery/#validation-constraints","title":"Validation &amp; Constraints","text":""},{"location":"configuration-mastery/#custom-validators","title":"Custom Validators","text":"<p>Implement custom validation logic to ensure configuration consistency and catch errors early.</p> <pre><code>from typing import Union, List, Optional\nimport re\n\nclass ValidationError(ValueError):\n    \"\"\"Custom exception for configuration validation errors.\"\"\"\n    pass\n\n@dataclass\nclass ValidatedConfig(DreamTrainerConfig):\n    \"\"\"Configuration with comprehensive validation.\"\"\"\n\n    # Model settings with validation\n    model_name: str = \"transformer\"\n    hidden_size: int = 768\n    num_layers: int = 12\n    num_heads: int = 12\n\n    # Training settings\n    learning_rate: float = 3e-4\n    batch_size: int = 32\n    gradient_clip_val: Optional[float] = 1.0\n\n    # Paths\n    checkpoint_path: Optional[Path] = None\n\n    def __post_init__(self):\n        \"\"\"Run all validations after initialization.\"\"\"\n        self.validate_model_architecture()\n        self.validate_training_parameters()\n        self.validate_paths()\n        self.validate_dependencies()\n\n    def validate_model_architecture(self):\n        \"\"\"Validate model architecture consistency.\"\"\"\n        # Check if hidden size is divisible by num_heads\n        if self.hidden_size % self.num_heads != 0:\n            raise ValidationError(\n                f\"hidden_size ({self.hidden_size}) must be divisible by \"\n                f\"num_heads ({self.num_heads})\"\n            )\n\n        # Check reasonable layer count\n        if not 1 &lt;= self.num_layers &lt;= 200:\n            raise ValidationError(\n                f\"num_layers ({self.num_layers}) should be between 1 and 200\"\n            )\n\n        # Validate model name format\n        if not re.match(r\"^[a-zA-Z][a-zA-Z0-9_-]*$\", self.model_name):\n            raise ValidationError(\n                f\"Invalid model name: {self.model_name}. \"\n                \"Must start with letter and contain only alphanumeric, dash, or underscore\"\n            )\n\n    def validate_training_parameters(self):\n        \"\"\"Validate training hyperparameters.\"\"\"\n        # Learning rate range\n        if not 1e-6 &lt;= self.learning_rate &lt;= 1e-1:\n            raise ValidationError(\n                f\"learning_rate ({self.learning_rate}) should be between 1e-6 and 1e-1\"\n            )\n\n        # Batch size must be positive\n        if self.batch_size &lt; 1:\n            raise ValidationError(f\"batch_size must be positive, got {self.batch_size}\")\n\n        # Gradient clipping validation\n        if self.gradient_clip_val is not None and self.gradient_clip_val &lt;= 0:\n            raise ValidationError(\n                f\"gradient_clip_val must be positive or None, got {self.gradient_clip_val}\"\n            )\n\n    def validate_paths(self):\n        \"\"\"Validate file paths and permissions.\"\"\"\n        if self.checkpoint_path is not None:\n            # Ensure parent directory exists\n            parent = self.checkpoint_path.parent\n            if not parent.exists():\n                try:\n                    parent.mkdir(parents=True, exist_ok=True)\n                except Exception as e:\n                    raise ValidationError(\n                        f\"Cannot create checkpoint directory {parent}: {e}\"\n                    )\n\n            # Check write permissions\n            if parent.exists() and not os.access(parent, os.W_OK):\n                raise ValidationError(\n                    f\"No write permission for checkpoint directory: {parent}\"\n                )\n\n    def validate_dependencies(self):\n        \"\"\"Validate dependencies between configuration options.\"\"\"\n        # If using mixed precision, ensure compatible device\n        if hasattr(self, \"mixed_precision\") and self.mixed_precision:\n            if not torch.cuda.is_available():\n                raise ValidationError(\n                    \"Mixed precision training requires CUDA device\"\n                )\n\n            capability = torch.cuda.get_device_capability()\n            if self.mixed_precision == \"fp16\" and capability &lt; (7, 0):\n                raise ValidationError(\n                    f\"FP16 requires GPU capability &gt;= 7.0, got {capability}\"\n                )\n\n# Advanced constraint validation\n@dataclass\nclass ConstrainedConfig(DreamTrainerConfig):\n    \"\"\"Configuration with complex constraints.\"\"\"\n\n    # Constrained values\n    learning_rate_schedule: str = \"cosine\"\n    min_learning_rate: float = 1e-5\n    max_learning_rate: float = 1e-3\n\n    # Dependent parameters\n    use_weight_decay: bool = True\n    weight_decay: float = 0.1\n\n    # Resource constraints\n    max_memory_gb: float = 40.0\n    model_size: str = \"large\"\n    batch_size: int = 32\n\n    VALID_SCHEDULES = [\"cosine\", \"linear\", \"constant\", \"exponential\"]\n\n    MODEL_MEMORY_REQUIREMENTS = {\n        \"small\": 2.0,  # GB\n        \"base\": 6.0,\n        \"large\": 18.0,\n        \"xl\": 40.0,\n    }\n\n    def validate_constraints(self):\n        \"\"\"Validate complex constraints between parameters.\"\"\"\n        # Schedule validation\n        if self.learning_rate_schedule not in self.VALID_SCHEDULES:\n            raise ValidationError(\n                f\"Invalid schedule: {self.learning_rate_schedule}. \"\n                f\"Must be one of {self.VALID_SCHEDULES}\"\n            )\n\n        # Learning rate ordering\n        if self.min_learning_rate &gt;= self.max_learning_rate:\n            raise ValidationError(\n                f\"min_learning_rate ({self.min_learning_rate}) must be less than \"\n                f\"max_learning_rate ({self.max_learning_rate})\"\n            )\n\n        # Weight decay dependency\n        if not self.use_weight_decay and self.weight_decay != 0:\n            raise ValidationError(\n                \"weight_decay must be 0 when use_weight_decay is False\"\n            )\n\n        # Memory constraints\n        model_memory = self.MODEL_MEMORY_REQUIREMENTS.get(self.model_size, 0)\n        batch_memory = model_memory * self.batch_size / 32  # Scale by batch size\n\n        if batch_memory &gt; self.max_memory_gb:\n            raise ValidationError(\n                f\"Configuration requires ~{batch_memory:.1f}GB memory, \"\n                f\"but max_memory_gb is {self.max_memory_gb}. \"\n                f\"Reduce batch_size or use smaller model.\"\n            )\n</code></pre>"},{"location":"configuration-mastery/#type-safety-patterns","title":"Type Safety Patterns","text":"<p>Leverage Python's type system for compile-time safety and better IDE support.</p> <pre><code>from typing import TypeVar, Generic, Protocol, runtime_checkable\nfrom typing import Literal, Union, get_type_hints, get_origin, get_args\n\nT = TypeVar('T')\n\n@runtime_checkable\nclass Configurable(Protocol):\n    \"\"\"Protocol for objects that can be configured.\"\"\"\n\n    def configure(self, config: Any) -&gt; None:\n        \"\"\"Apply configuration to the object.\"\"\"\n        ...\n\n@dataclass\nclass TypedConfig(Generic[T]):\n    \"\"\"Generic typed configuration container.\"\"\"\n\n    value: T\n\n    def __post_init__(self):\n        \"\"\"Validate type at runtime.\"\"\"\n        expected_type = get_type_hints(self.__class__)['value']\n\n        if hasattr(expected_type, '__origin__'):\n            # Handle generic types like List[int], Dict[str, float], etc.\n            origin = get_origin(expected_type)\n            args = get_args(expected_type)\n\n            if not isinstance(self.value, origin):\n                raise TypeError(\n                    f\"Expected {origin}, got {type(self.value)}\"\n                )\n\n            # Additional validation for container types\n            if origin is list and args:\n                item_type = args[0]\n                for item in self.value:\n                    if not isinstance(item, item_type):\n                        raise TypeError(\n                            f\"List item {item} is not of type {item_type}\"\n                        )\n        else:\n            # Simple type check\n            if not isinstance(self.value, expected_type):\n                raise TypeError(\n                    f\"Expected {expected_type}, got {type(self.value)}\"\n                )\n\n# Strongly typed model configuration\n@dataclass\nclass ModelConfig:\n    \"\"\"Strongly typed model configuration.\"\"\"\n\n    architecture: Literal[\"transformer\", \"cnn\", \"rnn\"]\n    hidden_size: int\n    num_layers: int\n    activation: Literal[\"relu\", \"gelu\", \"swish\"]\n\n    # Use NewType for additional type safety\n    from typing import NewType\n\n    Dimension = NewType('Dimension', int)\n    BatchSize = NewType('BatchSize', int)\n\n    def validate_positive(self, value: int, name: str) -&gt; None:\n        \"\"\"Ensure positive integer values.\"\"\"\n        if value &lt;= 0:\n            raise ValueError(f\"{name} must be positive, got {value}\")\n\n    def __post_init__(self):\n        \"\"\"Validate all integer parameters.\"\"\"\n        self.validate_positive(self.hidden_size, \"hidden_size\")\n        self.validate_positive(self.num_layers, \"num_layers\")\n\n# Type-safe configuration builder\nclass ConfigBuilder(Generic[T]):\n    \"\"\"Fluent interface for building configurations.\"\"\"\n\n    def __init__(self, config_class: Type[T]):\n        self.config_class = config_class\n        self.params = {}\n\n    def set(self, **kwargs) -&gt; 'ConfigBuilder[T]':\n        \"\"\"Set configuration parameters.\"\"\"\n        # Validate against class annotations\n        hints = get_type_hints(self.config_class)\n\n        for key, value in kwargs.items():\n            if key not in hints:\n                raise ValueError(f\"Unknown parameter: {key}\")\n\n            expected_type = hints[key]\n            # Basic type checking (can be extended)\n            if not self._check_type(value, expected_type):\n                raise TypeError(\n                    f\"Parameter {key} expects {expected_type}, got {type(value)}\"\n                )\n\n            self.params[key] = value\n\n        return self\n\n    def _check_type(self, value: Any, expected_type: Type) -&gt; bool:\n        \"\"\"Check if value matches expected type.\"\"\"\n        # Handle Optional types\n        if get_origin(expected_type) is Union:\n            args = get_args(expected_type)\n            return any(self._check_type(value, arg) for arg in args)\n\n        # Handle Literal types\n        if get_origin(expected_type) is Literal:\n            return value in get_args(expected_type)\n\n        # Simple instance check\n        return isinstance(value, expected_type)\n\n    def build(self) -&gt; T:\n        \"\"\"Build the configuration object.\"\"\"\n        return self.config_class(**self.params)\n\n# Usage example\nconfig = (ConfigBuilder(ModelConfig)\n    .set(architecture=\"transformer\")\n    .set(hidden_size=768)\n    .set(num_layers=12)\n    .set(activation=\"gelu\")\n    .build())\n</code></pre>"},{"location":"configuration-mastery/#configuration-testing","title":"Configuration Testing","text":"<p>Implement comprehensive testing for your configurations to ensure they work correctly across different scenarios.</p> <pre><code>import unittest\nfrom unittest.mock import patch, MagicMock\nimport tempfile\nimport json\n\nclass ConfigTestCase(unittest.TestCase):\n    \"\"\"Base class for configuration testing.\"\"\"\n\n    def assertConfigValid(self, config: Any) -&gt; None:\n        \"\"\"Assert that a configuration is valid.\"\"\"\n        try:\n            # Attempt to create trainer with config\n            trainer = DreamTrainer(config)\n            # Attempt to access all properties\n            _ = config.__dict__\n        except Exception as e:\n            self.fail(f\"Configuration validation failed: {e}\")\n\n    def assertConfigInvalid(self, config_fn: Callable, error_pattern: str) -&gt; None:\n        \"\"\"Assert that a configuration raises expected error.\"\"\"\n        with self.assertRaisesRegex(ValidationError, error_pattern):\n            config_fn()\n\nclass TestModelConfiguration(ConfigTestCase):\n    \"\"\"Test model configuration validation.\"\"\"\n\n    def test_valid_configuration(self):\n        \"\"\"Test that valid configurations pass.\"\"\"\n        config = ValidatedConfig(\n            model_name=\"test_model\",\n            hidden_size=768,\n            num_heads=12,\n            num_layers=12,\n        )\n        self.assertConfigValid(config)\n\n    def test_invalid_hidden_size(self):\n        \"\"\"Test that invalid hidden size is caught.\"\"\"\n        self.assertConfigInvalid(\n            lambda: ValidatedConfig(\n                hidden_size=769,  # Not divisible by 12\n                num_heads=12,\n            ),\n            \"must be divisible by\"\n        )\n\n    def test_invalid_layer_count(self):\n        \"\"\"Test that invalid layer count is caught.\"\"\"\n        self.assertConfigInvalid(\n            lambda: ValidatedConfig(\n                num_layers=300,  # Too many\n            ),\n            \"should be between\"\n        )\n\nclass TestEnvironmentConfiguration(ConfigTestCase):\n    \"\"\"Test environment-based configuration.\"\"\"\n\n    @patch.dict(os.environ, {\"TRAINING_ENV\": \"production\"})\n    def test_production_environment(self):\n        \"\"\"Test production environment settings.\"\"\"\n        config = EnvironmentConfig()\n\n        self.assertEqual(config.environment, Environment.PRODUCTION)\n        self.assertTrue(config.compile_model)\n        self.assertEqual(config.num_workers, 16)\n        self.assertTrue(config.use_wandb)\n\n    @patch.dict(os.environ, {\"TRAINING_ENV\": \"local\"})\n    def test_local_environment(self):\n        \"\"\"Test local environment settings.\"\"\"\n        config = EnvironmentConfig()\n\n        self.assertEqual(config.environment, Environment.LOCAL)\n        self.assertFalse(config.compile_model)\n        self.assertEqual(config.num_workers, 0)\n        self.assertFalse(config.use_wandb)\n\n    @patch.dict(os.environ, {\"DEBUG\": \"1\"})\n    def test_debug_mode(self):\n        \"\"\"Test debug mode configuration.\"\"\"\n        config = ConditionalConfig()\n\n        self.assertTrue(config.debug_mode)\n        self.assertEqual(config.batch_size, 2)\n        self.assertEqual(config.checkpoint_frequency, 0.1)\n\nclass TestConfigurationSerialization(ConfigTestCase):\n    \"\"\"Test configuration serialization/deserialization.\"\"\"\n\n    def test_config_to_dict(self):\n        \"\"\"Test converting config to dictionary.\"\"\"\n        config = BaseExperimentConfig(\n            project=\"test_project\",\n            group=\"test_group\",\n            learning_rate=1e-4,\n        )\n\n        config_dict = asdict(config)\n\n        self.assertEqual(config_dict[\"project\"], \"test_project\")\n        self.assertEqual(config_dict[\"learning_rate\"], 1e-4)\n\n    def test_config_from_json(self):\n        \"\"\"Test loading configuration from JSON.\"\"\"\n        config_data = {\n            \"project\": \"test_project\",\n            \"group\": \"test_group\",\n            \"experiment\": \"test_exp\",\n            \"learning_rate\": 0.0001,\n        }\n\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json') as f:\n            json.dump(config_data, f)\n            f.flush()\n\n            # Load config from file\n            loaded_data = json.load(open(f.name))\n            config = BaseExperimentConfig(**loaded_data)\n\n            self.assertEqual(config.project, \"test_project\")\n            self.assertEqual(config.learning_rate, 0.0001)\n\nclass TestConfigurationIntegration(ConfigTestCase):\n    \"\"\"Integration tests for configurations.\"\"\"\n\n    @patch('torch.cuda.is_available')\n    @patch('torch.cuda.get_device_properties')\n    def test_gpu_adaptive_config(self, mock_props, mock_available):\n        \"\"\"Test GPU-adaptive configuration.\"\"\"\n        mock_available.return_value = True\n        mock_device = MagicMock()\n        mock_device.total_memory = 40 * 1e9  # 40GB\n        mock_props.return_value = mock_device\n\n        config = ConditionalConfig()\n\n        self.assertEqual(config.batch_size, 32)\n        self.assertIsNotNone(config.mixed_precision)\n\n    def test_multi_experiment_configs(self):\n        \"\"\"Test multiple experiment configurations.\"\"\"\n        configs = [\n            SmallBaselineConfig(),\n            LargeScaleConfig(),\n            AblationNoWarmupConfig(),\n        ]\n\n        # Ensure all configs are valid\n        for config in configs:\n            self.assertConfigValid(config)\n\n        # Ensure they have different settings\n        self.assertNotEqual(configs[0].learning_rate, configs[1].learning_rate)\n        self.assertNotEqual(configs[0].num_layers, configs[1].num_layers)\n        self.assertEqual(configs[2].warmup_steps, 0)\n\n# Property-based testing with hypothesis\nfrom hypothesis import given, strategies as st, assume\n\nclass TestPropertyBasedConfig(ConfigTestCase):\n    \"\"\"Property-based testing for configurations.\"\"\"\n\n    @given(\n        hidden_size=st.integers(min_value=64, max_value=4096),\n        num_heads=st.integers(min_value=1, max_value=64),\n    )\n    def test_hidden_size_divisibility(self, hidden_size, num_heads):\n        \"\"\"Test that valid hidden sizes are always divisible by num_heads.\"\"\"\n        assume(hidden_size % num_heads == 0)\n\n        config = ValidatedConfig(\n            hidden_size=hidden_size,\n            num_heads=num_heads,\n        )\n\n        self.assertConfigValid(config)\n\n    @given(\n        lr=st.floats(min_value=1e-6, max_value=1e-1),\n        batch_size=st.integers(min_value=1, max_value=512),\n    )\n    def test_training_parameters_ranges(self, lr, batch_size):\n        \"\"\"Test that training parameters within valid ranges work.\"\"\"\n        config = ValidatedConfig(\n            learning_rate=lr,\n            batch_size=batch_size,\n        )\n\n        self.assertConfigValid(config)\n</code></pre>"},{"location":"configuration-mastery/#real-world-examples","title":"Real-World Examples","text":""},{"location":"configuration-mastery/#multi-node-configurations","title":"Multi-Node Configurations","text":"<p>Configure training across multiple nodes with proper fault tolerance and communication settings.</p> <pre><code>@dataclass\nclass MultiNodeConfig(DreamTrainerConfig):\n    \"\"\"Configuration for multi-node distributed training.\"\"\"\n\n    # Node configuration\n    num_nodes: int = 1\n    node_rank: int = 0\n    master_addr: str = \"localhost\"\n    master_port: int = 29500\n\n    # Communication settings\n    nccl_debug: str = field(default_factory=lambda: os.getenv(\"NCCL_DEBUG\", \"INFO\"))\n    nccl_timeout: int = 1800  # 30 minutes\n\n    # Fault tolerance\n    enable_fault_tolerance: bool = True\n    checkpoint_frequency: float = 0.25  # Every 25% of epoch\n    resume_from_checkpoint: bool = True\n\n    def __post_init__(self):\n        \"\"\"Configure multi-node settings.\"\"\"\n        # Set environment variables for PyTorch distributed\n        os.environ[\"MASTER_ADDR\"] = self.master_addr\n        os.environ[\"MASTER_PORT\"] = str(self.master_port)\n        os.environ[\"WORLD_SIZE\"] = str(self.num_nodes * self.gpus_per_node)\n        os.environ[\"RANK\"] = str(self.node_rank * self.gpus_per_node + self.local_rank)\n\n        # NCCL optimizations\n        os.environ[\"NCCL_DEBUG\"] = self.nccl_debug\n        os.environ[\"NCCL_TREE_THRESHOLD\"] = \"0\"\n\n        # InfiniBand optimizations (if available)\n        if self.has_infiniband():\n            os.environ[\"NCCL_IB_DISABLE\"] = \"0\"\n            os.environ[\"NCCL_IB_GID_INDEX\"] = \"3\"\n            os.environ[\"NCCL_IB_TIMEOUT\"] = \"22\"\n\n        # Configure device parameters for multi-node\n        self.device_parameters = self.configure_multi_node_devices()\n\n    @property\n    def gpus_per_node(self) -&gt; int:\n        \"\"\"Get number of GPUs per node.\"\"\"\n        return torch.cuda.device_count() if torch.cuda.is_available() else 1\n\n    @property\n    def local_rank(self) -&gt; int:\n        \"\"\"Get local rank within the node.\"\"\"\n        return int(os.getenv(\"LOCAL_RANK\", 0))\n\n    def has_infiniband(self) -&gt; bool:\n        \"\"\"Check if InfiniBand is available.\"\"\"\n        try:\n            import subprocess\n            result = subprocess.run([\"ibstat\"], capture_output=True)\n            return result.returncode == 0\n        except:\n            return False\n\n    def configure_multi_node_devices(self) -&gt; DeviceParameters:\n        \"\"\"Configure device parameters for multi-node training.\"\"\"\n        total_gpus = self.num_nodes * self.gpus_per_node\n\n        # Automatic sharding configuration\n        if total_gpus &gt;= 64:\n            # Large scale: use HSDP\n            return DeviceParameters(\n                dp_shard=8,  # FSDP within groups of 8\n                dp_replicate=total_gpus // 8,  # Replicate across groups\n                tensor_parallel=1,\n                compile_model=True,\n                comm={\n                    \"backend\": \"nccl\",\n                    \"timeout\": self.nccl_timeout,\n                },\n            )\n        elif total_gpus &gt;= 16:\n            # Medium scale: use FSDP with TP\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                tensor_parallel=4,\n                compile_model=True,\n                async_tensor_parallel=True,\n                comm={\n                    \"backend\": \"nccl\",\n                    \"timeout\": self.nccl_timeout,\n                },\n            )\n        else:\n            # Small scale: simple FSDP\n            return DeviceParameters(\n                dp_shard=\"auto\",\n                compile_model=True,\n                comm={\n                    \"backend\": \"nccl\",\n                    \"timeout\": self.nccl_timeout,\n                },\n            )\n\n# SLURM-specific configuration\n@dataclass\nclass SlurmConfig(MultiNodeConfig):\n    \"\"\"Configuration for SLURM-managed clusters.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Auto-detect SLURM environment.\"\"\"\n        if slurm_nodeid := os.getenv(\"SLURM_NODEID\"):\n            self.node_rank = int(slurm_nodeid)\n\n        if slurm_nnodes := os.getenv(\"SLURM_NNODES\"):\n            self.num_nodes = int(slurm_nnodes)\n\n        if slurm_step_nodelist := os.getenv(\"SLURM_STEP_NODELIST\"):\n            # Parse first node as master\n            import re\n            match = re.match(r'([^,\\[]+)', slurm_step_nodelist)\n            if match:\n                self.master_addr = match.group(1)\n\n        # Set port based on job ID to avoid conflicts\n        if slurm_jobid := os.getenv(\"SLURM_JOBID\"):\n            self.master_port = 29500 + int(slurm_jobid) % 1000\n\n        super().__post_init__()\n</code></pre>"},{"location":"configuration-mastery/#ab-testing-setups","title":"A/B Testing Setups","text":"<p>Configure experiments for A/B testing different model architectures, training strategies, or hyperparameters.</p> <pre><code>@dataclass\nclass ABTestConfig(DreamTrainerConfig):\n    \"\"\"Base configuration for A/B testing.\"\"\"\n\n    # Test identification\n    test_name: str\n    variant: Literal[\"A\", \"B\"]\n\n    # Shared settings for fair comparison\n    random_seed: int = 42\n    dataset_seed: int = 123\n    num_epochs: int = 10\n    eval_metric: str = \"perplexity\"\n\n    # Logging for comparison\n    use_wandb: bool = True\n    wandb_project: str = \"ab_tests\"\n\n    def __post_init__(self):\n        \"\"\"Set up variant-specific configuration.\"\"\"\n        self.experiment = f\"{self.test_name}_{self.variant}\"\n        self.wandb_group = self.test_name\n        self.wandb_tags = [self.test_name, f\"variant_{self.variant}\"]\n\n@dataclass\nclass ArchitectureABTest(ABTestConfig):\n    \"\"\"A/B test comparing different architectures.\"\"\"\n\n    test_name: str = \"architecture_comparison\"\n\n    def get_model_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get variant-specific model configuration.\"\"\"\n        if self.variant == \"A\":\n            # Variant A: Traditional transformer\n            return {\n                \"architecture\": \"transformer\",\n                \"hidden_size\": 768,\n                \"num_heads\": 12,\n                \"num_layers\": 12,\n                \"ffn_multiplier\": 4,\n                \"activation\": \"gelu\",\n                \"use_rope\": False,\n            }\n        else:\n            # Variant B: Modern improvements\n            return {\n                \"architecture\": \"transformer\",\n                \"hidden_size\": 768,\n                \"num_heads\": 12,\n                \"num_layers\": 12,\n                \"ffn_multiplier\": 3,  # SwiGLU uses 3x\n                \"activation\": \"swiglu\",\n                \"use_rope\": True,\n                \"rope_theta\": 10000,\n            }\n\n@dataclass\nclass OptimizerABTest(ABTestConfig):\n    \"\"\"A/B test comparing different optimizers.\"\"\"\n\n    test_name: str = \"optimizer_comparison\"\n\n    # Same model for both variants\n    model_config: Dict[str, Any] = field(default_factory=lambda: {\n        \"hidden_size\": 768,\n        \"num_heads\": 12,\n        \"num_layers\": 12,\n    })\n\n    def get_optimizer_config(self) -&gt; Callable:\n        \"\"\"Get variant-specific optimizer configuration.\"\"\"\n        if self.variant == \"A\":\n            # Variant A: Standard AdamW\n            return partial(\n                torch.optim.AdamW,\n                lr=3e-4,\n                betas=(0.9, 0.999),\n                weight_decay=0.1,\n                eps=1e-8,\n            )\n        else:\n            # Variant B: Modified AdamW with different betas\n            return partial(\n                torch.optim.AdamW,\n                lr=3e-4,\n                betas=(0.95, 0.98),  # GPT-3 style\n                weight_decay=0.05,\n                eps=1e-8,\n            )\n\n@dataclass\nclass TrainingStrategyABTest(ABTestConfig):\n    \"\"\"A/B test comparing different training strategies.\"\"\"\n\n    test_name: str = \"training_strategy\"\n\n    def configure_training(self) -&gt; Dict[str, Any]:\n        \"\"\"Get variant-specific training configuration.\"\"\"\n        if self.variant == \"A\":\n            # Variant A: Standard training\n            return {\n                \"gradient_accumulation_steps\": 1,\n                \"gradient_clip_val\": 1.0,\n                \"warmup_steps\": 2000,\n                \"scheduler\": \"cosine\",\n                \"use_mixed_precision\": True,\n                \"mixed_precision_type\": \"fp16\",\n            }\n        else:\n            # Variant B: Advanced training\n            return {\n                \"gradient_accumulation_steps\": 4,\n                \"gradient_clip_val\": 0.5,\n                \"warmup_steps\": 4000,\n                \"scheduler\": \"cosine_with_restarts\",\n                \"use_mixed_precision\": True,\n                \"mixed_precision_type\": \"bf16\",\n                \"use_gradient_checkpointing\": True,\n            }\n\n# A/B test runner\nclass ABTestRunner:\n    \"\"\"Runner for A/B tests with statistical analysis.\"\"\"\n\n    def __init__(self, test_config_class: Type[ABTestConfig]):\n        self.test_config_class = test_config_class\n        self.results = {\"A\": [], \"B\": []}\n\n    def run_test(self, num_runs: int = 3) -&gt; Dict[str, Any]:\n        \"\"\"Run A/B test with multiple seeds.\"\"\"\n        for run in range(num_runs):\n            for variant in [\"A\", \"B\"]:\n                config = self.test_config_class(\n                    variant=variant,\n                    random_seed=42 + run,  # Different seed per run\n                )\n\n                # Run training\n                trainer = DreamTrainer(config)\n                trainer.fit()\n\n                # Collect metrics\n                final_metric = trainer.get_metric(config.eval_metric)\n                self.results[variant].append(final_metric)\n\n        # Statistical analysis\n        return self.analyze_results()\n\n    def analyze_results(self) -&gt; Dict[str, Any]:\n        \"\"\"Perform statistical analysis of A/B test results.\"\"\"\n        from scipy import stats\n        import numpy as np\n\n        a_results = np.array(self.results[\"A\"])\n        b_results = np.array(self.results[\"B\"])\n\n        # T-test\n        t_stat, p_value = stats.ttest_ind(a_results, b_results)\n\n        # Effect size (Cohen's d)\n        pooled_std = np.sqrt((np.var(a_results) + np.var(b_results)) / 2)\n        effect_size = (np.mean(b_results) - np.mean(a_results)) / pooled_std\n\n        return {\n            \"variant_A\": {\n                \"mean\": np.mean(a_results),\n                \"std\": np.std(a_results),\n                \"results\": a_results.tolist(),\n            },\n            \"variant_B\": {\n                \"mean\": np.mean(b_results),\n                \"std\": np.std(b_results),\n                \"results\": b_results.tolist(),\n            },\n            \"statistics\": {\n                \"t_statistic\": t_stat,\n                \"p_value\": p_value,\n                \"effect_size\": effect_size,\n                \"significant\": p_value &lt; 0.05,\n                \"better_variant\": \"B\" if np.mean(b_results) &lt; np.mean(a_results) else \"A\",\n            },\n        }\n</code></pre>"},{"location":"configuration-mastery/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Configure and run hyperparameter sweeps with different search strategies.</p> <pre><code>from typing import Tuple\nimport itertools\nimport random\n\n@dataclass\nclass SweepConfig(DreamTrainerConfig):\n    \"\"\"Base configuration for hyperparameter sweeps.\"\"\"\n\n    sweep_name: str\n    sweep_id: str = field(default_factory=lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n\n    # Hyperparameter bounds\n    learning_rate_range: Tuple[float, float] = (1e-5, 1e-3)\n    batch_size_options: List[int] = field(default_factory=lambda: [16, 32, 64])\n    weight_decay_range: Tuple[float, float] = (0.0, 0.3)\n    warmup_ratio_range: Tuple[float, float] = (0.0, 0.1)\n\n    # Search configuration\n    search_strategy: Literal[\"grid\", \"random\", \"bayesian\"] = \"random\"\n    num_trials: int = 20\n\n    # Tracking\n    use_wandb: bool = True\n    wandb_project: str = \"hyperparameter_sweeps\"\n\nclass GridSearchConfig(SweepConfig):\n    \"\"\"Grid search configuration.\"\"\"\n\n    search_strategy: Literal[\"grid\", \"random\", \"bayesian\"] = \"grid\"\n\n    def generate_configs(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Generate all combinations for grid search.\"\"\"\n        # Define discrete values for grid\n        learning_rates = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3]\n        batch_sizes = self.batch_size_options\n        weight_decays = [0.0, 0.01, 0.1, 0.2]\n        warmup_ratios = [0.0, 0.05, 0.1]\n\n        # Generate all combinations\n        combinations = itertools.product(\n            learning_rates,\n            batch_sizes,\n            weight_decays,\n            warmup_ratios,\n        )\n\n        configs = []\n        for lr, bs, wd, wr in combinations:\n            configs.append({\n                \"learning_rate\": lr,\n                \"batch_size\": bs,\n                \"weight_decay\": wd,\n                \"warmup_ratio\": wr,\n                \"config_id\": f\"lr{lr}_bs{bs}_wd{wd}_wr{wr}\",\n            })\n\n        return configs\n\nclass RandomSearchConfig(SweepConfig):\n    \"\"\"Random search configuration.\"\"\"\n\n    search_strategy: Literal[\"grid\", \"random\", \"bayesian\"] = \"random\"\n\n    def generate_configs(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Generate random configurations.\"\"\"\n        configs = []\n\n        for i in range(self.num_trials):\n            # Sample hyperparameters\n            lr = 10 ** random.uniform(\n                np.log10(self.learning_rate_range[0]),\n                np.log10(self.learning_rate_range[1])\n            )\n            bs = random.choice(self.batch_size_options)\n            wd = random.uniform(*self.weight_decay_range)\n            wr = random.uniform(*self.warmup_ratio_range)\n\n            configs.append({\n                \"learning_rate\": lr,\n                \"batch_size\": bs,\n                \"weight_decay\": wd,\n                \"warmup_ratio\": wr,\n                \"config_id\": f\"trial_{i:03d}\",\n            })\n\n        return configs\n\nclass BayesianSearchConfig(SweepConfig):\n    \"\"\"Bayesian optimization configuration.\"\"\"\n\n    search_strategy: Literal[\"grid\", \"random\", \"bayesian\"] = \"bayesian\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Initialize Bayesian optimization\n        from skopt import gp_minimize\n        from skopt.space import Real, Integer, Categorical\n\n        self.search_space = [\n            Real(np.log10(self.learning_rate_range[0]), \n                 np.log10(self.learning_rate_range[1]), \n                 name='log_learning_rate'),\n            Categorical(self.batch_size_options, name='batch_size'),\n            Real(*self.weight_decay_range, name='weight_decay'),\n            Real(*self.warmup_ratio_range, name='warmup_ratio'),\n        ]\n\n        self.optimization_results = []\n\n    def suggest_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Suggest next configuration using Bayesian optimization.\"\"\"\n        if len(self.optimization_results) &lt; 5:\n            # Random exploration for first few trials\n            return RandomSearchConfig.generate_configs(self)[0]\n\n        # Use Gaussian Process to suggest next point\n        from skopt import gp_minimize\n\n        X = [r['params'] for r in self.optimization_results]\n        y = [r['objective'] for r in self.optimization_results]\n\n        # Get suggestion (this is simplified, real implementation would be more complex)\n        suggestion = self._get_suggestion(X, y)\n\n        return {\n            \"learning_rate\": 10 ** suggestion[0],\n            \"batch_size\": suggestion[1],\n            \"weight_decay\": suggestion[2],\n            \"warmup_ratio\": suggestion[3],\n            \"config_id\": f\"bayes_{len(self.optimization_results):03d}\",\n        }\n\n    def update_results(self, config: Dict[str, Any], objective: float):\n        \"\"\"Update Bayesian optimization with results.\"\"\"\n        self.optimization_results.append({\n            \"params\": [\n                np.log10(config[\"learning_rate\"]),\n                config[\"batch_size\"],\n                config[\"weight_decay\"],\n                config[\"warmup_ratio\"],\n            ],\n            \"objective\": objective,\n        })\n\n# Hyperparameter sweep runner\nclass SweepRunner:\n    \"\"\"Runner for hyperparameter sweeps.\"\"\"\n\n    def __init__(self, base_config: Type[DreamTrainerConfig], sweep_config: SweepConfig):\n        self.base_config = base_config\n        self.sweep_config = sweep_config\n        self.results = []\n\n    def run_sweep(self) -&gt; pd.DataFrame:\n        \"\"\"Run the complete hyperparameter sweep.\"\"\"\n        if self.sweep_config.search_strategy == \"grid\":\n            configs = GridSearchConfig(**asdict(self.sweep_config)).generate_configs()\n        elif self.sweep_config.search_strategy == \"random\":\n            configs = RandomSearchConfig(**asdict(self.sweep_config)).generate_configs()\n        else:\n            # Bayesian optimization\n            bayes_config = BayesianSearchConfig(**asdict(self.sweep_config))\n            configs = []\n\n            for _ in range(self.sweep_config.num_trials):\n                config = bayes_config.suggest_config()\n                result = self.run_trial(config)\n                bayes_config.update_results(config, result[\"objective\"])\n                configs.append(config)\n\n        # Run all trials\n        for config in configs:\n            result = self.run_trial(config)\n            self.results.append(result)\n\n        # Convert to DataFrame for analysis\n        return pd.DataFrame(self.results)\n\n    def run_trial(self, hyperparams: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Run a single trial with given hyperparameters.\"\"\"\n        # Create configuration\n        config = self.base_config(\n            experiment=f\"{self.sweep_config.sweep_name}_{hyperparams['config_id']}\",\n            learning_rate=hyperparams[\"learning_rate\"],\n            batch_size=hyperparams[\"batch_size\"], \n            weight_decay=hyperparams[\"weight_decay\"],\n            warmup_ratio=hyperparams[\"warmup_ratio\"],\n            wandb_group=self.sweep_config.sweep_name,\n            wandb_tags=[self.sweep_config.sweep_name, hyperparams['config_id']],\n        )\n\n        # Train model\n        trainer = DreamTrainer(config)\n        trainer.fit()\n\n        # Get final metrics\n        final_loss = trainer.get_metric(\"val_loss\")\n        final_perplexity = trainer.get_metric(\"perplexity\")\n\n        return {\n            **hyperparams,\n            \"final_loss\": final_loss,\n            \"final_perplexity\": final_perplexity,\n            \"objective\": final_perplexity,  # Metric to optimize\n            \"training_time\": trainer.training_time,\n        }\n\n    def analyze_results(self) -&gt; Dict[str, Any]:\n        \"\"\"Analyze sweep results.\"\"\"\n        df = pd.DataFrame(self.results)\n\n        # Find best configuration\n        best_idx = df[\"objective\"].argmin()\n        best_config = df.iloc[best_idx]\n\n        # Analyze parameter importance\n        correlations = {}\n        for param in [\"learning_rate\", \"batch_size\", \"weight_decay\", \"warmup_ratio\"]:\n            if param in df.columns:\n                correlations[param] = df[param].corr(df[\"objective\"])\n\n        return {\n            \"best_config\": best_config.to_dict(),\n            \"parameter_correlations\": correlations,\n            \"results_summary\": {\n                \"mean_objective\": df[\"objective\"].mean(),\n                \"std_objective\": df[\"objective\"].std(),\n                \"best_objective\": df[\"objective\"].min(),\n                \"worst_objective\": df[\"objective\"].max(),\n            },\n        }\n</code></pre>"},{"location":"configuration-mastery/#summary","title":"Summary","text":"<p>This configuration mastery guide demonstrates how Dream Trainer's Python-based configuration system enables:</p> <ol> <li>Dynamic Configuration: Use factory functions and runtime computation to create adaptive configurations</li> <li>Type Safety: Leverage Python's type system for compile-time checks and better IDE support</li> <li>Validation: Implement comprehensive validation to catch errors early</li> <li>Environment Adaptation: Automatically adjust settings based on runtime environment</li> <li>Experiment Management: Organize and run complex experiments with proper tracking</li> <li>Testing: Ensure configuration correctness with comprehensive testing</li> </ol> <p>The power of configs-as-code allows you to build maintainable, scalable, and production-ready training pipelines that adapt to different environments and requirements while maintaining type safety and validation throughout. </p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide explains all configuration options available in Dream Trainer.</p>"},{"location":"configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Configuration Philosophy: Code Over YAML</li> <li>Basic Configuration</li> <li>Device Parameters</li> <li>Training Parameters</li> <li>Checkpoint Parameters</li> <li>Logging Parameters</li> <li>Advanced Configuration</li> </ul>"},{"location":"configuration/#configuration-philosophy-code-over-yaml","title":"Configuration Philosophy: Code Over YAML","text":"<p>Dream Trainer uses Python code for configuration instead of YAML/JSON files. This design choice provides significant advantages:</p>"},{"location":"configuration/#why-configs-as-code","title":"Why Configs as Code?","text":"<pre><code># Traditional YAML approach - many pitfalls\n# config.yaml:\n# model:\n#   name: \"gpt2-medium\"  # Is this string correct?\n#   layers: \"24\"         # Should this be a string or int?\n#   lr: 0.0003          # Easy to add/remove zeros by mistake\n#   dropout: 0.1\n#   use_flash_attn: yes  # yes, true, True, or 1?\n\n# Dream Trainer approach - type-safe Python code\nfrom dataclasses import dataclass\nfrom typing import Literal, Optional\nfrom dream_trainer import BaseTrainerConfig\n\n@dataclass\nclass GPT2Config(BaseTrainerConfig):\n    \"\"\"Configuration for GPT-2 training with full type safety.\"\"\"\n\n    # Model architecture - with type hints and validation\n    model_size: Literal[\"small\", \"medium\", \"large\", \"xl\"] = \"medium\"\n    num_layers: int = 24\n    hidden_size: int = 1024\n    num_heads: int = 16\n\n    # Training hyperparameters\n    learning_rate: float = 3e-4  # Scientific notation preserved\n    dropout: float = 0.1\n    use_flash_attention: bool = True  # Clear boolean type\n\n    def __post_init__(self):\n        \"\"\"Validate configuration consistency.\"\"\"\n        # Ensure model size matches architecture\n        size_configs = {\n            \"small\": (12, 768, 12),\n            \"medium\": (24, 1024, 16),\n            \"large\": (36, 1280, 20),\n            \"xl\": (48, 1600, 25)\n        }\n\n        expected = size_configs[self.model_size]\n        if (self.num_layers, self.hidden_size, self.num_heads) != expected:\n            print(f\"Warning: Custom architecture differs from {self.model_size}\")\n</code></pre>"},{"location":"configuration/#key-benefits","title":"Key Benefits","text":"<ol> <li> <p>Type Safety and IDE Support <pre><code>config = GPT2Config(\n    model_size=\"medium\",  # IDE shows valid options\n    learning_rate=3e-4,   # Type-checked as float\n    use_flash_attention=True  # Clear boolean, not string\n)\n\n# IDE provides autocomplete and catches errors\nconfig.learning_rate = \"high\"  # \u274c Type error caught immediately\n</code></pre></p> </li> <li> <p>Composability with Functions <pre><code>from functools import partial\n\ndef create_optimizer_config(model_size: str):\n    \"\"\"Factory function for optimizer configs based on model size.\"\"\"\n    base_lr = {\"small\": 6e-4, \"medium\": 3e-4, \"large\": 2.5e-4}\n    return partial(\n        torch.optim.AdamW,\n        lr=base_lr.get(model_size, 3e-4),\n        betas=(0.9, 0.95),\n        weight_decay=0.1\n    )\n\n# Use in configuration\nconfig = GPT2Config(\n    model_size=\"large\",\n    optimizer=create_optimizer_config(\"large\")\n)\n</code></pre></p> </li> <li> <p>Dynamic Configuration <pre><code>import os\nfrom pathlib import Path\n\n@dataclass\nclass DataConfig(BaseConfig):\n    \"\"\"Dynamically configured data settings.\"\"\"\n\n    # Paths can be computed at runtime\n    data_root: Path = Path(os.environ.get(\"DATA_ROOT\", \"./data\"))\n\n    # Conditional configuration\n    batch_size: int = field(default_factory=lambda: \n        32 if torch.cuda.device_count() &lt;= 4 else 64\n    )\n\n    # Computed properties\n    @property\n    def train_path(self) -&gt; Path:\n        return self.data_root / \"train\"\n\n    @property\n    def val_path(self) -&gt; Path:\n        return self.data_root / \"validation\"\n</code></pre></p> </li> <li> <p>Configuration Inheritance <pre><code># Base configuration for all experiments\n@dataclass\nclass BaseExperimentConfig(DreamTrainerConfig):\n    project: str = \"gpt2-experiments\"\n    learning_rate: float = 3e-4\n    weight_decay: float = 0.1\n    warmup_steps: int = 1000\n\n# Specific experiment configurations\n@dataclass \nclass SmallModelConfig(BaseExperimentConfig):\n    \"\"\"Config for debugging on small model.\"\"\"\n    model_size: str = \"small\"\n    batch_size: int = 8\n    learning_rate: float = 6e-4  # Override base\n\n@dataclass\nclass ProductionConfig(BaseExperimentConfig):\n    \"\"\"Config for production training.\"\"\"\n    model_size: str = \"large\"\n    batch_size: int = 256\n    compile_model: bool = True\n    enable_mixed_precision: bool = True\n</code></pre></p> </li> <li> <p>Validation and Documentation <pre><code>@dataclass\nclass ValidatedConfig(BaseConfig):\n    \"\"\"Configuration with built-in validation and documentation.\"\"\"\n\n    learning_rate: float = 3e-4\n    \"\"\"Learning rate for AdamW optimizer. \n    Typically 3e-4 for small models, 2e-4 for large.\"\"\"\n\n    gradient_clip: float = 1.0\n    \"\"\"Gradient clipping value. Set to 0 to disable.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Validate configuration values.\"\"\"\n        if not 0 &lt; self.learning_rate &lt; 1:\n            raise ValueError(f\"Invalid learning rate: {self.learning_rate}\")\n\n        if self.gradient_clip &lt; 0:\n            raise ValueError(\"Gradient clip must be non-negative\")\n</code></pre></p> </li> </ol>"},{"location":"configuration/#real-world-example","title":"Real-World Example","text":"<p>Here's how the Llama3 example uses configs as code:</p> <pre><code>from functools import partial\nfrom dream_trainer import callbacks\nfrom dream_trainer.configs import TrainingParameters, DeviceParameters\n\n# Modular configuration with clear types\nconfig = StudentTrainerConfig(\n    # Project metadata\n    project=\"llama3-training\",\n    group=\"baseline\",\n\n    # Model configuration using factories\n    model=partial(LlamaModel, num_heads=32, rope_theta=10_000),\n\n    # Device configuration with helper methods\n    device_parameters=DeviceParameters.FSDP(\n        tensor_parallel=4,\n        compile_model=True,\n    ),\n\n    # Training configuration with validation\n    training_parameters=TrainingParameters(\n        n_epochs=10,\n        train_batch_size=32,\n        gradient_accumulation_steps=calculate_grad_accum_steps(),\n    ),\n\n    # Composable callbacks\n    callbacks=callbacks.CallbackCollection([\n        callbacks.LoggerCallback(code_dir=\"../\"),\n        callbacks.CheckpointCallback(monitor=\"val_loss\", mode=\"min\"),\n        callbacks.ProfileCallback() if DEBUG else None,  # Conditional\n    ].filter(None))  # Remove None values\n)\n</code></pre>"},{"location":"configuration/#migration-from-yaml","title":"Migration from YAML","text":"<p>If you're coming from YAML-based configs:</p> <pre><code># Old YAML approach\n# with open(\"config.yaml\") as f:\n#     config = yaml.safe_load(f)\n# model = Model(**config[\"model\"])  # No type checking!\n\n# Dream Trainer approach\nconfig = MyTrainerConfig.from_file(\"config.py\")  # If needed\n# Or better, just import it:\nfrom configs.experiment import production_config\nmodel = Model(config.model)  # Full type safety!\n</code></pre>"},{"location":"configuration/#basic-configuration","title":"Basic Configuration","text":"<p>The main configuration class is <code>DreamTrainerConfig</code>. Here's a basic example:</p> <pre><code>from dream_trainer import DreamTrainerConfig\n\nconfig = DreamTrainerConfig(\n    project=\"my-project\",\n    group=\"experiments\",\n    experiment=\"run-001\"\n)\n</code></pre>"},{"location":"configuration/#project-settings","title":"Project Settings","text":"Parameter Type Description <code>project</code> str Project name for organization <code>group</code> str Group name for related experiments <code>experiment</code> str Unique experiment identifier"},{"location":"configuration/#device-parameters","title":"Device Parameters","text":"<p>Configure hardware and distributed training settings:</p> <pre><code>from dream_trainer.configs import DeviceParameters\nimport torch\n\ndevice_params = DeviceParameters(\n    # Distributed training\n    data_parallel_size=1,\n    tensor_parallel_size=1,\n    pipeline_parallel_size=1,\n    context_parallel_size=1,\n\n    # Performance\n    compile_model=True,\n    param_dtype=torch.bfloat16,\n    activation_dtype=torch.bfloat16,\n\n    # Memory optimization\n    checkpoint_activations=False,\n    offload_optimizer=False,\n    offload_parameters=False\n)\n</code></pre>"},{"location":"configuration/#distributed-training","title":"Distributed Training","text":"Parameter Type Description <code>data_parallel_size</code> int Number of GPUs for data parallelism <code>tensor_parallel_size</code> int Tensor parallelism degree <code>pipeline_parallel_size</code> int Pipeline parallelism degree <code>context_parallel_size</code> int Context parallelism degree"},{"location":"configuration/#performance-settings","title":"Performance Settings","text":"Parameter Type Description <code>compile_model</code> bool Use torch.compile for optimization <code>param_dtype</code> torch.dtype Parameter data type (e.g., bfloat16) <code>activation_dtype</code> torch.dtype Activation data type"},{"location":"configuration/#memory-optimization","title":"Memory Optimization","text":"Parameter Type Description <code>checkpoint_activations</code> bool Enable activation checkpointing <code>offload_optimizer</code> bool Offload optimizer states to CPU <code>offload_parameters</code> bool Offload parameters to CPU"},{"location":"configuration/#training-parameters","title":"Training Parameters","text":"<p>Configure training loop settings:</p> <pre><code>from dream_trainer.configs import TrainingParameters\n\ntraining_params = TrainingParameters(\n    # Basic training\n    n_epochs=10,\n    train_batch_size=32,\n    val_batch_size=32,\n\n    # Optimization\n    gradient_clip_val=1.0,\n    gradient_accumulation_steps=1,\n    max_grad_norm=1.0,\n\n    # Validation\n    val_frequency=0.5,\n    num_sanity_val_steps=2,\n\n    # Learning rate\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    warmup_steps=1000\n)\n</code></pre>"},{"location":"configuration/#basic-training","title":"Basic Training","text":"Parameter Type Description <code>n_epochs</code> int Number of training epochs <code>train_batch_size</code> int Training batch size <code>val_batch_size</code> int Validation batch size"},{"location":"configuration/#optimization","title":"Optimization","text":"Parameter Type Description <code>gradient_clip_val</code> float Gradient clipping value <code>gradient_accumulation_steps</code> int Steps for gradient accumulation <code>max_grad_norm</code> float Maximum gradient norm"},{"location":"configuration/#validation","title":"Validation","text":"Parameter Type Description <code>val_frequency</code> float Validation frequency (epochs) <code>num_sanity_val_steps</code> int Sanity check steps"},{"location":"configuration/#learning-rate","title":"Learning Rate","text":"Parameter Type Description <code>learning_rate</code> float Initial learning rate <code>weight_decay</code> float Weight decay coefficient <code>warmup_steps</code> int Learning rate warmup steps"},{"location":"configuration/#checkpoint-parameters","title":"Checkpoint Parameters","text":"<p>Configure model checkpointing:</p> <pre><code>from dream_trainer.configs import CheckpointParameters\n\ncheckpoint_params = CheckpointParameters(\n    # Basic settings\n    root_dir=\"./checkpoints\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n\n    # Checkpoint frequency\n    checkpoint_every_n_epochs=1,\n    checkpoint_every_n_steps=None,\n\n    # Checkpoint management\n    keep_top_k=3,\n    save_last=True,\n\n    # Resume settings\n    resume_mode=\"latest\",  # or \"best\"\n    resume_path=None\n)\n</code></pre>"},{"location":"configuration/#basic-settings","title":"Basic Settings","text":"Parameter Type Description <code>root_dir</code> str Checkpoint directory <code>monitor</code> str Metric to monitor <code>mode</code> str \"min\" or \"max\" for monitored metric"},{"location":"configuration/#checkpoint-frequency","title":"Checkpoint Frequency","text":"Parameter Type Description <code>checkpoint_every_n_epochs</code> int Save every N epochs <code>checkpoint_every_n_steps</code> int Save every N steps"},{"location":"configuration/#checkpoint-management","title":"Checkpoint Management","text":"Parameter Type Description <code>keep_top_k</code> int Keep best K checkpoints <code>save_last</code> bool Always save latest"},{"location":"configuration/#resume-settings","title":"Resume Settings","text":"Parameter Type Description <code>resume_mode</code> str \"latest\" or \"best\" <code>resume_path</code> str Path to checkpoint"},{"location":"configuration/#logging-parameters","title":"Logging Parameters","text":"<p>Configure experiment tracking:</p> <pre><code>from dream_trainer.configs import WandBParameters\n\nwandb_params = WandBParameters(\n    # Basic settings\n    project=\"my-project\",\n    entity=\"my-team\",\n\n    # Experiment info\n    tags=[\"experiment\", \"classification\"],\n    notes=\"Initial baseline run\",\n\n    # Logging settings\n    log_model=True,\n    log_artifacts=True,\n    log_code=True\n)\n</code></pre>"},{"location":"configuration/#basic-settings_1","title":"Basic Settings","text":"Parameter Type Description <code>project</code> str WandB project name <code>entity</code> str WandB entity/team"},{"location":"configuration/#experiment-info","title":"Experiment Info","text":"Parameter Type Description <code>tags</code> List[str] Experiment tags <code>notes</code> str Experiment notes"},{"location":"configuration/#logging-settings","title":"Logging Settings","text":"Parameter Type Description <code>log_model</code> bool Log model checkpoints <code>log_artifacts</code> bool Log artifacts <code>log_code</code> bool Log code changes"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#custom-configuration","title":"Custom Configuration","text":"<p>You can create custom configuration classes:</p> <pre><code>from dream_trainer.configs import BaseConfig\n\nclass CustomConfig(BaseConfig):\n    def __init__(\n        self,\n        custom_param: str,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.custom_param = custom_param\n</code></pre>"},{"location":"configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>Configurations can be inherited and extended:</p> <pre><code>class ExtendedConfig(DreamTrainerConfig):\n    def __init__(\n        self,\n        new_param: int,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.new_param = new_param\n</code></pre>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Add validation to your configurations:</p> <pre><code>from dream_trainer.configs import BaseConfig\nfrom typing import Optional\n\nclass ValidatedConfig(BaseConfig):\n    def __init__(\n        self,\n        required_param: str,\n        optional_param: Optional[int] = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.required_param = required_param\n        self.optional_param = optional_param\n\n    def validate(self):\n        if not self.required_param:\n            raise ValueError(\"required_param cannot be empty\")\n        if self.optional_param is not None and self.optional_param &lt; 0:\n            raise ValueError(\"optional_param must be non-negative\")\n</code></pre>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use Type Hints: Always use type hints for better IDE support</li> <li>Validate Inputs: Add validation for critical parameters</li> <li>Document Parameters: Add docstrings for custom parameters</li> <li>Use Sensible Defaults: Provide reasonable default values</li> <li>Group Related Parameters: Use nested configs for related settings</li> </ol>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Dream Trainer respects several environment variables:</p> <pre><code># PyTorch distributed settings\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=29500\nexport WORLD_SIZE=8\nexport RANK=0\n\n# NCCL settings for better performance\nexport NCCL_DEBUG=INFO\nexport NCCL_TREE_THRESHOLD=0\n\n# GPU memory settings\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n</code></pre>"},{"location":"configuration/#configuration-validation_1","title":"Configuration Validation","text":"<p>Dream Trainer validates configurations at runtime:</p> <pre><code># These will raise errors:\nDeviceParameters(\n    data_parallel_size=3,\n    tensor_parallel_size=2,\n    # Error: Total devices (6) must match available GPUs\n)\n\nTrainingParameters(\n    train_batch_size=7,\n    # Error: Batch size must be divisible by data parallel size\n)\n</code></pre>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See Trainer Guide for implementing custom trainers</li> <li>Check Callbacks for extending functionality</li> <li>Read Distributed Training for multi-GPU details</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>Welcome to the Dream Trainer Core Concepts guide. This document explains the fundamental ideas and design patterns that power Dream Trainer's flexible, distributed training framework.</p>"},{"location":"core-concepts/#understanding-dtensor","title":"Understanding DTensor","text":""},{"location":"core-concepts/#what-is-dtensor","title":"What is DTensor?","text":"<p>DTensor (Distributed Tensor) is PyTorch's next-generation distributed computing primitive that provides a unified abstraction for tensor parallelism. Unlike traditional distributed approaches that treat each device's data separately, DTensor represents a global logical tensor that can be sharded across multiple devices while maintaining a single, coherent view.</p> <p>Key Benefits: - Unified API: Write code once, run with any parallelism strategy - Automatic gradient synchronization: DTensor handles communication patterns - Composability: Easily combine different parallelism strategies - Device mesh awareness: Understands multi-dimensional device topologies</p>"},{"location":"core-concepts/#dtensor-vs-traditional-distributed-training","title":"DTensor vs Traditional Distributed Training","text":"<pre><code># Traditional DDP approach - each rank has its own tensor\nmodel = MyModel().to(rank)\nmodel = DDP(model, device_ids=[rank])\noutput = model(input)  # Each rank processes different data\n\n# DTensor approach - single logical tensor across all ranks\nfrom torch.distributed.tensor import DTensor, Shard, Replicate, DeviceMesh\n\n# Create a 2D device mesh for hybrid parallelism\ndevice_mesh = DeviceMesh(\"cuda\", [[0, 1], [2, 3]])  # 2x2 mesh\n\n# Create a DTensor with specific sharding\ntensor = DTensor.from_local(\n    local_tensor,\n    device_mesh,\n    placements=[Shard(0), Replicate()]  # Shard on dim 0, replicate on dim 1\n)\n</code></pre>"},{"location":"core-concepts/#placement-and-sharding","title":"Placement and Sharding","text":"<p>DTensor uses placements to describe how data is distributed:</p> <ul> <li><code>Replicate()</code>: Each device has a full copy of the tensor</li> <li><code>Shard(dim)</code>: Tensor is split along the specified dimension</li> <li><code>Partial()</code>: Each device has a partial value (used during reduction)</li> </ul> <pre><code># Example: Creating different DTensor distributions\nimport torch\nfrom torch.distributed.tensor import DTensor, DeviceMesh, Shard, Replicate\n\n# Initialize a simple device mesh\ndevice_mesh = DeviceMesh(\"cuda\", [0, 1, 2, 3])\n\n# Full replication - all devices have complete tensor\nweight_replicated = DTensor.from_local(\n    torch.randn(1024, 1024),\n    device_mesh,\n    [Replicate()]\n)\n\n# Sharding along dimension 0 - each device gets 256 rows\nweight_sharded = DTensor.from_local(\n    torch.randn(256, 1024),  # Local shard size\n    device_mesh,\n    [Shard(0)]\n)\n\n# 2D mesh with hybrid sharding\nmesh_2d = DeviceMesh(\"cuda\", [[0, 1], [2, 3]])\nweight_2d = DTensor.from_local(\n    torch.randn(512, 512),\n    mesh_2d,\n    [Shard(0), Shard(1)]  # Shard rows on first mesh dim, cols on second\n)\n</code></pre>"},{"location":"core-concepts/#devicemesh-concept","title":"DeviceMesh Concept","text":"<p>DeviceMesh represents the topology of devices participating in distributed computation:</p> <pre><code># 1D mesh - simple data parallel\ndp_mesh = DeviceMesh(\"cuda\", [0, 1, 2, 3])\n\n# 2D mesh - hybrid data + tensor parallel\n# First dimension: data parallel groups\n# Second dimension: tensor parallel groups\nmesh_2d = DeviceMesh(\"cuda\", [\n    [0, 1, 2, 3],  # TP group 0\n    [4, 5, 6, 7],  # TP group 1\n])\n\n# 3D mesh - data + tensor + pipeline parallel\nmesh_3d = DeviceMesh(\"cuda\", [\n    [[0, 1], [2, 3]],    # PP stage 0\n    [[4, 5], [6, 7]],    # PP stage 1\n])\n</code></pre>"},{"location":"core-concepts/#the-mixin-architecture","title":"The Mixin Architecture","text":""},{"location":"core-concepts/#why-mixins","title":"Why Mixins?","text":"<p>Dream Trainer uses a mixin-based architecture to achieve maximum flexibility and code reuse. Instead of a monolithic trainer class with every feature built-in, we compose trainers from smaller, focused components.</p> <p>Traditional Inheritance Problems: <pre><code># Rigid hierarchy - hard to mix features\nclass Trainer:\n    def train(self): ...\n\nclass DistributedTrainer(Trainer):\n    def setup_distributed(self): ...\n\nclass LoggingTrainer(DistributedTrainer):\n    def log_metrics(self): ...\n\n# What if I want logging without distributed? \ud83e\udd14\n</code></pre></p> <p>Mixin Solution: <pre><code># Flexible composition - take what you need\nclass MyTrainer(BaseTrainer, SetupMixin, LoggerMixin):\n    # Get distributed setup from SetupMixin\n    # Get logging from LoggerMixin\n    # Skip what you don't need!\n</code></pre></p>"},{"location":"core-concepts/#how-mixins-work","title":"How Mixins Work","text":"<p>Mixins leverage Python's Method Resolution Order (MRO) to combine functionality:</p> <pre><code># Understanding MRO with mixins\nclass BaseTrainer:\n    def setup(self):\n        print(\"BaseTrainer.setup()\")\n\nclass ModelMixin:\n    def setup(self):\n        super().setup()  # Calls next in MRO\n        print(\"ModelMixin.setup()\")\n        self.setup_models()\n\nclass OptimizerMixin:\n    def setup(self):\n        super().setup()  # Calls next in MRO\n        print(\"OptimizerMixin.setup()\")\n        self.setup_optimizers()\n\nclass MyTrainer(BaseTrainer, ModelMixin, OptimizerMixin):\n    def setup(self):\n        super().setup()  # Triggers the chain\n        print(\"MyTrainer.setup()\")\n\n# MRO: MyTrainer -&gt; OptimizerMixin -&gt; ModelMixin -&gt; BaseTrainer\n# Output:\n# BaseTrainer.setup()\n# ModelMixin.setup()\n# OptimizerMixin.setup()\n# MyTrainer.setup()\n</code></pre>"},{"location":"core-concepts/#available-mixins","title":"Available Mixins","text":"<p>Dream Trainer provides several mixins for different aspects of training:</p>"},{"location":"core-concepts/#setupmixin-family","title":"SetupMixin Family","text":"<p>Handles model initialization, parallelism, and optimization:</p> <pre><code>from dream_trainer.trainer.mixins import SetupMixin\n\nclass MyTrainer(BaseTrainer, SetupMixin):\n    def configure_models(self):\n        # Create models on meta device for efficiency\n        self.model = TransformerModel(self.config)\n\n    def init_weights(self):\n        # Initialize weights after device placement\n        self.model.apply(self._init_weights)\n\n    def configure_optimizers(self):\n        # Create optimizers after model is on device\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.config.learning_rate\n        )\n\n    def configure_dataloaders(self):\n        # Return train and validation dataloaders\n        return train_dataloader, val_dataloader\n</code></pre> <p>The SetupMixin is actually composed of several sub-mixins: - ModelSetupMixin: Model creation, parallelism application, compilation - OptimizerAndSchedulerSetupMixin: Optimizer and LR scheduler management - DataLoaderSetupMixin: DataLoader configuration and setup</p>"},{"location":"core-concepts/#evalmetricmixin","title":"EvalMetricMixin","text":"<p>Integrates torchmetrics for standardized evaluation:</p> <pre><code>from dream_trainer.trainer.mixins import EvalMetricMixin\n\nclass MyTrainer(BaseTrainer, SetupMixin, EvalMetricMixin):\n    def configure_metrics(self):\n        # Metrics are automatically moved to correct devices\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=10)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x)\n\n        # Update metrics - handles distributed sync automatically\n        self.accuracy(logits, y)\n        self.f1_score(logits, y)\n\n        return {\"val_loss\": F.cross_entropy(logits, y)}\n</code></pre>"},{"location":"core-concepts/#loggermixin-variants","title":"LoggerMixin Variants","text":"<p>Different logging backends with consistent interfaces:</p> <pre><code>from dream_trainer.trainer.mixins import WandBLoggerMixin\n\nclass MyTrainer(BaseTrainer, SetupMixin, WandBLoggerMixin):\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_loss(batch)\n\n        # Automatic logging to Weights &amp; Biases\n        self.log_scalar(\"train/loss\", loss)\n\n        if batch_idx % 100 == 0:\n            self.log_image(\"train/samples\", batch[\"image\"][:8])\n\n        return {\"loss\": loss}\n</code></pre>"},{"location":"core-concepts/#quantizemixin","title":"QuantizeMixin","text":"<p>For model quantization (FP8, INT8):</p> <pre><code>from dream_trainer.trainer.mixins import QuantizeMixin\n\nclass MyTrainer(BaseTrainer, SetupMixin, QuantizeMixin):\n    def setup(self):\n        super().setup()\n        # Quantize after model setup\n        self.apply_quantization()\n</code></pre>"},{"location":"core-concepts/#building-a-custom-trainer","title":"Building a Custom Trainer","text":"<p>Here's a complete example showing how to compose a custom trainer:</p> <pre><code>from dataclasses import dataclass\nfrom dream_trainer import BaseTrainer, BaseTrainerConfig\nfrom dream_trainer.trainer.mixins import (\n    SetupMixin, SetupConfigMixin,\n    EvalMetricMixin, EvalMetricConfigMixin,\n    WandBLoggerMixin, WandBLoggerConfigMixin\n)\n\n# Step 1: Define configuration by mixing config classes\n@dataclass(kw_only=True)\nclass MyTrainerConfig(\n    BaseTrainerConfig,\n    SetupConfigMixin,\n    EvalMetricConfigMixin,\n    WandBLoggerConfigMixin\n):\n    # Add custom config fields\n    model_dim: int = 768\n    num_heads: int = 12\n    num_layers: int = 12\n    vocab_size: int = 50257\n\n# Step 2: Define trainer by mixing trainer classes\nclass MyTrainer(\n    BaseTrainer,\n    SetupMixin,\n    EvalMetricMixin,\n    WandBLoggerMixin\n):\n    config: MyTrainerConfig\n\n    def configure_models(self):\n        self.model = TransformerLM(\n            dim=self.config.model_dim,\n            heads=self.config.num_heads,\n            layers=self.config.num_layers,\n            vocab_size=self.config.vocab_size\n        )\n\n    def init_weights(self):\n        # Custom weight initialization\n        def _init_weights(module):\n            if isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean=0.0, std=0.02)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n\n        self.model.apply(_init_weights)\n\n    def configure_optimizers(self):\n        # Separate weight decay for different param groups\n        decay_params = []\n        no_decay_params = []\n\n        for name, param in self.model.named_parameters():\n            if \"bias\" in name or \"norm\" in name:\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        self.optimizer = torch.optim.AdamW([\n            {\"params\": decay_params, \"weight_decay\": 0.1},\n            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n        ], lr=self.config.learning_rate)\n\n    def configure_schedulers(self):\n        # Cosine annealing with warmup\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=self.config.training_parameters.epochs\n        )\n\n    def configure_dataloaders(self):\n        train_dataset = MyDataset(\"train\", self.config)\n        val_dataset = MyDataset(\"validation\", self.config)\n\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=4\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=4\n        )\n\n        return train_loader, val_loader\n\n    def configure_metrics(self):\n        self.perplexity = torchmetrics.Perplexity()\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        labels = batch[\"labels\"]\n\n        # Forward pass\n        logits = self.model(input_ids)\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            labels.view(-1)\n        )\n\n        # Backward pass with gradient accumulation\n        self.backward(loss)\n\n        # Step optimizer when not accumulating\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n            self.log_scalar(\"train/grad_norm\", grad_norm)\n\n        # Log metrics\n        self.log_scalar(\"train/loss\", loss)\n        self.log_scalar(\"train/lr\", self.optimizer.param_groups[0][\"lr\"])\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        labels = batch[\"labels\"]\n\n        logits = self.model(input_ids)\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            labels.view(-1)\n        )\n\n        # Update metrics\n        self.perplexity(logits, labels)\n\n        return {\"val_loss\": loss}\n</code></pre>"},{"location":"core-concepts/#training-loop-lifecycle","title":"Training Loop Lifecycle","text":"<p>The training loop in Dream Trainer follows a well-defined lifecycle with clear phases and hooks for customization:</p>"},{"location":"core-concepts/#initialization-phase","title":"Initialization Phase","text":"<pre><code>trainer = MyTrainer(config)\n# 1. __init__ is called\n#    - Initialize world/distributed setup\n#    - Create callback collection\n#    - Set initial state (epoch=0, global_step=0)\n\ntrainer.fit()\n# 2. configure() is called\n#    - configure_models() on meta device\n#    - post_configure_models() hook\n\n# 3. setup() is called\n#    - Apply parallelism (TP, PP, FSDP)\n#    - Initialize weights\n#    - Setup optimizers &amp; schedulers\n#    - Setup dataloaders\n#    - Setup metrics\n</code></pre>"},{"location":"core-concepts/#training-phase","title":"Training Phase","text":"<p>The training loop executes these steps for each epoch:</p> <pre><code>def perform_training_epoch(self):\n    self.train()  # Set models to training mode\n    self.callbacks.pre_train_epoch(self)\n\n    for batch_idx, batch in enumerate(self.train_dataloader):\n        # 1. Pre-batch callbacks\n        self.callbacks.pre_train_batch(self, batch, batch_idx)\n\n        # 2. Training step with autocast\n        with self.train_context():  # Includes autocast, profiling, etc.\n            output = self.training_step(batch, batch_idx)\n\n        # 3. Post-batch callbacks\n        self.callbacks.post_train_batch(self, output, batch, batch_idx)\n\n        # 4. Increment counters\n        self.local_batches += 1\n        if not self.is_accumulating_gradients:\n            self.global_step += 1\n\n        # 5. Validation check\n        if should_validate():\n            self.perform_validation_epoch()\n\n    self.callbacks.post_train_epoch(self)\n</code></pre>"},{"location":"core-concepts/#gradient-accumulation-flow","title":"Gradient Accumulation Flow","text":"<p>Dream Trainer handles gradient accumulation automatically:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # Your implementation\n    loss = self.model(batch)\n\n    # backward() handles scaling by accumulation steps\n    self.backward(loss)  # Internally: (loss / accumulation_steps).backward()\n\n    # Only step optimizer when gradients are ready\n    if not self.is_accumulating_gradients:\n        # All gradients accumulated, time to step\n        grad_norm = self.step(self.model, self.optimizer)\n\n    return {\"loss\": loss}\n</code></pre> <p>The <code>no_gradient_sync</code> context manager optimizes distributed training:</p> <pre><code># Automatically applied during accumulation steps\nwith self.no_gradient_sync(self.model):\n    loss.backward()  # No distributed sync until final accumulation\n</code></pre>"},{"location":"core-concepts/#validation-phase","title":"Validation Phase","text":"<p>Validation runs with gradients disabled and models in eval mode:</p> <pre><code>@torch.no_grad()\ndef perform_validation_epoch(self):\n    self.eval()  # Set models to eval mode\n    self.callbacks.pre_val_epoch(self)\n\n    for batch_idx, batch in enumerate(self.val_dataloader):\n        # 1. Pre-batch callbacks\n        self.callbacks.pre_val_batch(self, batch, batch_idx)\n\n        # 2. Validation step\n        output = self.validation_step(batch, batch_idx)\n\n        # 3. Post-batch callbacks\n        self.callbacks.post_val_batch(self, output, batch, batch_idx)\n\n    # 4. Compute epoch metrics\n    if hasattr(self, \"compute_metrics\"):\n        metrics = self.compute_metrics()\n        self.callbacks.on_val_epoch_end(self, metrics)\n\n    self.callbacks.post_val_epoch(self)\n</code></pre>"},{"location":"core-concepts/#callback-integration","title":"Callback Integration","text":"<p>Callbacks provide hooks at every stage of training:</p> <pre><code>from dream_trainer.callbacks import Callback\n\nclass MyCallback(Callback):\n    def on_train_start(self, trainer):\n        print(\"Training started!\")\n\n    def on_train_batch_end(self, trainer, output, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx}: loss = {output['loss']:.4f}\")\n\n    def on_validation_end(self, trainer, metrics):\n        print(f\"Validation perplexity: {metrics['perplexity']:.2f}\")\n</code></pre>"},{"location":"core-concepts/#state-management","title":"State Management","text":"<p>Dream Trainer provides comprehensive state management for checkpointing and resumption:</p>"},{"location":"core-concepts/#trainer-state","title":"Trainer State","text":"<p>The trainer maintains several state variables:</p> <pre><code>class BaseTrainer:\n    # Global state\n    global_step: int = 0        # Total optimizer steps across all epochs\n    current_epoch: int = 0      # Current epoch number\n    local_batches: int = 0      # Batches processed since start\n\n    # Training flags\n    training: bool = False      # True during training, False during eval\n\n    # Properties\n    @property\n    def is_accumulating_gradients(self) -&gt; bool:\n        \"\"\"True if currently accumulating gradients\"\"\"\n        return (\n            (self.local_batches + 1) % self._num_gradient_accumulation_steps != 0\n        ) and not self._is_last_training_batch\n</code></pre>"},{"location":"core-concepts/#checkpointing","title":"Checkpointing","text":"<p>State dict includes all components needed for exact resumption:</p> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    return {\n        \"trainer\": {\n            \"global_step\": self.global_step,\n            \"current_epoch\": self.current_epoch,\n            \"callbacks\": self.callbacks.state_dict(),\n        },\n        \"models\": {\n            name: model.state_dict() \n            for name, model in self.named_models().items()\n        },\n        \"optimizers\": {\n            name: optimizer.state_dict()\n            for name, optimizer in self.named_optimizers().items()\n        },\n        \"schedulers\": {\n            name: scheduler.state_dict()\n            for name, scheduler in (self.named_schedulers() or {}).items()\n        },\n        \"dataloaders\": {\n            \"train\": getattr(self.train_dataloader, \"state_dict\", lambda: {})(),\n            \"val\": getattr(self.val_dataloader, \"state_dict\", lambda: {})(),\n        },\n    }\n\ndef load_state_dict(self, state_dict: dict[str, Any], strict: bool = True):\n    # Restore all components\n    trainer_state = state_dict[\"trainer\"]\n    self.global_step = trainer_state[\"global_step\"]\n    self.current_epoch = trainer_state[\"current_epoch\"]\n    self.callbacks.load_state_dict(trainer_state[\"callbacks\"])\n\n    # Load model, optimizer, scheduler states...\n</code></pre>"},{"location":"core-concepts/#distributed-state","title":"Distributed State","text":"<p>The <code>DistributedWorld</code> class manages distributed training state:</p> <pre><code>class DistributedWorld:\n    def __init__(self, device_parameters: DeviceParameters):\n        self.world_size = torch.distributed.get_world_size()\n        self.rank = torch.distributed.get_rank()\n        self.local_rank = torch.distributed.get_local_rank()\n\n        # Device meshes for different parallelism types\n        self.dp_mesh = self._build_mesh(\"dp\", device_parameters)\n        self.tp_mesh = self._build_mesh(\"tp\", device_parameters)\n        self.pp_mesh = self._build_mesh(\"pp\", device_parameters)\n\n    @contextlib.contextmanager\n    def train_context(self):\n        \"\"\"Context for training with autocast and other optimizations\"\"\"\n        with torch.cuda.amp.autocast(\n            enabled=self.mixed_precision_enabled,\n            dtype=self.mixed_precision_dtype\n        ):\n            yield\n\n    def all_reduce(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"All-reduce across data parallel group\"\"\"\n        return dist_ops.all_reduce(tensor, group=self.dp_process_group)\n\n    def all_gather(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"All-gather across data parallel group\"\"\"\n        return dist_ops.all_gather_tensor(tensor, group=self.dp_process_group)\n</code></pre>"},{"location":"core-concepts/#fault-tolerance","title":"Fault Tolerance","text":"<p>Dream Trainer integrates with PyTorch's fault tolerance mechanisms:</p> <pre><code># Automatic checkpoint saving\ncheckpoint_callback = CheckpointCallback(\n    checkpoint_dir=\"./checkpoints\",\n    save_frequency=1000,  # Save every 1000 steps\n    keep_last_n=3,        # Keep last 3 checkpoints\n)\n\n# Fault-tolerant training with torchft\nif config.enable_fault_tolerance:\n    from torchft import FaultTolerantWorld\n    trainer.world = FaultTolerantWorld(trainer.world)\n</code></pre>"},{"location":"core-concepts/#memory-management","title":"Memory Management","text":"<p>DTensor enables efficient memory usage through sharding:</p> <pre><code># Memory usage comparison\n# Traditional: Each GPU stores full 405B parameter model = 810GB per GPU\n# With FSDP sharding: 810GB / 8 GPUs = ~101GB per GPU\n\n# Configure memory-efficient training\nconfig = DreamTrainerConfig(\n    device_parameters=DeviceParameters(\n        dp_degree=8,  # Shard across 8 GPUs\n        enable_fsdp=True,\n        fsdp_limit_all_gathers=True,  # Rate limit to save memory\n        fsdp_forward_prefetch=True,   # Overlap computation/communication\n    )\n)\n</code></pre>"},{"location":"core-concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts:</p> <ol> <li>Getting Started: Install Dream Trainer and run your first training</li> <li>Configuration Guide: Deep dive into configuration options</li> <li>Trainer Guide: Build custom trainers for your use case</li> <li>Callbacks: Extend functionality with the callback system</li> <li>API Reference: Detailed API documentation</li> </ol>"},{"location":"core-concepts/#references","title":"References","text":"<ul> <li>PyTorch DTensor RFC</li> <li>PyTorch FSDP Tutorial</li> <li>TorchTitan: Production-Ready LLM Training </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"documentation-roadmap/","title":"Dream Trainer Documentation Roadmap","text":""},{"location":"documentation-roadmap/#priority-matrix","title":"Priority Matrix","text":""},{"location":"documentation-roadmap/#critical-must-have","title":"\ud83d\udd34 Critical (Must Have)","text":"<p>These documents are essential for users to successfully use Dream Trainer.</p> Document Current State Priority Effort Owner API Reference: Trainers Missing P0 High API Reference: Mixins Missing P0 High Core Concepts Guide Missing P0 Medium Debugging Common Issues Missing P0 Medium Installation Troubleshooting Partial P0 Low"},{"location":"documentation-roadmap/#important-should-have","title":"\ud83d\udfe1 Important (Should Have)","text":"<p>These enhance user experience and adoption.</p> Document Current State Priority Effort Owner Tutorial: First Trainer Missing P1 Medium Tutorial: Multi-GPU Missing P1 Medium Performance Tuning Guide Missing P1 High Migration from Lightning Missing P1 Medium Example: Vision Models Partial P1 Low"},{"location":"documentation-roadmap/#nice-to-have","title":"\ud83d\udfe2 Nice to Have","text":"<p>These provide additional value for advanced users.</p> Document Current State Priority Effort Owner Video Tutorials Missing P2 High Benchmarks Missing P2 Medium Case Studies Missing P2 Medium Contributing Guide Missing P2 Low"},{"location":"documentation-roadmap/#detailed-page-specifications","title":"Detailed Page Specifications","text":""},{"location":"documentation-roadmap/#1-core-concepts-guide-new-page","title":"1. Core Concepts Guide (New Page)","text":"<p>File: <code>docs/core-concepts.md</code></p> <pre><code># Core Concepts\n\n## Understanding DTensor\n\n### What is DTensor?\n- Definition and purpose\n- Comparison with traditional distributed tensors\n- Code example: Creating a DTensor\n\n### Placement and Sharding\n- DeviceMesh concept\n- Sharding specifications\n- Interactive examples\n\n## The Mixin Architecture\n\n### Why Mixins?\n- Composition vs inheritance\n- Flexibility and reusability\n- Real-world analogy\n\n### How Mixins Work\n- Method Resolution Order (MRO)\n- Combining functionalities\n- Code example: Building a custom trainer\n\n### Available Mixins\n- SetupMixin: Model and optimizer management\n- EvalMetricMixin: Metrics tracking\n- LoggerMixin: Logging capabilities\n- QuantizeMixin: Model quantization\n\n## Training Loop Lifecycle\n\n### Initialization Phase\n- Configuration loading\n- Device setup\n- Model initialization on meta device\n\n### Training Phase\n- Forward pass\n- Loss computation\n- Backward pass with gradient accumulation\n- Optimizer step\n\n### Validation Phase\n- Metric computation\n- Checkpointing decisions\n- Early stopping logic\n\n## State Management\n\n### Trainer State\n- global_step tracking\n- epoch management\n- checkpoint resumption\n\n### Distributed State\n- Process synchronization\n- Collective operations\n- Fault tolerance\n</code></pre>"},{"location":"documentation-roadmap/#2-api-reference-structure","title":"2. API Reference Structure","text":"<p>Directory: <code>docs/api/</code></p> <pre><code>api/\n\u251c\u2500\u2500 index.md                 # API overview and navigation\n\u251c\u2500\u2500 trainers/\n\u2502   \u251c\u2500\u2500 abstract.md         # AbstractTrainer reference\n\u2502   \u251c\u2500\u2500 base.md            # BaseTrainer reference\n\u2502   \u2514\u2500\u2500 dream.md           # DreamTrainer reference\n\u251c\u2500\u2500 mixins/\n\u2502   \u251c\u2500\u2500 setup.md           # Setup mixins\n\u2502   \u251c\u2500\u2500 eval_metric.md     # Evaluation mixins\n\u2502   \u251c\u2500\u2500 loggers.md         # Logger mixins\n\u2502   \u2514\u2500\u2500 quantize.md        # Quantization mixins\n\u251c\u2500\u2500 callbacks/\n\u2502   \u251c\u2500\u2500 base.md            # Callback base class\n\u2502   \u251c\u2500\u2500 checkpoint.md      # Checkpoint callbacks\n\u2502   \u251c\u2500\u2500 monitoring.md      # Monitoring callbacks\n\u2502   \u2514\u2500\u2500 performance.md     # Performance callbacks\n\u251c\u2500\u2500 configuration/\n\u2502   \u251c\u2500\u2500 parameters.md      # Parameter classes\n\u2502   \u251c\u2500\u2500 device.md          # Device configuration\n\u2502   \u2514\u2500\u2500 training.md        # Training configuration\n\u2514\u2500\u2500 utilities/\n    \u251c\u2500\u2500 world.md           # World management\n    \u251c\u2500\u2500 data.md            # Data utilities\n    \u2514\u2500\u2500 common.md          # Common utilities\n</code></pre>"},{"location":"documentation-roadmap/#3-tutorial-series","title":"3. Tutorial Series","text":"<p>Directory: <code>docs/tutorials/</code></p>"},{"location":"documentation-roadmap/#tutorial-1-your-first-trainer","title":"Tutorial 1: Your First Trainer","text":"<pre><code># Tutorial 1: Building Your First Trainer\n\n**Time**: 30 minutes  \n**Prerequisites**: Basic PyTorch knowledge\n\n## Learning Objectives\n- Understand Dream Trainer's basic structure\n- Build a simple MNIST classifier\n- Run training locally\n\n## Step 1: Setup\n```python\n# Install Dream Trainer\npip install dream-trainer[metrics]\n\n# Verify installation\nimport dream_trainer\nprint(dream_trainer.__version__)\n</code></pre>"},{"location":"documentation-roadmap/#step-2-create-the-model","title":"Step 2: Create the Model","text":"<pre><code>import torch.nn as nn\n\nclass MNISTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n</code></pre>"},{"location":"documentation-roadmap/#step-3-define-the-trainer","title":"Step 3: Define the Trainer","text":"<p>[... complete tutorial content ...] <pre><code>### 4. Example Gallery Structure\n\n**Directory**: `examples/`\n\n```python\n# examples/vision/resnet_imagenet.py\n\"\"\"\nResNet ImageNet Training Example\n\nThis example demonstrates:\n- Multi-GPU training with FSDP2\n- Mixed precision training\n- Efficient data loading\n- Learning rate scheduling\n\nUsage:\n    python resnet_imagenet.py --batch-size 256 --gpus 8\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom dream_trainer import DreamTrainer, DreamTrainerConfig\nfrom dream_trainer.configs import DeviceParameters, TrainingParameters\n\n@dataclass\nclass ResNetConfig(DreamTrainerConfig):\n    \"\"\"Configuration for ResNet training\"\"\"\n    model_name: str = \"resnet50\"\n    num_classes: int = 1000\n    learning_rate: float = 0.1\n    momentum: float = 0.9\n    weight_decay: float = 1e-4\n\n# ... complete example ...\n</code></pre></p>"},{"location":"documentation-roadmap/#5-migration-guides","title":"5. Migration Guides","text":"<p>File: <code>docs/migration/from-pytorch-lightning.md</code></p> <pre><code># Migrating from PyTorch Lightning\n\n## Overview\n\nThis guide helps you migrate existing PyTorch Lightning code to Dream Trainer.\n\n## Conceptual Mapping\n\n| PyTorch Lightning | Dream Trainer | Notes |\n|-------------------|---------------|-------|\n| `LightningModule` | `BaseTrainer` + Mixins | Composition over inheritance |\n| `training_step()` | `training_step()` | Same interface |\n| `configure_optimizers()` | `configure_optimizers()` | Same interface |\n| `LightningDataModule` | `DataLoaderSetupMixin` | More flexible |\n| `Trainer(...)` | `Config + Callbacks` | Explicit configuration |\n\n## Step-by-Step Migration\n\n### Step 1: Convert LightningModule\n\n**Before (Lightning):**\n```python\nclass MyModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Linear(10, 1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n</code></pre> <p>After (Dream Trainer): <pre><code>class MyTrainer(BaseTrainer, SetupMixin):\n    def configure_models(self):\n        self.model = torch.nn.Linear(10, 1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        self.backward(loss)\n        return {\"loss\": loss}\n</code></pre></p> <p>[... continue with more examples ...] <pre><code>## Implementation Schedule\n\n### Week 1-2: Foundation\n- [ ] Core Concepts guide\n- [ ] API reference structure setup\n- [ ] Trainer API documentation\n\n### Week 3-4: API Documentation  \n- [ ] Mixins API documentation\n- [ ] Callbacks API documentation\n- [ ] Configuration API documentation\n- [ ] Utilities documentation\n\n### Week 5-6: Tutorials\n- [ ] Tutorial 1: First Trainer\n- [ ] Tutorial 2: Multi-GPU Training\n- [ ] Tutorial 3: Custom Components\n- [ ] Tutorial 4: Production Setup\n\n### Week 7-8: Examples &amp; Advanced\n- [ ] Vision examples (3)\n- [ ] NLP examples (3)\n- [ ] Performance tuning guide\n- [ ] Debugging guide\n\n### Week 9: Community\n- [ ] Migration guides\n- [ ] Contributing guide\n- [ ] FAQ compilation\n\n## Quality Checklist\n\n### For Each Document:\n- [ ] Clear learning objectives\n- [ ] Runnable code examples\n- [ ] Links to related documents\n- [ ] Tested on latest version\n- [ ] Reviewed by subject expert\n\n### For API Reference:\n- [ ] All public methods documented\n- [ ] Type hints included\n- [ ] Examples for each class\n- [ ] Inheritance hierarchy clear\n\n### For Tutorials:\n- [ ] Progressive difficulty\n- [ ] Time estimates accurate\n- [ ] Prerequisites listed\n- [ ] Downloadable notebooks\n\n## Metrics &amp; Tracking\n\n### Documentation Coverage\n```python\n# Track with automated script\ntotal_public_apis = count_public_apis()\ndocumented_apis = count_documented_apis()\ncoverage = documented_apis / total_public_apis * 100\n</code></pre></p>"},{"location":"documentation-roadmap/#user-success-metrics","title":"User Success Metrics","text":"<ul> <li>Time to first successful training run</li> <li>Support ticket reduction</li> <li>Community engagement</li> <li>Documentation feedback scores</li> </ul>"},{"location":"documentation-roadmap/#next-steps","title":"Next Steps","text":"<ol> <li>Set up documentation infrastructure</li> <li>Create issue templates for docs</li> <li>Assign owners to priority items</li> <li>Begin with Critical (P0) items</li> <li>Weekly progress reviews </li> </ol> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"getting-started/","title":"Getting Started with Dream Trainer","text":"<p>Welcome to Dream Trainer! This guide will help you understand what makes Dream Trainer unique and get you training models with advanced parallelism in minutes.</p>"},{"location":"getting-started/#why-dream-trainer","title":"Why Dream Trainer?","text":"<p>Before we dive in, let's understand what makes Dream Trainer different:</p>"},{"location":"getting-started/#composable-mixin-architecture","title":"\ud83e\udde9 Composable Mixin Architecture","text":"<p>Unlike monolithic frameworks, Dream Trainer lets you compose exactly the features you need:</p> <pre><code># Minimal trainer - just the essentials\nclass SimpleTrainer(BaseTrainer, SetupMixin):\n    pass\n\n# Add features as needed\nclass ProductionTrainer(BaseTrainer, SetupMixin, WandBLoggerMixin, \n                       EvalMetricMixin, QuantizeMixin):\n    pass  # Now with logging, metrics, and quantization!\n</code></pre>"},{"location":"getting-started/#dtensor-native-from-day-one","title":"\ud83d\ude80 DTensor-Native from Day One","text":"<p>Every parameter in Dream Trainer is a DTensor, giving you: - Automatic support for new PyTorch sharding patterns - Clean, debuggable distributed code - First-class support for TP, PP, CP, and FSDP2</p>"},{"location":"getting-started/#zero-compromise-performance","title":"\u26a1 Zero-Compromise Performance","text":"<ul> <li>Intelligent FSDP prefetching that traces execution order</li> <li>Loss parallelism for tensor-parallel training</li> <li>Async tensor parallelism support</li> <li>Compiled autograd integration</li> </ul>"},{"location":"getting-started/#configs-as-code-for-type-safety","title":"\ud83d\udcdd Configs as Code for Type Safety","text":"<p>Dream Trainer embraces Python configs over YAML/JSON for better developer experience:</p> <pre><code># \u274c Traditional approach - error-prone strings\nconfig = {\n    \"model\": \"gpt2\",  # Typo? Wrong name? Who knows!\n    \"lr\": \"3e-4\",     # String or float? \n    \"layers\": 12,     # Is this valid for gpt2?\n}\n\n# \u2705 Dream Trainer - full type safety and IDE support\n@dataclass\nclass MyConfig(BaseTrainerConfig):\n    learning_rate: float = 3e-4  # Type-checked!\n    num_layers: int = 12         # Auto-completion!\n\n    def validate(self):\n        \"\"\"Custom validation logic\"\"\"\n        if self.num_layers &lt; 1:\n            raise ValueError(\"Need at least 1 layer!\")\n</code></pre> <p>Benefits of configs as code: - Type Safety: Catch config errors at definition time, not runtime - IDE Support: Auto-completion, refactoring, and go-to-definition - Composability: Use functions, inheritance, and composition - Validation: Add custom validation logic and constraints - Documentation: Docstrings and type hints document themselves</p> <pre><code># Example: Composable configs with validation\ndef make_model_config(size: Literal[\"small\", \"base\", \"large\"]) -&gt; ModelConfig:\n    \"\"\"Factory function for common model sizes\"\"\"\n    sizes = {\n        \"small\": ModelConfig(hidden_size=768, num_layers=12),\n        \"base\": ModelConfig(hidden_size=1024, num_layers=24),\n        \"large\": ModelConfig(hidden_size=1536, num_layers=48),\n    }\n    return sizes[size]\n\n# Use partial functions for complex configs\nfrom functools import partial\n\nconfig = TrainerConfig(\n    model=partial(TransformerModel, num_heads=16),\n    optimizer=partial(torch.optim.AdamW, betas=(0.9, 0.95)),\n    scheduler=make_cosine_scheduler(warmup_steps=1000),\n)\n</code></pre>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>To get started with Dream Trainer:</p> <pre><code>pip install dream-trainer\n</code></pre> <p>For detailed installation instructions, including: - System requirements and CUDA compatibility - Feature-specific installations (wandb, metrics, quantization, etc.) - Development setup - Docker and cluster deployments - Troubleshooting common issues</p> <p>Please see our comprehensive Installation Guide.</p>"},{"location":"getting-started/#your-first-trainer","title":"Your First Trainer","text":"<p>Let's build a trainer that showcases Dream Trainer's strengths:</p>"},{"location":"getting-started/#step-1-understanding-the-mixin-pattern","title":"Step 1: Understanding the Mixin Pattern","text":"<p>Dream Trainer uses mixins to compose functionality. Here's the anatomy:</p> <pre><code>from dataclasses import dataclass\nfrom dream_trainer import BaseTrainer, BaseTrainerConfig\nfrom dream_trainer.trainer.mixins import SetupMixin, SetupConfigMixin\n\n# 1. Configuration uses the same mixin pattern\n@dataclass\nclass MyTrainerConfig(BaseTrainerConfig, SetupConfigMixin):\n    # BaseTrainerConfig provides: epochs, batch_size, etc.\n    # SetupConfigMixin adds: model/optimizer/dataloader configs\n    learning_rate: float = 3e-4\n    hidden_size: int = 768\n\n# 2. Trainer mirrors the config structure\nclass MyTrainer(BaseTrainer, SetupMixin):\n    config: MyTrainerConfig\n\n    # SetupMixin requires these methods:\n    def configure_models(self):\n        \"\"\"Define your models (on meta device - no memory used!)\"\"\"\n        self.model = TransformerModel(self.config.hidden_size)\n\n    def init_weights(self):\n        \"\"\"Initialize weights after parallelism is applied\"\"\"\n        self.model.apply(self._init_weights)\n\n    def configure_optimizers(self):\n        \"\"\"Define optimizers\"\"\"\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.config.learning_rate\n        )\n\n    def configure_dataloaders(self):\n        \"\"\"Return train and validation dataloaders\"\"\"\n        return self._make_train_loader(), self._make_val_loader()\n\n    # BaseTrainer requires these methods:\n    def training_step(self, batch, batch_idx):\n        \"\"\"Your forward pass and loss computation\"\"\"\n        loss = self.model(batch)\n        self.backward(loss)  # Handles gradient accumulation!\n\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n            return {\"loss\": loss, \"grad_norm\": grad_norm}\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation forward pass\"\"\"\n        with torch.no_grad():\n            loss = self.model(batch)\n        return {\"val_loss\": loss}\n</code></pre>"},{"location":"getting-started/#step-2-add-advanced-parallelism","title":"Step 2: Add Advanced Parallelism","text":"<p>Here's where Dream Trainer shines - adding parallelism is simple:</p> <pre><code>from dream_trainer.configs import DeviceParameters\n\n# Configure parallelism declaratively\nconfig = MyTrainerConfig(\n    device_parameters=DeviceParameters(\n        # Data parallelism\n        dp_shard=4,           # FSDP2 across 4 devices\n        dp_replicate=2,       # DDP across 2 nodes (HSDP)\n\n        # Model parallelism  \n        tensor_parallel=4,    # Tensor parallel degree\n        pipeline_parallel=2,  # Pipeline stages\n        context_parallel=2,   # For long sequences\n\n        # Optimizations\n        async_tensor_parallel=True,\n        compile_model=True,\n        loss_parallel=True,\n    )\n)\n\n# That's it! Dream Trainer handles all the complexity\n</code></pre>"},{"location":"getting-started/#step-3-implement-parallelism-methods","title":"Step 3: Implement Parallelism Methods","text":"<p>For advanced parallelism, implement these methods in your trainer:</p> <pre><code>class MyTrainer(BaseTrainer, SetupMixin):\n    # ... previous methods ...\n\n    def apply_tensor_parallel(self, tp_mesh):\n        \"\"\"Apply tensor parallelism to your model\"\"\"\n        # Dream Trainer provides the mesh, you decide the sharding\n        from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n\n        # Parallelize attention layers\n        for layer in self.model.layers:\n            tp_plan = {\n                \"attention.wq\": ColwiseParallel(),\n                \"attention.wk\": ColwiseParallel(), \n                \"attention.wv\": ColwiseParallel(),\n                \"attention.wo\": RowwiseParallel(),\n            }\n            parallelize_module(layer, tp_mesh, tp_plan)\n\n    def apply_pipeline_parallel(self, pp_mesh):\n        \"\"\"Split model into pipeline stages\"\"\"\n        # Return pipeline schedule and split modules\n        stages = [\n            self.model.embed,\n            self.model.layers[:8],\n            self.model.layers[8:16],\n            self.model.output\n        ]\n\n        from torch.distributed.pipelining import pipeline_parallel\n        schedule = pipeline_parallel(stages, pp_mesh)\n\n        return {\"model\": (schedule, stages, True, True)}\n\n    def apply_fully_shard(self, fsdp_config):\n        \"\"\"Apply FSDP2 sharding\"\"\"\n        from torch.distributed._composable.fsdp import fully_shard\n\n        # Shard each transformer layer\n        for layer in self.model.layers:\n            fully_shard(layer, **fsdp_config)\n\n        # Shard the whole model\n        fully_shard(self.model, **fsdp_config)\n</code></pre>"},{"location":"getting-started/#complete-example-multi-gpu-language-model","title":"Complete Example: Multi-GPU Language Model","text":"<p>Let's put it all together with a realistic example:</p> <pre><code>from dataclasses import dataclass, field\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom dream_trainer import DreamTrainer, DreamTrainerConfig\nfrom dream_trainer.callbacks import (\n    LoggerCallback, \n    CheckpointCallback,\n    OptimizeFSDP,\n    CallbackCollection\n)\nfrom dream_trainer.configs import (\n    DeviceParameters,\n    CheckpointParameters,\n    TrainingParameters\n)\n\n@dataclass\nclass LMConfig(DreamTrainerConfig):\n    # Model architecture\n    vocab_size: int = 50257\n    hidden_size: int = 768\n    num_layers: int = 12\n    num_heads: int = 12\n\n    # Training\n    learning_rate: float = 3e-4\n    warmup_steps: int = 1000\n    weight_decay: float = 0.1\n\n    # Data\n    sequence_length: int = 2048\n    dataset_path: str = \"data/openwebtext\"\n\nclass LanguageModelTrainer(DreamTrainer):\n    config: LMConfig\n\n    def configure_models(self):\n        \"\"\"Models are created on meta device - no memory used!\"\"\"\n        from my_models import GPTModel\n\n        self.model = GPTModel(\n            vocab_size=self.config.vocab_size,\n            hidden_size=self.config.hidden_size,\n            num_layers=self.config.num_layers,\n            num_heads=self.config.num_heads,\n        )\n\n    def init_weights(self):\n        \"\"\"Initialize after parallelism is applied\"\"\"\n        def _init_weights(module):\n            if isinstance(module, nn.Linear):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    torch.nn.init.zeros_(module.bias)\n\n        self.model.apply(_init_weights)\n\n    def configure_optimizers(self):\n        \"\"\"Configure AdamW with weight decay\"\"\"\n        # Separate parameters for weight decay\n        decay_params = []\n        no_decay_params = []\n\n        for name, param in self.model.named_parameters():\n            if 'bias' in name or 'norm' in name:\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        self.optimizer = torch.optim.AdamW([\n            {'params': decay_params, 'weight_decay': self.config.weight_decay},\n            {'params': no_decay_params, 'weight_decay': 0.0}\n        ], lr=self.config.learning_rate)\n\n    def configure_schedulers(self):\n        \"\"\"Cosine schedule with warmup\"\"\"\n        from torch.optim.lr_scheduler import CosineAnnealingLR\n\n        self.scheduler = CosineAnnealingLR(\n            self.optimizer,\n            T_max=self.config.training_parameters.n_epochs,\n            eta_min=self.config.learning_rate * 0.1\n        )\n\n    def configure_dataloaders(self):\n        \"\"\"Create distributed dataloaders\"\"\"\n        from my_data import TextDataset\n\n        train_dataset = TextDataset(\n            self.config.dataset_path,\n            sequence_length=self.config.sequence_length,\n            split='train'\n        )\n\n        # Dream Trainer provides distributed sampling utilities\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.training_parameters.train_batch_size,\n            sampler=self.get_train_sampler(train_dataset),  # Handles DP/PP\n            num_workers=4,\n            pin_memory=True\n        )\n\n        val_dataset = TextDataset(\n            self.config.dataset_path,\n            sequence_length=self.config.sequence_length,\n            split='validation'\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.training_parameters.train_batch_size * 2,\n            sampler=self.get_val_sampler(val_dataset),\n            num_workers=4,\n            pin_memory=True\n        )\n\n        return train_loader, val_loader\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Forward pass with next-token prediction\"\"\"\n        input_ids = batch['input_ids']\n\n        # Forward pass\n        logits = self.model(input_ids)\n\n        # Shift for next-token prediction\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = input_ids[..., 1:].contiguous()\n\n        # Loss computation with optional loss parallelism\n        with self.loss_parallel():\n            loss = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n        # Backward handles gradient accumulation automatically\n        self.backward(loss)\n\n        # Step optimizer only when not accumulating\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n\n            return {\n                \"loss\": loss,\n                \"grad_norm\": grad_norm,\n                \"learning_rate\": self.optimizer.param_groups[0]['lr'],\n            }\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Compute validation perplexity\"\"\"\n        input_ids = batch['input_ids']\n\n        with torch.no_grad():\n            logits = self.model(input_ids)\n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = input_ids[..., 1:].contiguous()\n\n            loss = F.cross_entropy(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n            perplexity = torch.exp(loss)\n\n        return {\n            \"val_loss\": loss,\n            \"val_perplexity\": perplexity,\n        }\n\n    def apply_tensor_parallel(self, tp_mesh):\n        \"\"\"Apply tensor parallelism to transformer layers\"\"\"\n        from torch.distributed.tensor.parallel import (\n            ColwiseParallel, \n            RowwiseParallel,\n            PrepareModuleInput,\n            parallelize_module\n        )\n\n        # Parallelize each transformer block\n        for i, block in enumerate(self.model.blocks):\n            layer_plan = {\n                # Attention\n                \"attn.q_proj\": ColwiseParallel(),\n                \"attn.k_proj\": ColwiseParallel(), \n                \"attn.v_proj\": ColwiseParallel(),\n                \"attn.out_proj\": RowwiseParallel(),\n\n                # MLP\n                \"mlp.up_proj\": ColwiseParallel(),\n                \"mlp.down_proj\": RowwiseParallel(),\n            }\n\n            parallelize_module(\n                block,\n                tp_mesh,\n                layer_plan,\n                input_fn=PrepareModuleInput(),\n            )\n\n# Create configuration with advanced features\nconfig = LMConfig(\n    # Distributed settings\n    device_parameters=DeviceParameters(\n        dp_shard=4,              # 4-way FSDP2\n        tensor_parallel=2,       # 2-way tensor parallelism\n        compile_model=True,      # torch.compile\n        enable_compiled_autograd=True,\n        loss_parallel=True,      # Parallel loss computation\n    ),\n\n    # Training settings\n    training_parameters=TrainingParameters(\n        n_epochs=10,\n        train_batch_size=8,\n        gradient_accumulation_steps=4,  # Effective batch = 32\n        gradient_clip_val=1.0,\n        val_frequency=0.25,  # Validate 4x per epoch\n    ),\n\n    # Callbacks for production features\n    callbacks=CallbackCollection([\n        LoggerCallback(log_every_n_train_batches=10),\n        CheckpointCallback(\n            CheckpointParameters(\n                checkpoint_every_n_epochs=1,\n                keep_top_k=3,\n                monitor=\"val_perplexity\",\n            )\n        ),\n        OptimizeFSDP(prefetch=2),  # Intelligent FSDP prefetching\n    ])\n)\n\nif __name__ == \"__main__\":\n    # Dream Trainer handles distributed launch automatically\n    from dream_trainer.utils import distributed_launch\n\n    def main():\n        trainer = LanguageModelTrainer(config)\n        trainer.fit()\n\n    distributed_launch(main)\n</code></pre>"},{"location":"getting-started/#launch-training","title":"Launch Training","text":""},{"location":"getting-started/#single-gpu","title":"Single GPU","text":"<pre><code>python train.py\n</code></pre>"},{"location":"getting-started/#multiple-gpus-single-node","title":"Multiple GPUs (Single Node)","text":"<pre><code># Dream Trainer auto-detects available GPUs\npython train.py\n\n# Or explicitly with torchrun\ntorchrun --nproc_per_node=8 train.py\n</code></pre>"},{"location":"getting-started/#multiple-nodes","title":"Multiple Nodes","text":"<pre><code># Node 0\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \\\n    --master_addr=$MASTER_ADDR --master_port=29500 train.py\n\n# Node 1  \ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \\\n    --master_addr=$MASTER_ADDR --master_port=29500 train.py\n</code></pre>"},{"location":"getting-started/#understanding-dream-trainers-advantages","title":"Understanding Dream Trainer's Advantages","text":""},{"location":"getting-started/#1-clean-parallelism-abstractions","title":"1. Clean Parallelism Abstractions","text":"<p>Dream Trainer makes complex parallelism approachable:</p> <pre><code># Bad (Raw PyTorch)\nif rank == 0:\n    model = model.cuda(0)\n    # Complex manual sharding...\n\n# Good (Dream Trainer)\ndef apply_tensor_parallel(self, tp_mesh):\n    # Clean, reusable parallelism logic\n</code></pre>"},{"location":"getting-started/#2-automatic-mixed-precision","title":"2. Automatic Mixed Precision","text":"<p>Dream Trainer wraps forward methods intelligently:</p> <pre><code># Automatic - Dream Trainer handles autocast placement\ndef training_step(self, batch, batch_idx):\n    loss = self.model(batch)  # Autocast applied automatically\n\n# No need for manual autocast contexts!\n</code></pre>"},{"location":"getting-started/#3-gradient-accumulation-that-just-works","title":"3. Gradient Accumulation That Just Works","text":"<pre><code># Dream Trainer handles the complexity\nself.backward(loss)  # Scales by accumulation steps\n\nif not self.is_accumulating_gradients:\n    # Only step when ready\n    self.step(self.model, self.optimizer)\n</code></pre>"},{"location":"getting-started/#4-composable-features","title":"4. Composable Features","text":"<p>Add features without modifying your core trainer:</p> <pre><code># Start simple\nclass V1Trainer(BaseTrainer, SetupMixin):\n    pass\n\n# Add metrics later\nclass V2Trainer(BaseTrainer, SetupMixin, EvalMetricMixin):\n    def configure_metrics(self):\n        self.accuracy = Accuracy()\n\n# Add logging even later\nclass V3Trainer(BaseTrainer, SetupMixin, EvalMetricMixin, WandBLoggerMixin):\n    pass  # No changes to existing code!\n</code></pre>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#adding-custom-callbacks","title":"Adding Custom Callbacks","text":"<pre><code>from dream_trainer.callbacks import Callback\n\nclass LearningRateWarmup(Callback[SetupMixin]):\n    def __init__(self, warmup_steps: int):\n        self.warmup_steps = warmup_steps\n\n    def post_train_step(self, result, batch_idx):\n        if self.trainer.global_step &lt; self.warmup_steps:\n            # Linear warmup\n            lr_scale = self.trainer.global_step / self.warmup_steps\n            for param_group in self.trainer.optimizer.param_groups:\n                param_group['lr'] = param_group['initial_lr'] * lr_scale\n</code></pre>"},{"location":"getting-started/#debugging-distributed-training","title":"Debugging Distributed Training","text":"<pre><code># Dream Trainer provides utilities for distributed debugging\nfrom dream_trainer.utils import rank_zero_print\n\nclass MyTrainer(DreamTrainer):\n    def training_step(self, batch, batch_idx):\n        # Only prints from rank 0\n        rank_zero_print(f\"Batch shape: {batch['input_ids'].shape}\")\n\n        # Check DTensor sharding\n        if hasattr(self.model.weight, 'placements'):\n            rank_zero_print(f\"Weight sharding: {self.model.weight.placements}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand Dream Trainer's core concepts:</p> <ol> <li>Explore Mixins: Check out the Trainer Guide to see all available mixins</li> <li>Master Parallelism: Read the Parallelism Guide for advanced distributed training</li> <li>Extend with Callbacks: Learn to create custom callbacks in the Callbacks Guide</li> <li>Optimize Performance: See Best Practices for performance tips</li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#installation-issues","title":"Installation Issues","text":"<pre><code># Check CUDA compatibility\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# Verify Dream Trainer features\npython -c \"from dream_trainer.utils import check_features; check_features()\"\n</code></pre>"},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<ol> <li>OOM with Large Models: Enable CPU offloading or use gradient checkpointing</li> <li>Slow Data Loading: Increase <code>num_workers</code> and use <code>pin_memory=True</code></li> <li>Debugging Distributed: Set <code>TORCH_CPP_LOG_LEVEL=INFO</code> for detailed logs</li> </ol>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcda Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> <li>\ud83d\udca1 Examples Repository</li> </ul> <p>Ready to train with Dream Trainer? You now understand what makes it unique. Happy training! \ud83d\ude80</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"installation/","title":"Installation Guide","text":"<p>This comprehensive guide covers all aspects of installing Dream Trainer, from basic setup to production deployment.</p>"},{"location":"installation/#requirements","title":"Requirements","text":""},{"location":"installation/#python-version-requirements","title":"Python Version Requirements","text":"<p>Dream Trainer requires Python 3.10 or later. Here's the compatibility matrix:</p> Dream Trainer Version Python Versions PyTorch Versions CUDA Versions 0.1.x 3.10 - 3.12 2.7.1+ 11.8, 12.1, 12.4"},{"location":"installation/#system-requirements","title":"System Requirements","text":""},{"location":"installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: x86_64 or ARM64 processor</li> <li>RAM: 8GB (for basic usage)</li> <li>Storage: 2GB free space</li> <li>OS: Linux, macOS, or Windows (WSL2 recommended)</li> </ul>"},{"location":"installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>GPU: NVIDIA GPU with CUDA capability 7.0+ (V100, A100, H100, RTX 3090+)</li> <li>RAM: 32GB+ (for multi-GPU training)</li> <li>Storage: 50GB+ free space (for datasets and checkpoints)</li> <li>Network: High-speed interconnect for multi-node training (InfiniBand preferred)</li> </ul>"},{"location":"installation/#pytorch-and-cuda-compatibility","title":"PyTorch and CUDA Compatibility","text":"<p>Dream Trainer builds on PyTorch's distributed training capabilities. Ensure your PyTorch installation matches your CUDA version:</p> <pre><code># Check your CUDA version\nnvidia-smi\n\n# Check PyTorch and CUDA compatibility\npython -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')\"\n</code></pre>"},{"location":"installation/#cuda-compatibility-table","title":"CUDA Compatibility Table","text":"CUDA Version PyTorch Install Command CUDA 11.8 <code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code> CUDA 12.1 <code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</code> CUDA 12.4+ <code>pip install torch torchvision torchaudio</code> CPU only <code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu</code>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#1-basic-installation","title":"1. Basic Installation","text":"<p>The simplest way to install Dream Trainer:</p> <pre><code>pip install dream-trainer\n</code></pre> <p>This installs the core framework with minimal dependencies: - PyTorch (&gt;= 2.7.1) - loguru (for logging) - tqdm (for progress bars) - dist-util (distributed utilities)</p>"},{"location":"installation/#2-feature-specific-installation","title":"2. Feature-Specific Installation","text":"<p>Dream Trainer uses optional dependencies to keep the core lightweight. Install only what you need:</p> <pre><code># Weights &amp; Biases Integration\n# Includes: wandb with media logging support\npip install \"dream-trainer[wandb]\"\n\n# TorchMetrics Integration\n# Includes: torchmetrics for standardized metric computation\npip install \"dream-trainer[metrics]\"\n\n# Quantization Support\n# Includes: torchao for FP8/INT8 quantization\npip install \"dream-trainer[torchao]\"\n\n# Fault Tolerance\n# Includes: torchft for automatic failure recovery\npip install \"dream-trainer[torchft]\"\n\n# Enhanced CLI Output\n# Includes: rich for better terminal formatting\npip install \"dream-trainer[rich]\"\n\n# Multiple features\npip install \"dream-trainer[wandb,metrics,torchao]\"\n\n# All features\npip install \"dream-trainer[all]\"\n</code></pre>"},{"location":"installation/#3-development-installation","title":"3. Development Installation","text":"<p>For contributing or customizing Dream Trainer:</p> <pre><code># Clone the repository\ngit clone https://github.com/dream3d-ai/dream-trainer.git\ncd dream-trainer\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks (optional)\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"installation/#4-docker-installation","title":"4. Docker Installation","text":"<p>For reproducible environments and cluster deployment:</p>"},{"location":"installation/#using-pre-built-images","title":"Using Pre-built Images","text":"<pre><code># Pull the latest image\ndocker pull dream3d/dream-trainer:latest\n\n# Run with GPU support\ndocker run --gpus all -it dream3d/dream-trainer:latest\n\n# With specific CUDA version\ndocker pull dream3d/dream-trainer:cuda12.1-pytorch2.7\n</code></pre>"},{"location":"installation/#building-custom-image","title":"Building Custom Image","text":"<p>Create a <code>Dockerfile</code>:</p> <pre><code># Base image with CUDA and PyTorch\nFROM pytorch/pytorch:2.7.1-cuda12.4-cudnn9-runtime\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Dream Trainer\nRUN pip install dream-trainer[all]\n\n# Optional: Install your project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Set working directory\nWORKDIR /workspace\n\n# Default command\nCMD [\"python\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t my-dream-trainer .\ndocker run --gpus all -v $(pwd):/workspace -it my-dream-trainer\n</code></pre>"},{"location":"installation/#5-cluster-specific-installations","title":"5. Cluster-Specific Installations","text":""},{"location":"installation/#slurm-clusters","title":"SLURM Clusters","text":"<p>Create a module file or use a shared environment:</p> <pre><code># Load required modules\nmodule load python/3.11\nmodule load cuda/12.1\nmodule load gcc/11.2\n\n# Create shared environment\npython -m venv /shared/envs/dream-trainer\nsource /shared/envs/dream-trainer/bin/activate\n\n# Install with MPI support\npip install dream-trainer[all]\npip install mpi4py  # For MPI-based launchers\n</code></pre> <p>Example SLURM job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=dream-trainer\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=8\n#SBATCH --gpus-per-node=8\n#SBATCH --time=24:00:00\n\n# Load environment\nsource /shared/envs/dream-trainer/bin/activate\n\n# Run training\nsrun python train.py\n</code></pre>"},{"location":"installation/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Create a Kubernetes manifest:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: dream-trainer-job\nspec:\n  parallelism: 4\n  template:\n    spec:\n      containers:\n      - name: trainer\n        image: dream3d/dream-trainer:latest\n        resources:\n          limits:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: checkpoints\n          mountPath: /checkpoints\n        env:\n        - name: MASTER_ADDR\n          value: \"dream-trainer-master\"\n        - name: MASTER_PORT\n          value: \"29500\"\n        command: [\"python\", \"train.py\"]\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: training-data\n      - name: checkpoints\n        persistentVolumeClaim:\n          claimName: model-checkpoints\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#1-basic-verification","title":"1. Basic Verification","text":"<p>Verify your installation is working correctly:</p> <pre><code># verify_installation.py\nimport dream_trainer\nimport torch\nimport sys\n\ndef check_installation():\n    \"\"\"Comprehensive installation check\"\"\"\n    print(f\"Python version: {sys.version}\")\n    print(f\"Dream Trainer version: {dream_trainer.__version__}\")\n    print(f\"PyTorch version: {torch.__version__}\")\n\n    # Check CUDA availability\n    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"CUDA version: {torch.version.cuda}\")\n        print(f\"GPU count: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n    # Check optional features\n    features = {\n        \"wandb\": \"wandb\",\n        \"torchmetrics\": \"torchmetrics\", \n        \"torchao\": \"torchao\",\n        \"torchft\": \"torchft\",\n        \"rich\": \"rich\"\n    }\n\n    print(\"\\nOptional features:\")\n    for name, module in features.items():\n        try:\n            __import__(module)\n            print(f\"  \u2713 {name}\")\n        except ImportError:\n            print(f\"  \u2717 {name} (install with: pip install dream-trainer[{name}])\")\n\n    # Test basic functionality\n    print(\"\\nTesting basic trainer creation...\")\n    try:\n        from dream_trainer import BaseTrainer, BaseTrainerConfig\n        from dream_trainer.trainer.mixins import SetupMixin\n\n        class TestTrainer(BaseTrainer, SetupMixin):\n            def configure_models(self):\n                self.model = torch.nn.Linear(10, 1)\n            def configure_optimizers(self):\n                self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)\n            def training_step(self, batch, batch_idx):\n                return {\"loss\": torch.tensor(1.0)}\n\n        config = BaseTrainerConfig(training_parameters={\"n_epochs\": 1})\n        print(\"  \u2713 Trainer creation successful\")\n\n    except Exception as e:\n        print(f\"  \u2717 Trainer creation failed: {e}\")\n\n    return True\n\nif __name__ == \"__main__\":\n    check_installation()\n</code></pre> <p>Run the verification script (available in <code>examples/verify_installation.py</code>):</p> <pre><code>python examples/verify_installation.py\n</code></pre>"},{"location":"installation/#2-performance-verification","title":"2. Performance Verification","text":"<p>Test that your GPU setup is performing optimally using the provided benchmark script:</p> <pre><code>python examples/benchmark_gpu.py\n</code></pre> <p>Or use the code below for a custom benchmark:</p> <pre><code># benchmark.py\nimport torch\nimport time\nfrom dream_trainer.utils import get_rank, get_world_size\n\ndef benchmark_gpu():\n    \"\"\"Simple GPU benchmark\"\"\"\n    if not torch.cuda.is_available():\n        print(\"No GPU available for benchmarking\")\n        return\n\n    device = torch.cuda.current_device()\n    print(f\"Benchmarking GPU {device}: {torch.cuda.get_device_name(device)}\")\n\n    # Test different sizes\n    sizes = [1024, 2048, 4096, 8192]\n\n    for size in sizes:\n        # Create random matrices\n        a = torch.randn(size, size, device='cuda')\n        b = torch.randn(size, size, device='cuda')\n\n        # Warmup\n        for _ in range(10):\n            c = torch.matmul(a, b)\n        torch.cuda.synchronize()\n\n        # Benchmark\n        start = time.time()\n        iterations = 100\n        for _ in range(iterations):\n            c = torch.matmul(a, b)\n        torch.cuda.synchronize()\n        end = time.time()\n\n        # Calculate TFLOPS\n        flops = 2 * size ** 3 * iterations\n        duration = end - start\n        tflops = flops / duration / 1e12\n\n        print(f\"  Matrix size {size}x{size}: {tflops:.2f} TFLOPS\")\n\n    # Test distributed communication if available\n    if get_world_size() &gt; 1:\n        print(f\"\\nDistributed setup detected: rank {get_rank()}/{get_world_size()}\")\n        tensor = torch.ones(1000, 1000, device='cuda')\n        start = time.time()\n        torch.distributed.all_reduce(tensor)\n        torch.cuda.synchronize()\n        duration = time.time() - start\n        print(f\"  All-reduce latency: {duration*1000:.2f} ms\")\n\nif __name__ == \"__main__\":\n    benchmark_gpu()\n</code></pre>"},{"location":"installation/#3-multi-gpu-verification","title":"3. Multi-GPU Verification","text":"<p>Test distributed training setup:</p> <pre><code># Single node, multiple GPUs\ntorchrun --nproc_per_node=2 verify_distributed.py\n\n# Multiple nodes (replace with your hostnames)\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \\\n    --master_addr=node1 --master_port=29500 verify_distributed.py\n</code></pre> <pre><code># verify_distributed.py\nimport torch\nimport torch.distributed as dist\nfrom dream_trainer import DreamTrainer, DreamTrainerConfig\n\ndef verify_distributed():\n    \"\"\"Verify distributed training setup\"\"\"\n    if not dist.is_initialized():\n        print(\"Distributed not initialized!\")\n        return\n\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    print(f\"Rank {rank}/{world_size} initialized successfully\")\n\n    # Test all-reduce\n    tensor = torch.ones(1) * rank\n    if torch.cuda.is_available():\n        tensor = tensor.cuda()\n\n    dist.all_reduce(tensor)\n    expected = sum(range(world_size))\n\n    if tensor.item() == expected:\n        print(f\"Rank {rank}: All-reduce test PASSED\")\n    else:\n        print(f\"Rank {rank}: All-reduce test FAILED (got {tensor.item()}, expected {expected})\")\n\nif __name__ == \"__main__\":\n    # Initialize distributed\n    torch.distributed.init_process_group(backend=\"nccl\")\n    verify_distributed()\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"installation/#1-cudapytorch-mismatch","title":"1. CUDA/PyTorch Mismatch","text":"<p>Problem: <code>RuntimeError: CUDA error: no kernel image is available</code></p> <p>Solution: <pre><code># Uninstall existing PyTorch\npip uninstall torch torchvision torchaudio\n\n# Reinstall with correct CUDA version\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre></p>"},{"location":"installation/#2-missing-system-libraries","title":"2. Missing System Libraries","text":"<p>Problem: <code>ImportError: libcudnn.so.8: cannot open shared object file</code></p> <p>Solution: <pre><code># On Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y libcudnn8\n\n# Or use conda\nconda install -c conda-forge cudnn\n</code></pre></p>"},{"location":"installation/#3-memory-issues-during-installation","title":"3. Memory Issues During Installation","text":"<p>Problem: <code>MemoryError</code> or killed process during pip install</p> <p>Solution: <pre><code># Use --no-cache-dir to reduce memory usage\npip install --no-cache-dir dream-trainer\n\n# Or increase swap space\nsudo fallocate -l 8G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre></p>"},{"location":"installation/#4-distributed-training-issues","title":"4. Distributed Training Issues","text":"<p>Problem: <code>torch.distributed.DistNetworkError</code></p> <p>Solution: <pre><code># Check network connectivity\nping &lt;other-node-hostname&gt;\n\n# Check firewall rules (allow PyTorch's default port)\nsudo ufw allow 29500\n\n# Test with different backend\nexport TORCH_DISTRIBUTED_DEBUG=DETAIL\npython train.py  # Will show detailed error messages\n</code></pre></p>"},{"location":"installation/#5-import-errors-after-installation","title":"5. Import Errors After Installation","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'dream_trainer'</code></p> <p>Solution: <pre><code># Ensure you're in the correct environment\nwhich python\npip list | grep dream-trainer\n\n# If using conda/venv, activate it\nconda activate myenv  # or\nsource venv/bin/activate\n\n# Verify Python path\npython -c \"import sys; print(sys.path)\"\n</code></pre></p>"},{"location":"installation/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li> <p>Enable NVIDIA Apex (if available): <pre><code>pip install -v --disable-pip-version-check --no-cache-dir \\\n    --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n    git+https://github.com/NVIDIA/apex\n</code></pre></p> </li> <li> <p>Set optimal environment variables: <pre><code># Optimize NCCL performance\nexport NCCL_IB_DISABLE=0\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_DEBUG=INFO\n\n# PyTorch optimizations\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\nexport TORCH_CUDNN_V8_API_ENABLED=1\n</code></pre></p> </li> <li> <p>Verify GPU-Direct RDMA (for multi-node): <pre><code># Check if available\nnvidia-smi topo -m\n\n# Enable P2P access\nsudo nvidia-smi -pm 1\n</code></pre></p> </li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Follow the Getting Started Guide: Learn the basics of Dream Trainer</li> <li>Explore Examples: Check out the <code>examples/</code> directory</li> <li>Read the API Documentation: Understand the framework components</li> <li>Join the Community: Get help and share experiences</li> </ol>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the FAQ</li> <li>Search GitHub Issues</li> <li>Join our Discord Community</li> <li>Create a new issue with:</li> <li>Your system information (OS, Python, PyTorch, CUDA versions)</li> <li>Complete error messages</li> <li>Minimal reproduction code </li> </ol> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"parallelism/","title":"Parallelism Guide","text":"<p>Dream Trainer provides first-class support for all modern parallelism strategies in PyTorch. This guide covers everything from basic data parallelism to advanced self-parallelizing models using <code>fsdp2_utils</code>.</p>"},{"location":"parallelism/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Understanding Memory and Batch Size Constraints</li> <li>Parallelism Strategies</li> <li>Network Topology Considerations</li> <li>Basic Configuration</li> <li>Self-Parallelizing Models with fsdp2_utils</li> <li>Manual Parallelism Implementation</li> <li>Combining Parallelism Strategies</li> <li>Performance Optimization</li> <li>Debugging Tips</li> </ul>"},{"location":"parallelism/#overview","title":"Overview","text":"<p>Dream Trainer supports five types of parallelism, all built on PyTorch's DTensor infrastructure:</p> <ol> <li>Data Parallelism (DP): Split batches across devices</li> <li><code>dp_replicate</code>: Traditional DDP replication</li> <li> <p><code>dp_shard</code>: FSDP2 sharding</p> </li> <li> <p>Tensor Parallelism (TP): Split model layers across devices</p> </li> <li> <p>Pipeline Parallelism (PP): Split model stages across devices</p> </li> <li> <p>Context Parallelism (CP): Split sequence dimension for long contexts</p> </li> <li> <p>Hybrid Strategies: Combine multiple types (e.g., HSDP = DDP + FSDP)</p> </li> </ol>"},{"location":"parallelism/#understanding-memory-and-batch-size-constraints","title":"Understanding Memory and Batch Size Constraints","text":"<p>Before diving into parallelism strategies, it's crucial to understand what consumes memory during training and how batch size affects training dynamics.</p>"},{"location":"parallelism/#memory-breakdown-during-training","title":"Memory Breakdown During Training","text":"<p>According to Jeremy Jordan's analysis, training a model requires keeping several components in memory:</p> <ol> <li>Model Parameters: The learnable weights (e.g., 405B parameters = ~810GB in FP16)</li> <li>Optimizer States: </li> <li>SGD: Just parameters (1x memory)</li> <li>Adam/AdamW: Parameters + first moment + second moment (3x memory)</li> <li>Model Activations: Intermediate values needed for backpropagation</li> <li>Scales with batch size and model architecture</li> <li>Can be the dominant memory consumer for large batches</li> <li>Gradients: Same size as model parameters</li> <li>Input Data: The actual batch being processed</li> </ol> <pre><code># Example memory calculation for a 7B parameter model\nmodel_params = 7e9 * 2  # 14GB in FP16\noptimizer_states = model_params * 3  # 42GB for AdamW\ngradients = model_params  # 14GB\n# Total: 70GB + activations + data\n</code></pre>"},{"location":"parallelism/#the-science-of-batch-size-scaling","title":"The Science of Batch Size Scaling","text":"<p>Not all batch sizes are created equal. As explained in \"An Empirical Model of Large-Batch Training\", there are two distinct regimes:</p> <ol> <li>Perfect Scaling Regime: When batch size is small, you can double the batch size and double the learning rate to train in half the steps</li> <li>Ineffective Scaling Regime: Beyond a critical batch size, increasing batch size provides diminishing returns</li> </ol> <p>The transition point (called the gradient noise scale) depends on your data and model. Importantly, this transition point increases during training, which is why models like Llama 3.1 progressively increase batch size:</p> <pre><code># Llama 3.1 training schedule\ninitial_batch = 4M tokens\nafter 252M tokens: batch_size = 8M\nafter 2.87T tokens: batch_size = 16M\n</code></pre>"},{"location":"parallelism/#memory-reduction-techniques","title":"Memory Reduction Techniques","text":"<p>When you hit memory limits on a single GPU, you have several options before resorting to parallelism:</p> <pre><code># 1. Gradient Accumulation (trade compute for memory)\nconfig = TrainingParameters(\n    train_batch_size=8,\n    gradient_accumulation_steps=4,  # Effective batch = 32\n)\n\n# 2. Activation Checkpointing (recompute vs store)\nconfig = DeviceParameters(\n    checkpoint_activations=True,  # ~30% memory savings\n)\n\n# 3. Mixed Precision (reduce precision)\nconfig = DeviceParameters(\n    param_dtype=torch.bfloat16,  # 2x memory savings vs FP32\n)\n</code></pre>"},{"location":"parallelism/#parallelism-strategies","title":"Parallelism Strategies","text":""},{"location":"parallelism/#understanding-the-trade-offs","title":"Understanding the Trade-offs","text":"<p>Each parallelism strategy makes different trade-offs between memory savings, communication overhead, and implementation complexity:</p> Strategy Memory Savings Communication Best For DDP None All-reduce gradients (once per step) Small models, high bandwidth FSDP2 High All-gather params + reduce-scatter grads Large models TP Medium All-reduce activations (multiple per layer) Wide models (large hidden dims) PP High Point-to-point activations Deep models (many layers) CP Medium All-to-all for attention Long sequences"},{"location":"parallelism/#data-parallelism-the-foundation","title":"Data Parallelism: The Foundation","text":"<p>Data parallelism is the simplest and most common form of distributed training. Each GPU maintains a complete copy of the model and processes different batches:</p> <pre><code># Traditional DDP - each GPU has full model\nconfig = DeviceParameters(dp_replicate=8)  # 8 GPUs\n\n# FSDP2 - shards model across GPUs\nconfig = DeviceParameters(dp_shard=8)  # 8 GPUs, ~8x memory reduction\n</code></pre> <p>When to use DDP: - Model fits on single GPU - High-bandwidth interconnect available - Want simplest implementation</p> <p>When to use FSDP2: - Model too large for single GPU - Need to train larger models with same hardware - Can tolerate some communication overhead</p>"},{"location":"parallelism/#tensor-parallelism-splitting-layers","title":"Tensor Parallelism: Splitting Layers","text":"<p>Tensor parallelism splits individual layers across GPUs. As explained by Jeremy Jordan, there are two main approaches:</p>"},{"location":"parallelism/#column-partitioning","title":"Column Partitioning","text":"<p>Splits weight matrix along output dimension:</p> <pre><code># Weight W: [input_dim, output_dim]\n# Each GPU gets W_i: [input_dim, output_dim/n_gpus]\n\n# Forward: X @ W_i \u2192 Y_i (then all-gather)\n# Backward: Gradient flows naturally\n</code></pre>"},{"location":"parallelism/#row-partitioning","title":"Row Partitioning","text":"<p>Splits weight matrix along input dimension:</p> <pre><code># Weight W: [input_dim, output_dim]\n# Each GPU gets W_i: [input_dim/n_gpus, output_dim]\n\n# Forward: X_i @ W_i \u2192 partial Y (then all-reduce)\n# Backward: Need all-gather for input gradients\n</code></pre>"},{"location":"parallelism/#optimizing-communication","title":"Optimizing Communication","text":"<p>The Megatron-LM paper showed how clever partitioning reduces communication:</p> <pre><code>class OptimizedMLP(nn.Module, ParallelPlan):\n    def parallelize_plan(self):\n        return {\n            # First layer: column partition (output split)\n            \"fc1\": colwise_parallel(self.fc1),\n\n            # Activation: computed locally on each GPU\n\n            # Second layer: row partition (input split)\n            # Takes split input directly - no communication!\n            \"fc2\": rowwise_parallel(self.fc2),\n\n            # Only one all-reduce at the end\n        }\n</code></pre> <p>This pattern reduces communication by 50% compared to naive partitioning!</p>"},{"location":"parallelism/#pipeline-parallelism-splitting-stages","title":"Pipeline Parallelism: Splitting Stages","text":"<p>Pipeline parallelism splits the model into sequential stages, with each GPU responsible for a subset of layers:</p> <pre><code># Model split across 4 GPUs\nGPU 0: Embedding + Layers[0:8]\nGPU 1: Layers[8:16]  \nGPU 2: Layers[16:24]\nGPU 3: Layers[24:32] + Output\n\n# Microbatching keeps GPUs busy\nmicrobatch_size = batch_size // num_microbatches\n</code></pre> <p>Advantages: - Each GPU only stores its layers (high memory savings) - Only forward/backward activations communicated - Works well across nodes with slower interconnect</p> <p>Disadvantages: - Pipeline bubbles reduce efficiency - Requires careful load balancing - More complex implementation</p>"},{"location":"parallelism/#context-parallelism-splitting-sequences","title":"Context Parallelism: Splitting Sequences","text":"<p>For extremely long sequences, context parallelism splits the sequence dimension:</p> <pre><code># Sequence length 128K split across 4 GPUs\nGPU 0: tokens[0:32K]\nGPU 1: tokens[32K:64K]\nGPU 2: tokens[64K:96K]  \nGPU 3: tokens[96K:128K]\n\n# Attention requires all-to-all communication\n</code></pre>"},{"location":"parallelism/#network-topology-considerations","title":"Network Topology Considerations","text":"<p>Modern GPU clusters have hierarchical network structures that significantly impact parallelism choices. As noted in the Llama 3.1 training details:</p> <pre><code>Within node: 8 GPUs connected via NVLink (900 GB/s)\nWithin rack: 2 nodes connected via network switch\nWithin pod: 192 racks (3,072 GPUs)\nFull cluster: 8 pods (24,576 GPUs)\n</code></pre>"},{"location":"parallelism/#optimal-parallelism-placement","title":"Optimal Parallelism Placement","text":"<p>Based on network topology, place parallelism strategies hierarchically:</p> <pre><code># Optimal configuration for hierarchical networks\nconfig = DeviceParameters(\n    # Within node (NVLink - highest bandwidth)\n    tensor_parallel=8,  # Requires frequent communication\n\n    # Across nodes within rack (InfiniBand - medium bandwidth)  \n    pipeline_parallel=2,  # Only activations communicated\n\n    # Across racks (Ethernet - lowest bandwidth)\n    dp_shard=16,  # Only gradient sync required\n)\n\n# Total: 8 * 2 * 16 = 256 GPUs\n</code></pre>"},{"location":"parallelism/#example-llama-31-405b-configuration","title":"Example: Llama 3.1 405B Configuration","text":"<p>The Llama 3.1 training used a carefully optimized setup:</p> <pre><code># Llama 3.1 405B parallelism\ntensor_parallel = 8      # Within node (NVLink)\npipeline_parallel = 16   # Across nodes\ndata_parallel = 128      # Across everything\n\n# Total: 16,384 GPUs\n# Memory per GPU: ~50GB (405B params / 8 TP / 16 PP)\n</code></pre>"},{"location":"parallelism/#basic-configuration","title":"Basic Configuration","text":"<p>Configure parallelism through <code>DeviceParameters</code>:</p> <pre><code>from dream_trainer.configs import DeviceParameters\n\nconfig = DeviceParameters(\n    # Data parallelism\n    dp_replicate=2,      # 2-way DDP\n    dp_shard=4,          # 4-way FSDP within each DDP group\n\n    # Model parallelism\n    tensor_parallel=2,   # 2-way TP\n    pipeline_parallel=4, # 4 pipeline stages\n    context_parallel=2,  # 2-way sequence splitting\n\n    # Optimizations\n    compile_model=True,\n    loss_parallel=True,  # Parallel loss with TP\n    async_tensor_parallel=True,  # Overlap TP communication\n)\n</code></pre>"},{"location":"parallelism/#self-parallelizing-models-with-fsdp2_utils","title":"Self-Parallelizing Models with fsdp2_utils","text":"<p>The most elegant approach to parallelism in Dream Trainer is using <code>fsdp2_utils</code> to create models that know how to parallelize themselves. This encapsulates complex sharding logic within the model definition.</p>"},{"location":"parallelism/#the-pattern-models-that-parallelize-themselves","title":"The Pattern: Models That Parallelize Themselves","text":"<p>Instead of implementing parallelism in the trainer, models inherit from <code>FullyShard</code> and <code>ParallelPlan</code>:</p> <pre><code>from typing import Any\nimport torch.nn as nn\nfrom fsdp2_utils import FullyShard, ParallelPlan, apply_tensor_parallel, apply_fully_shard\nfrom fsdp2_utils.tensor_parallel.plan import (\n    colwise_parallel, \n    rowwise_parallel,\n    sequence_parallel,\n    prepare_module_input\n)\nfrom torch.distributed.fsdp import fully_shard\nfrom torch.distributed.tensor.placement_types import Replicate, Shard\n\nclass TransformerModel(nn.Module, FullyShard, ParallelPlan):\n    \"\"\"A transformer that knows how to parallelize itself\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = nn.ModuleList([\n            TransformerLayer(config) for _ in range(config.num_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def fully_shard(self, config: dict[str, Any]):\n        \"\"\"Define FSDP sharding strategy\"\"\"\n        # Shard each transformer layer independently\n        for layer in self.layers:\n            fully_shard(layer, **config)\n\n        # Shard embeddings and output separately\n        fully_shard(self.embeddings, **config)\n        fully_shard(self.lm_head, **config)\n\n    def parallelize_plan(self):\n        \"\"\"Define tensor parallel strategy\"\"\"\n        plan = {\n            # Embeddings: row-wise parallel (vocab dimension)\n            \"embeddings\": rowwise_parallel(\n                self.embeddings,\n                input_layouts=Replicate(),  # Input tokens are replicated\n                output_layouts=Shard(1),    # Output is sharded on sequence dim\n            ),\n\n            # Final layer norm: sequence parallel\n            \"norm\": sequence_parallel(self.norm),\n\n            # LM head: column-wise parallel (vocab dimension)\n            \"lm_head\": colwise_parallel(\n                self.lm_head,\n                input_layouts=Shard(1),     # Input is sharded\n                output_layouts=Replicate(), # Output is replicated\n                use_local_output=True,\n            ),\n        }\n\n        # Add plans for each transformer layer\n        for i, layer in enumerate(self.layers):\n            plan.update(layer.parallelize_plan(prefix=f\"layers.{i}\"))\n\n        return plan\n</code></pre>"},{"location":"parallelism/#implementing-parallelism-in-submodules","title":"Implementing Parallelism in Submodules","text":"<p>Each component defines its own parallelism strategy:</p> <pre><code>class TransformerLayer(nn.Module, ParallelPlan):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MultiHeadAttention(config)\n        self.mlp = MLP(config)\n        self.norm1 = RMSNorm(config.hidden_size)\n        self.norm2 = RMSNorm(config.hidden_size)\n\n    def parallelize_plan(self, prefix=\"\"):\n        \"\"\"Define TP plan for this layer\"\"\"\n        def add_prefix(name):\n            return f\"{prefix}.{name}\" if prefix else name\n\n        return {\n            # Attention: QKV column-wise, O row-wise\n            add_prefix(\"attention.q_proj\"): colwise_parallel(self.attention.q_proj),\n            add_prefix(\"attention.k_proj\"): colwise_parallel(self.attention.k_proj),\n            add_prefix(\"attention.v_proj\"): colwise_parallel(self.attention.v_proj),\n            add_prefix(\"attention.o_proj\"): rowwise_parallel(\n                self.attention.o_proj,\n                input_layouts=Shard(-1),  # Sharded on head dimension\n            ),\n\n            # MLP: standard Megatron-style parallelism\n            add_prefix(\"mlp.gate_proj\"): colwise_parallel(self.mlp.gate_proj),\n            add_prefix(\"mlp.up_proj\"): colwise_parallel(self.mlp.up_proj),\n            add_prefix(\"mlp.down_proj\"): rowwise_parallel(\n                self.mlp.down_proj,\n                input_layouts=Shard(-1),  # Sharded on hidden dimension\n            ),\n\n            # Layer norms use sequence parallelism\n            add_prefix(\"norm1\"): sequence_parallel(self.norm1),\n            add_prefix(\"norm2\"): sequence_parallel(self.norm2),\n        }\n</code></pre>"},{"location":"parallelism/#simplified-trainer-implementation","title":"Simplified Trainer Implementation","text":"<p>With self-parallelizing models, trainers become remarkably simple:</p> <pre><code>from fsdp2_utils import apply_tensor_parallel, apply_fully_shard\n\nclass MyTrainer(DreamTrainer):\n    def configure_models(self):\n        # Model knows how to parallelize itself!\n        self.model = TransformerModel(self.config)\n\n    def apply_tensor_parallel(self, tp_mesh: DeviceMesh):\n        \"\"\"Just delegate to the model's built-in TP strategy\"\"\"\n        apply_tensor_parallel(self.model, tp_mesh)\n\n    def apply_fully_shard(self, config: dict[str, Any]):\n        \"\"\"Just delegate to the model's built-in FSDP strategy\"\"\"\n        apply_fully_shard(self.model, config)\n</code></pre>"},{"location":"parallelism/#advanced-heterogeneous-parallelism-for-multi-modal-models","title":"Advanced: Heterogeneous Parallelism for Multi-Modal Models","text":"<p>Different model components can use different strategies:</p> <pre><code>class VisionLanguageModel(nn.Module, FullyShard, ParallelPlan):\n    \"\"\"Multi-modal model with component-specific parallelism\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.vision_encoder = VisionTransformer(config.vision)\n        self.text_encoder = TextTransformer(config.text)\n        self.cross_attention = CrossModalAttention(config)\n        self.projection = nn.Linear(config.hidden_size, config.output_size)\n\n    def fully_shard(self, config: dict[str, Any]):\n        \"\"\"Different sharding for different components\"\"\"\n        # Vision: larger sharding units (less communication)\n        vision_config = {**config, \"min_num_params_per_shard\": 50_000_000}\n        self.vision_encoder.fully_shard(vision_config)\n\n        # Text: standard sharding\n        self.text_encoder.fully_shard(config)\n\n        # Cross-attention: aggressive sharding (memory intensive)\n        cross_config = {**config, \"min_num_params_per_shard\": 10_000_000}\n        fully_shard(self.cross_attention, **cross_config)\n\n    def parallelize_plan(self):\n        \"\"\"Different TP strategies for different modalities\"\"\"\n        return {\n            # Vision doesn't use TP (compute bound, not memory bound)\n            \"vision_encoder\": prepare_module_input(\n                self.vision_encoder,\n                input_layouts=Replicate(),\n            ),\n\n            # Text uses full TP\n            **{f\"text_encoder.{k}\": v \n               for k, v in self.text_encoder.parallelize_plan().items()},\n\n            # Cross-attention uses custom strategy\n            \"cross_attention.q_proj\": colwise_parallel(\n                self.cross_attention.q_proj,\n                input_layouts=Replicate(),  # From vision\n            ),\n            \"cross_attention.kv_proj\": colwise_parallel(\n                self.cross_attention.kv_proj,\n                input_layouts=Shard(1),     # From text\n            ),\n        }\n</code></pre>"},{"location":"parallelism/#benefits-of-self-parallelizing-models","title":"Benefits of Self-Parallelizing Models","text":"<ol> <li>Encapsulation: Parallelism logic lives with model definition</li> <li>Reusability: Same model works in any trainer</li> <li>Clarity: Structure and strategy are co-located</li> <li>Composability: Complex models built from simple parallel components</li> <li>Type Safety: IDE understands the parallelism interface</li> </ol>"},{"location":"parallelism/#manual-parallelism-implementation","title":"Manual Parallelism Implementation","text":"<p>For cases where you need fine-grained control:</p>"},{"location":"parallelism/#manual-tensor-parallelism","title":"Manual Tensor Parallelism","text":"<pre><code>from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel\n\nclass MyTrainer(DreamTrainer):\n    def apply_tensor_parallel(self, tp_mesh: DeviceMesh):\n        # Manual TP for specific layers\n        for layer in self.model.layers:\n            plan = {\n                \"attention.wq\": ColwiseParallel(),\n                \"attention.wk\": ColwiseParallel(),\n                \"attention.wv\": ColwiseParallel(),\n                \"attention.wo\": RowwiseParallel(),\n                \"mlp.w1\": ColwiseParallel(),\n                \"mlp.w2\": RowwiseParallel(),\n            }\n            parallelize_module(layer, tp_mesh, plan)\n</code></pre>"},{"location":"parallelism/#manual-fsdp2","title":"Manual FSDP2","text":"<pre><code>from torch.distributed._composable.fsdp import fully_shard\n\nclass MyTrainer(DreamTrainer):\n    def apply_fully_shard(self, config: dict[str, Any]):\n        # Custom sharding per layer\n        for i, layer in enumerate(self.model.layers):\n            if i &lt; 10:  # First 10 layers\n                fully_shard(layer, **config)\n            else:  # Later layers use different config\n                custom_config = {**config, \"reshard_after_forward\": False}\n                fully_shard(layer, **custom_config)\n</code></pre>"},{"location":"parallelism/#manual-pipeline-parallelism","title":"Manual Pipeline Parallelism","text":"<pre><code>from torch.distributed.pipelining import pipeline, SplitPoint, PipelineStage\n\nclass MyTrainer(DreamTrainer):\n    def apply_pipeline_parallel(self, pp_mesh: DeviceMesh):\n        # Define split points\n        mb = [self.model.embeddings, *self.model.layers, self.model.output]\n\n        # Create pipeline stages\n        stages = [\n            PipelineStage(mb[:8], 0),   # First 8 layers\n            PipelineStage(mb[8:16], 1),  # Next 8 layers\n            PipelineStage(mb[16:24], 2), # Next 8 layers  \n            PipelineStage(mb[24:], 3),   # Rest\n        ]\n\n        # Create schedule\n        schedule = pipeline(\n            stages,\n            pp_mesh,\n            mb_size=self.config.pipeline_parallel_microbatch_size,\n        )\n\n        return {\"model\": (schedule, stages, True, True)}\n</code></pre>"},{"location":"parallelism/#combining-parallelism-strategies","title":"Combining Parallelism Strategies","text":""},{"location":"parallelism/#real-world-examples","title":"Real-World Examples","text":"<p>Based on model size and available hardware, here are recommended configurations:</p>"},{"location":"parallelism/#small-model-1b-parameters","title":"Small Model (&lt; 1B parameters)","text":"<pre><code># Fits on single GPU - use pure data parallelism\nconfig = DeviceParameters(\n    dp_replicate=num_gpus,  # Simple DDP\n    compile_model=True,     # Optimize single GPU perf\n)\n</code></pre>"},{"location":"parallelism/#medium-model-1b-70b-parameters","title":"Medium Model (1B - 70B parameters)","text":"<pre><code># Needs FSDP for memory, optional TP for speed\nconfig = DeviceParameters(\n    dp_shard=8,          # FSDP for memory efficiency\n    tensor_parallel=2,   # Optional: faster but more GPUs\n)\n</code></pre>"},{"location":"parallelism/#large-model-70b-500b-parameters","title":"Large Model (70B - 500B parameters)","text":"<pre><code># Requires both memory and compute parallelism\nconfig = DeviceParameters(\n    tensor_parallel=8,    # Split within nodes\n    pipeline_parallel=4,  # Split across nodes\n    dp_shard=16,         # FSDP across everything\n)\n</code></pre>"},{"location":"parallelism/#extreme-scale-500b-parameters","title":"Extreme Scale (500B+ parameters)","text":"<pre><code># Everything at maximum scale\nconfig = DeviceParameters(\n    tensor_parallel=8,     # Max TP within node\n    pipeline_parallel=16,  # Many pipeline stages\n    dp_replicate=2,       # HSDP: DDP across pods\n    dp_shard=64,          # FSDP within pods\n    context_parallel=4,    # For long sequences\n)\n</code></pre>"},{"location":"parallelism/#3d-parallelism-dp-tp-pp","title":"3D Parallelism (DP + TP + PP)","text":"<pre><code>config = DeviceParameters(\n    dp_shard=4,          # 4-way FSDP\n    tensor_parallel=2,   # 2-way TP  \n    pipeline_parallel=2, # 2 pipeline stages\n    # Total: 4 * 2 * 2 = 16 GPUs\n)\n</code></pre>"},{"location":"parallelism/#hsdp-hybrid-sharded-data-parallel","title":"HSDP (Hybrid Sharded Data Parallel)","text":"<p>HSDP combines the benefits of DDP and FSDP by creating a hierarchy:</p> <pre><code>config = DeviceParameters(\n    dp_replicate=2,  # 2 DDP groups (across pods)\n    dp_shard=4,      # 4-way FSDP within each group\n)\n\n# Communication pattern:\n# - Gradient reduce-scatter within FSDP groups (high bandwidth)\n# - Gradient all-reduce across DDP groups (low bandwidth)\n# Result: Less cross-pod communication\n</code></pre>"},{"location":"parallelism/#context-parallel-tensor-parallel","title":"Context Parallel + Tensor Parallel","text":"<pre><code>config = DeviceParameters(\n    context_parallel=4,  # 4-way sequence splitting\n    tensor_parallel=2,   # 2-way tensor parallel\n    # Enables 4x longer sequences with 2x model parallel\n)\n</code></pre>"},{"location":"parallelism/#performance-optimization","title":"Performance Optimization","text":""},{"location":"parallelism/#optimizing-for-your-hardware","title":"Optimizing for Your Hardware","text":"<ol> <li> <p>Measure Your Baseline <pre><code># Profile single GPU performance first\nwith torch.profiler.profile() as prof:\n    trainer.fit()\n\n# Key metrics:\n# - GPU utilization (target: &gt;90%)\n# - Memory bandwidth utilization\n# - Time spent in communication vs compute\n</code></pre></p> </li> <li> <p>Scale Gradually <pre><code># Start simple, add parallelism incrementally\nconfigs = [\n    DeviceParameters(dp_replicate=8),  # Pure DDP\n    DeviceParameters(dp_shard=8),      # Pure FSDP\n    DeviceParameters(dp_shard=4, tensor_parallel=2),  # FSDP+TP\n    DeviceParameters(dp_shard=2, tensor_parallel=2, pipeline_parallel=2),  # 3D\n]\n</code></pre></p> </li> <li> <p>Monitor Scaling Efficiency <pre><code># Perfect scaling: 2x GPUs = 2x throughput\nscaling_efficiency = (\n    throughput_ngpus / throughput_1gpu\n) / n_gpus\n\n# Good: &gt;0.8\n# Okay: 0.6-0.8  \n# Poor: &lt;0.6 (reconsider strategy)\n</code></pre></p> </li> </ol>"},{"location":"parallelism/#1-fsdp-prefetching","title":"1. FSDP Prefetching","text":"<pre><code>from dream_trainer.callbacks import OptimizeFSDP\n\nconfig.callbacks = CallbackCollection([\n    OptimizeFSDP(prefetch=2),  # Prefetch next 2 modules\n])\n</code></pre>"},{"location":"parallelism/#2-async-tensor-parallelism","title":"2. Async Tensor Parallelism","text":"<pre><code>config = DeviceParameters(\n    tensor_parallel=4,\n    async_tensor_parallel=True,  # Overlap TP comms\n)\n</code></pre>"},{"location":"parallelism/#3-compiled-autograd","title":"3. Compiled Autograd","text":"<pre><code>config = DeviceParameters(\n    compile_model=True,\n    enable_compiled_autograd=True,  # Compile backward too\n)\n</code></pre>"},{"location":"parallelism/#4-communication-optimization","title":"4. Communication Optimization","text":"<pre><code># Set NCCL environment variables\nimport os\nos.environ[\"NCCL_ALGO\"] = \"Tree\"  # Better for small messages\nos.environ[\"NCCL_PROTO\"] = \"Simple\"  # Lower latency\nos.environ[\"NCCL_NSOCKS_PERTHREAD\"] = \"4\"\nos.environ[\"NCCL_SOCKET_NTHREADS\"] = \"8\"\n</code></pre>"},{"location":"parallelism/#5-batch-size-optimization","title":"5. Batch Size Optimization","text":"<p>Based on gradient noise scale theory:</p> <pre><code># Start with smaller batch size\ninitial_batch_size = 256\n\n# Measure gradient noise scale\n# (See \"An Empirical Model of Large-Batch Training\")\n\n# Progressively increase batch size during training\nschedule = {\n    0: 256,        # Initial: high gradient noise\n    100_000: 512,  # Model improving, can use larger batches\n    500_000: 1024, # Near convergence, maximize efficiency\n}\n</code></pre>"},{"location":"parallelism/#debugging-tips","title":"Debugging Tips","text":""},{"location":"parallelism/#1-verify-sharding","title":"1. Verify Sharding","text":"<pre><code>def training_step(self, batch, batch_idx):\n    if batch_idx == 0:\n        # Check parameter sharding\n        for name, param in self.model.named_parameters():\n            if hasattr(param, 'placements'):\n                print(f\"{name}: {param.placements}\")\n</code></pre>"},{"location":"parallelism/#2-monitor-memory-usage","title":"2. Monitor Memory Usage","text":"<pre><code>from dream_trainer.utils import log_memory_usage\n\ndef training_step(self, batch, batch_idx):\n    log_memory_usage(\"before_forward\")\n    output = self.model(batch)\n    log_memory_usage(\"after_forward\")\n</code></pre>"},{"location":"parallelism/#3-profile-communication","title":"3. Profile Communication","text":"<pre><code>import torch.distributed as dist\n\n# Enable NCCL debug logging\nos.environ[\"NCCL_DEBUG\"] = \"INFO\"\n\n# Profile with PyTorch profiler\nwith torch.profiler.profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n) as prof:\n    trainer.fit()\n</code></pre>"},{"location":"parallelism/#4-test-incrementally","title":"4. Test Incrementally","text":"<ol> <li>Start with single GPU</li> <li>Add DP (either DDP or FSDP)</li> <li>Add TP</li> <li>Add PP</li> <li>Combine strategies</li> </ol>"},{"location":"parallelism/#best-practices","title":"Best Practices","text":"<ol> <li>Understand Your Bottleneck</li> <li>Memory bound? \u2192 Use FSDP or PP</li> <li>Compute bound? \u2192 Use TP for large matmuls</li> <li> <p>Communication bound? \u2192 Optimize placement based on topology</p> </li> <li> <p>Profile Before Optimizing</p> </li> <li>Measure baseline single-GPU performance</li> <li>Identify whether you're compute or memory bound</li> <li> <p>Profile communication patterns</p> </li> <li> <p>Consider Total System Design</p> </li> <li>Network topology (NVLink vs InfiniBand vs Ethernet)</li> <li>Model architecture (depth vs width)</li> <li> <p>Batch size requirements (gradient noise scale)</p> </li> <li> <p>Use Self-Parallelizing Models</p> </li> <li>Encapsulate complexity in model definitions</li> <li>Makes experimentation easier</li> <li> <p>Improves code reusability</p> </li> <li> <p>Test Scaling Incrementally</p> </li> <li>Start with single GPU</li> <li>Add data parallelism</li> <li>Add model parallelism only if needed</li> <li>Verify near-linear scaling at each step</li> </ol>"},{"location":"parallelism/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Configuration Guide for detailed parameter options</li> <li>Check Examples for real-world parallelism usage</li> <li>See Performance Guide for optimization tips</li> <li>Review Jeremy Jordan's distributed training guide for more theory</li> </ul> <p>Remember: The best parallelism strategy depends on your model architecture, hardware topology, and training requirements. Dream Trainer gives you the flexibility to experiment and find what works best! \ud83d\ude80</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"trainer-guide/","title":"Trainer Guide","text":"<p>This guide explains how to create and customize trainers in Dream Trainer.</p>"},{"location":"trainer-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Trainer</li> <li>Training Loop</li> <li>Model Configuration</li> <li>Optimizer Configuration</li> <li>DataLoader Configuration</li> <li>Advanced Features</li> </ul>"},{"location":"trainer-guide/#basic-trainer","title":"Basic Trainer","text":"<p>The simplest way to create a trainer is to extend <code>DreamTrainer</code>:</p> <pre><code>from dream_trainer import DreamTrainer, DreamTrainerConfig\nimport torch.nn as nn\n\nclass MyTrainer(DreamTrainer):\n    def __init__(self, config: DreamTrainerConfig, model: nn.Module):\n        super().__init__(config)\n        self.model = model\n</code></pre>"},{"location":"trainer-guide/#required-methods","title":"Required Methods","text":"<p>Every trainer must implement these methods:</p> <pre><code>class MyTrainer(DreamTrainer):\n    def configure_models(self):\n        \"\"\"Configure your model(s) here\"\"\"\n        pass\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimizer(s)\"\"\"\n        pass\n\n    def configure_dataloaders(self):\n        \"\"\"Configure train and validation dataloaders\"\"\"\n        pass\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Define a single training step\"\"\"\n        pass\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Define a single validation step\"\"\"\n        pass\n</code></pre>"},{"location":"trainer-guide/#training-loop","title":"Training Loop","text":""},{"location":"trainer-guide/#training-step","title":"Training Step","text":"<p>The training step defines what happens in each iteration:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # Unpack batch\n    inputs, targets = batch\n\n    # Forward pass\n    outputs = self.model(inputs)\n    loss = self.criterion(outputs, targets)\n\n    # Backward pass (handled automatically)\n    self.backward(loss)\n\n    # Return metrics to log\n    return {\n        \"loss\": loss,\n        \"accuracy\": self.compute_accuracy(outputs, targets)\n    }\n</code></pre>"},{"location":"trainer-guide/#validation-step","title":"Validation Step","text":"<p>The validation step is similar but without backward pass:</p> <pre><code>def validation_step(self, batch, batch_idx):\n    # Unpack batch\n    inputs, targets = batch\n\n    # Forward pass\n    outputs = self.model(inputs)\n    loss = self.criterion(outputs, targets)\n\n    # Return metrics to log\n    return {\n        \"val_loss\": loss,\n        \"val_accuracy\": self.compute_accuracy(outputs, targets)\n    }\n</code></pre>"},{"location":"trainer-guide/#custom-training-loop","title":"Custom Training Loop","text":"<p>For more control, you can override the training loop:</p> <pre><code>def training_loop(self):\n    \"\"\"Custom training loop\"\"\"\n    for epoch in range(self.current_epoch, self.config.n_epochs):\n        # Training\n        self.model.train()\n        for batch_idx, batch in enumerate(self.train_loader):\n            # Custom training logic\n            outputs = self.training_step(batch, batch_idx)\n\n            # Custom logging\n            self.log_metrics(outputs)\n\n        # Validation\n        if self.should_validate():\n            self.validation_loop()\n</code></pre>"},{"location":"trainer-guide/#model-configuration","title":"Model Configuration","text":""},{"location":"trainer-guide/#single-model","title":"Single Model","text":"<p>Configure a single model:</p> <pre><code>def configure_models(self):\n    \"\"\"Configure a single model\"\"\"\n    # Model is automatically moved to device\n    # and wrapped with distributed training wrappers\n    pass\n</code></pre>"},{"location":"trainer-guide/#multiple-models","title":"Multiple Models","text":"<p>Configure multiple models (e.g., GAN):</p> <pre><code>def configure_models(self):\n    \"\"\"Configure multiple models\"\"\"\n    # Generator\n    self.generator = self.generator.to(self.device)\n    if self.is_distributed:\n        self.generator = self.wrap_model(self.generator)\n\n    # Discriminator\n    self.discriminator = self.discriminator.to(self.device)\n    if self.is_distributed:\n        self.discriminator = self.wrap_model(self.discriminator)\n</code></pre>"},{"location":"trainer-guide/#model-compilation","title":"Model Compilation","text":"<p>Enable model compilation for better performance:</p> <pre><code>def configure_models(self):\n    \"\"\"Configure model with compilation\"\"\"\n    if self.config.compile_model:\n        self.model = torch.compile(self.model)\n</code></pre>"},{"location":"trainer-guide/#optimizer-configuration","title":"Optimizer Configuration","text":""},{"location":"trainer-guide/#basic-optimizer","title":"Basic Optimizer","text":"<p>Configure a single optimizer:</p> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure a single optimizer\"\"\"\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=self.config.learning_rate,\n        weight_decay=self.config.weight_decay\n    )\n</code></pre>"},{"location":"trainer-guide/#multiple-optimizers","title":"Multiple Optimizers","text":"<p>Configure multiple optimizers:</p> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure multiple optimizers\"\"\"\n    # Generator optimizer\n    self.g_optimizer = torch.optim.AdamW(\n        self.generator.parameters(),\n        lr=self.config.g_lr\n    )\n\n    # Discriminator optimizer\n    self.d_optimizer = torch.optim.AdamW(\n        self.discriminator.parameters(),\n        lr=self.config.d_lr\n    )\n</code></pre>"},{"location":"trainer-guide/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Add learning rate schedulers:</p> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer with scheduler\"\"\"\n    # Optimizer\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=self.config.learning_rate\n    )\n\n    # Scheduler\n    self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        self.optimizer,\n        T_max=self.config.n_epochs\n    )\n</code></pre>"},{"location":"trainer-guide/#dataloader-configuration","title":"DataLoader Configuration","text":""},{"location":"trainer-guide/#basic-dataloaders","title":"Basic DataLoaders","text":"<p>Configure train and validation dataloaders:</p> <pre><code>def configure_dataloaders(self):\n    \"\"\"Configure basic dataloaders\"\"\"\n    # Training data\n    train_dataset = MyDataset(\n        data_dir=self.config.train_data_dir,\n        split=\"train\"\n    )\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.train_batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers\n    )\n\n    # Validation data\n    val_dataset = MyDataset(\n        data_dir=self.config.val_data_dir,\n        split=\"val\"\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=self.config.val_batch_size,\n        shuffle=False,\n        num_workers=self.config.num_workers\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"trainer-guide/#distributed-dataloaders","title":"Distributed DataLoaders","text":"<p>Configure distributed dataloaders:</p> <pre><code>def configure_dataloaders(self):\n    \"\"\"Configure distributed dataloaders\"\"\"\n    # Training data\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=self.world_size,\n        rank=self.global_rank\n    )\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.train_batch_size,\n        sampler=train_sampler,\n        num_workers=self.config.num_workers\n    )\n\n    # Validation data\n    val_sampler = DistributedSampler(\n        val_dataset,\n        num_replicas=self.world_size,\n        rank=self.global_rank,\n        shuffle=False\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=self.config.val_batch_size,\n        sampler=val_sampler,\n        num_workers=self.config.num_workers\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"trainer-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"trainer-guide/#custom-metrics","title":"Custom Metrics","text":"<p>Add custom metrics:</p> <pre><code>def compute_metrics(self, outputs, targets):\n    \"\"\"Compute custom metrics\"\"\"\n    return {\n        \"accuracy\": self.compute_accuracy(outputs, targets),\n        \"f1_score\": self.compute_f1(outputs, targets),\n        \"precision\": self.compute_precision(outputs, targets),\n        \"recall\": self.compute_recall(outputs, targets)\n    }\n</code></pre>"},{"location":"trainer-guide/#gradient-clipping","title":"Gradient Clipping","text":"<p>Enable gradient clipping:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # ... training logic ...\n\n    # Gradient clipping\n    if self.config.gradient_clip_val is not None:\n        self.clip_gradients(\n            self.optimizer,\n            max_norm=self.config.gradient_clip_val\n        )\n</code></pre>"},{"location":"trainer-guide/#mixed-precision","title":"Mixed Precision","text":"<p>Enable mixed precision training:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # Automatic mixed precision\n    with self.autocast():\n        outputs = self.model(inputs)\n        loss = self.criterion(outputs, targets)\n\n    # Backward pass with scaling\n    self.backward(loss)\n</code></pre>"},{"location":"trainer-guide/#checkpointing","title":"Checkpointing","text":"<p>Save and load checkpoints:</p> <pre><code>def save_checkpoint(self, path):\n    \"\"\"Save checkpoint\"\"\"\n    checkpoint = {\n        \"model\": self.model.state_dict(),\n        \"optimizer\": self.optimizer.state_dict(),\n        \"epoch\": self.current_epoch,\n        \"config\": self.config\n    }\n    torch.save(checkpoint, path)\n\ndef load_checkpoint(self, path):\n    \"\"\"Load checkpoint\"\"\"\n    checkpoint = torch.load(path)\n    self.model.load_state_dict(checkpoint[\"model\"])\n    self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    self.current_epoch = checkpoint[\"epoch\"]\n</code></pre>"},{"location":"trainer-guide/#custom-callbacks","title":"Custom Callbacks","text":"<p>Add custom callbacks:</p> <pre><code>class MyCallback(Callback):\n    def on_train_batch_end(self, trainer, outputs, batch, batch_idx):\n        # Custom logic after each training batch\n        pass\n\n    def on_validation_batch_end(self, trainer, outputs, batch, batch_idx):\n        # Custom logic after each validation batch\n        pass\n</code></pre>"},{"location":"trainer-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Keep It Simple: Start with basic implementation, then add features</li> <li>Use Type Hints: Add type hints for better IDE support</li> <li>Document Methods: Add docstrings to explain functionality</li> <li>Handle Errors: Add proper error handling and logging</li> <li>Test Thoroughly: Write unit tests for your trainer</li> <li>Profile Performance: Monitor memory usage and training speed</li> <li>Use Callbacks: Extend functionality through callbacks</li> <li>Follow PyTorch: Follow PyTorch best practices and patterns</li> </ol> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the Dream Trainer API Reference. This documentation provides detailed information about all public classes, functions, and modules in the Dream Trainer framework.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>Dream Trainer is built with a modular, composable architecture. The main components are:</p> <ul> <li>Trainers: Core training loop implementations</li> <li>Mixins: Composable functionality that can be mixed into trainers</li> <li>Callbacks: Hooks for extending training behavior</li> <li>Configuration: Type-safe configuration management</li> <li>Utilities: Helper functions and utilities</li> </ul>"},{"location":"api/#quick-links","title":"Quick Links","text":""},{"location":"api/#core-components","title":"Core Components","text":"<ul> <li>AbstractTrainer - Base interface for all trainers</li> <li>BaseTrainer - Default trainer implementation</li> <li>DreamTrainer - Production-ready trainer with all features</li> </ul>"},{"location":"api/#mixins","title":"Mixins","text":"<ul> <li>Setup Mixins - Model, optimizer, and dataloader setup</li> <li>Evaluation Mixins - Metrics and evaluation</li> <li>Logger Mixins - Logging integrations (WandB, TensorBoard)</li> <li>Quantization Mixins - FP8 and INT8 quantization</li> </ul>"},{"location":"api/#callbacks","title":"Callbacks","text":"<ul> <li>Callback Base - Base callback interface</li> <li>Checkpoint Callbacks - Model checkpointing</li> <li>Monitoring Callbacks - Training monitoring</li> <li>Performance Callbacks - Performance optimization</li> </ul>"},{"location":"api/#configuration","title":"Configuration","text":"<ul> <li>Parameter Classes - Configuration parameters</li> <li>Device Config - Device and parallelism settings</li> <li>Training Config - Training hyperparameters</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>World Management - Distributed training context</li> <li>Data Utilities - Data loading helpers</li> <li>Common Utilities - General utilities</li> </ul>"},{"location":"api/#usage-pattern","title":"Usage Pattern","text":"<p>Dream Trainer follows a composition pattern where trainers are built by combining mixins:</p> <pre><code>from dream_trainer import BaseTrainer\nfrom dream_trainer.mixins import ModelSetupMixin, OptimizerSetupMixin\n\nclass MyTrainer(BaseTrainer, ModelSetupMixin, OptimizerSetupMixin):\n    def training_step(self, batch, batch_idx):\n        # Your training logic here\n        pass\n</code></pre>"},{"location":"api/#finding-what-you-need","title":"Finding What You Need","text":""},{"location":"api/#by-task","title":"By Task","text":"<ul> <li>Starting a new project: See AbstractTrainer</li> <li>Setting up distributed training: See World Management</li> <li>Adding logging: See Logger Mixins</li> <li>Implementing checkpointing: See Checkpoint Callbacks</li> <li>Optimizing performance: See Performance Callbacks</li> </ul>"},{"location":"api/#by-integration","title":"By Integration","text":"<ul> <li>PyTorch Lightning users: Start with BaseTrainer</li> <li>HuggingFace users: See DreamTrainer</li> <li>Custom framework users: Start with AbstractTrainer</li> </ul>"},{"location":"api/#api-stability","title":"API Stability","text":"<p>API Stability Guidelines</p> <ul> <li>Classes and functions marked as public (not prefixed with <code>_</code>) are stable</li> <li>Internal APIs (prefixed with <code>_</code>) may change between minor versions</li> <li>Deprecation warnings will be provided for at least one minor version</li> </ul>"},{"location":"api/#type-hints","title":"Type Hints","text":"<p>All public APIs in Dream Trainer are fully type-hinted. We recommend using a type checker like <code>mypy</code> or <code>pyright</code> for the best development experience.</p>"},{"location":"api/#contributing","title":"Contributing","text":"<p>See our Contributing Guide for information on adding or improving API documentation. </p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/base/","title":"Callback System","text":"<p>The callback system in Dream Trainer provides a powerful way to extend and customize the training loop without modifying the core trainer code. Callbacks can hook into various points of the training lifecycle to add functionality like logging, checkpointing, profiling, and more.</p>"},{"location":"api/callbacks/base/#overview","title":"Overview","text":"<p>Callbacks provide: - Hooks at every stage of the training lifecycle - Type-safe dependency injection for trainer mixins - Automatic distributed training support - State management for resumable training - Context managers for training/validation steps</p>"},{"location":"api/callbacks/base/#base-classes","title":"Base Classes","text":""},{"location":"api/callbacks/base/#callback","title":"Callback","text":"<p>The base callback class that all callbacks inherit from:</p>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback","title":"Callback","text":"<p>A base class for any hooks that need to be called during the training loop.</p> <p>When subclassing, instantiate the generic with the mixins you need.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback-functions","title":"Functions","text":""},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_launch","title":"pre_launch","text":"<pre><code>pre_launch()\n</code></pre> <p>Called before the launch of the distributed world.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_launch(self):\n    \"\"\"\n    Called before the launch of the distributed world.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_configure","title":"pre_configure","text":"<pre><code>pre_configure()\n</code></pre> <p>Called before the model, optimizers, schedulers and dataloaders are configured.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_configure(self):\n    \"\"\"\n    Called before the model, optimizers, schedulers and dataloaders are configured.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_configure","title":"post_configure","text":"<pre><code>post_configure()\n</code></pre> <p>Called after the model, optimizers, schedulers and dataloaders are configured.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_configure(self):\n    \"\"\"\n    Called after the model, optimizers, schedulers and dataloaders are configured.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_setup","title":"pre_setup","text":"<pre><code>pre_setup()\n</code></pre> <p>Called before the model, optimizers, schedulers and dataloaders are setup.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_setup(self):\n    \"\"\"\n    Called before the model, optimizers, schedulers and dataloaders are setup.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_setup","title":"post_setup","text":"<pre><code>post_setup()\n</code></pre> <p>Called after the model, optimizers, schedulers and dataloaders are setup.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_setup(self):\n    \"\"\"\n    Called after the model, optimizers, schedulers and dataloaders are setup.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_fit","title":"pre_fit","text":"<pre><code>pre_fit()\n</code></pre> <p>Called when the fit process starts.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_fit(self):\n    \"\"\"\n    Called when the fit process starts.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_fit","title":"post_fit","text":"<pre><code>post_fit()\n</code></pre> <p>Called when the fit process ends.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_fit(self):\n    \"\"\"\n    Called when the fit process ends.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_epoch","title":"pre_epoch","text":"<pre><code>pre_epoch()\n</code></pre> <p>Called at the beginning of each epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_epoch(self):\n    \"\"\"\n    Called at the beginning of each epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_epoch","title":"post_epoch","text":"<pre><code>post_epoch()\n</code></pre> <p>Called at the end of each epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_epoch(self):\n    \"\"\"\n    Called at the end of each epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_train_epoch","title":"pre_train_epoch","text":"<pre><code>pre_train_epoch()\n</code></pre> <p>Called at the beginning of each training epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_train_epoch(self):\n    \"\"\"\n    Called at the beginning of each training epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_train_epoch","title":"post_train_epoch","text":"<pre><code>post_train_epoch(result: dict[str, Any])\n</code></pre> <p>Called at the end of each training epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>result</code> <p>The result of the training epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_train_epoch(self, result: dict[str, Any]):\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        result (dict[str, Any]): The result of the training epoch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_train_step","title":"pre_train_step","text":"<pre><code>pre_train_step(batch: dict[str, Any], batch_idx: int)\n</code></pre> <p>Called at the start of each training step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>batch</code> <p>The current batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>batch_idx</code> <p>The index of the current batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_train_step(self, batch: dict[str, Any], batch_idx: int):\n    \"\"\"\n    Called at the start of each training step.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        batch (dict[str, Any]): The current batch.\n        batch_idx (int): The index of the current batch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_train_step","title":"post_train_step","text":"<pre><code>post_train_step(result: dict[str, Any], batch_idx: int)\n</code></pre> <p>Called at the end of each training step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>result</code> <p>The result of the training step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Any</code> </p> <code>batch_idx</code> <p>The index of the current batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_train_step(self, result: dict[str, Any], batch_idx: int):\n    \"\"\"\n    Called at the end of each training step.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        result (Any): The result of the training step.\n        batch_idx (int): The index of the current batch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_optimizer_step","title":"pre_optimizer_step","text":"<pre><code>pre_optimizer_step(model: Module, optimizer: Optimizer)\n</code></pre> <p>Called at the before each optimizer step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_optimizer_step(self, model: nn.Module, optimizer: Optimizer):\n    \"\"\"\n    Called at the before each optimizer step.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_optimizer_step","title":"post_optimizer_step","text":"<pre><code>post_optimizer_step(model: Module, optimizer: Optimizer)\n</code></pre> <p>Called at the end of each optimizer step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_optimizer_step(self, model: nn.Module, optimizer: Optimizer):\n    \"\"\"\n    Called at the end of each optimizer step.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_optimizer_zero_grad","title":"pre_optimizer_zero_grad","text":"<pre><code>pre_optimizer_zero_grad(model: Module, optimizer: Optimizer)\n</code></pre> <p>Called at the before each optimizer zero grad.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_optimizer_zero_grad(self, model: nn.Module, optimizer: Optimizer):\n    \"\"\"\n    Called at the before each optimizer zero grad.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_optimizer_zero_grad","title":"post_optimizer_zero_grad","text":"<pre><code>post_optimizer_zero_grad(model: Module, optimizer: Optimizer)\n</code></pre> <p>Called at the end of each optimizer zero grad.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_optimizer_zero_grad(self, model: nn.Module, optimizer: Optimizer):\n    \"\"\"\n    Called at the end of each optimizer zero grad.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_validation_epoch","title":"pre_validation_epoch","text":"<pre><code>pre_validation_epoch()\n</code></pre> <p>Called at the beginning of each validation epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_validation_epoch(self):\n    \"\"\"\n    Called at the beginning of each validation epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_validation_epoch","title":"post_validation_epoch","text":"<pre><code>post_validation_epoch(result: dict[str, Any])\n</code></pre> <p>Called at the end of each validation epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>result</code> <p>The result of the validation epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_validation_epoch(self, result: dict[str, Any]):\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        result (Any): The result of the validation epoch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.pre_validation_step","title":"pre_validation_step","text":"<pre><code>pre_validation_step(batch: dict[str, Any], batch_idx: int)\n</code></pre> <p>Called at the start of each validation step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>batch_idx</code> <p>The index of the current batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def pre_validation_step(self, batch: dict[str, Any], batch_idx: int):\n    \"\"\"\n    Called at the start of each validation step.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        batch_idx (int): The index of the current batch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.post_validation_step","title":"post_validation_step","text":"<pre><code>post_validation_step(result: dict[str, Any], batch_idx: int)\n</code></pre> <p>Called at the end of each validation step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>trainer</code> <p>The trainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>FSDP2Trainer</code> </p> <code>result</code> <p>The result of the validation step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Any</code> </p> <code>batch_idx</code> <p>The index of the current batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def post_validation_step(self, result: dict[str, Any], batch_idx: int):\n    \"\"\"\n    Called at the end of each validation step.\n\n    Args:\n        trainer (FSDP2Trainer): The trainer instance.\n        result (Any): The result of the validation step.\n        batch_idx (int): The index of the current batch.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.train_context","title":"train_context","text":"<pre><code>train_context() -&gt; contextlib._GeneratorContextManager\n</code></pre> <p>Training steps happen under this context manager.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def train_context(self) -&gt; contextlib._GeneratorContextManager:\n    \"\"\"\n    Training steps happen under this context manager.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.Callback.validation_context","title":"validation_context","text":"<pre><code>validation_context() -&gt; contextlib._GeneratorContextManager\n</code></pre> <p>Validation steps happen under this context manager.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def validation_context(self) -&gt; contextlib._GeneratorContextManager:\n    \"\"\"\n    Validation steps happen under this context manager.\n    \"\"\"\n    raise NotImplementedError(\"This function should never be called\")\n</code></pre>"},{"location":"api/callbacks/base/#rankzerocallback","title":"RankZeroCallback","text":"<p>A special callback that only executes on rank 0 in distributed training:</p>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.RankZeroCallback","title":"RankZeroCallback","text":"<p>Equivalent to the base Callback, but ensures all functions are only called on the rank zero process.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/base/#callbackcollection","title":"CallbackCollection","text":"<p>Manages multiple callbacks efficiently:</p>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection","title":"CallbackCollection","text":"<pre><code>CallbackCollection(callbacks: Iterable[Callback | RankZeroCallback] | dict[str, Callback | RankZeroCallback] | Iterable[tuple[str, Callback | RankZeroCallback]] = [])\n</code></pre> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    callbacks: Iterable[Callback | RankZeroCallback]\n    | dict[str, Callback | RankZeroCallback]\n    | Iterable[tuple[str, Callback | RankZeroCallback]] = [],\n):\n    # Get all methods defined in Callback above\n    self.trigger_names = {m for m in dir(Callback) if not m.startswith(\"_\")}\n\n    # Allow callback initialization to match dict initialization\n    if not isinstance(callbacks, dict):\n        callbacks = [\n            callback\n            if isinstance(callback, tuple)\n            else (callback.__class__.__name__, callback)\n            for callback in callbacks\n        ]\n\n    super().__init__(callbacks)  # type: ignore\n    self.refresh()\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection-functions","title":"Functions","text":""},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection.initialize","title":"initialize","text":"<pre><code>initialize(trainer: AbstractTrainer)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def initialize(self, trainer: AbstractTrainer):\n    for callback in self.values():\n        deps = deps if isinstance(deps := callback._dependency, tuple) else (deps,)\n        assert all(isinstance(trainer, dependency) for dependency in deps)\n\n        callback.trainer = trainer\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection.state_dict","title":"state_dict","text":"<pre><code>state_dict()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def state_dict(self):\n    return {\n        name: getattr(callback, \"state_dict\")()\n        for name, callback in self.items()\n        if hasattr(callback, \"state_dict\")\n    }\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dicts: dict[str, dict[str, Any]], trainer: AbstractTrainer)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def load_state_dict(\n    self,\n    state_dicts: dict[str, dict[str, Any]],\n    trainer: AbstractTrainer,\n):\n    for name, state_dict in state_dicts.items():\n        getattr(self[name], \"load_state_dict\")(state_dict, trainer)\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection.append","title":"append","text":"<pre><code>append(callback: Callback | RankZeroCallback)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def append(self, callback: \"Callback | RankZeroCallback\"):\n    self[callback.__class__.__name__] = callback\n</code></pre>"},{"location":"api/callbacks/base/#dream_trainer.callbacks.CallbackCollection.refresh","title":"refresh","text":"<pre><code>refresh()\n</code></pre> <p>Organizes callbacks by method name for efficient execution.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/callback.py</code> <pre><code>def refresh(self):\n    \"\"\"\n    Organizes callbacks by method name for efficient execution.\n    \"\"\"\n    self.triggers = {\n        trigger: [\n            callback\n            for callback in self.values()\n            # Only add triggers that were overridden in the callback\n            if not getattr(callback, trigger).__qualname__.startswith(\"Callback.\")\n        ]\n        for trigger in self.trigger_names\n    }\n</code></pre>"},{"location":"api/callbacks/base/#creating-custom-callbacks","title":"Creating Custom Callbacks","text":""},{"location":"api/callbacks/base/#basic-callback","title":"Basic Callback","text":"<pre><code>from dream_trainer.callbacks import Callback\nfrom dream_trainer.trainer import BaseTrainer\n\nclass SimpleLoggingCallback(Callback[BaseTrainer]):\n    \"\"\"Log training progress to a file.\"\"\"\n\n    def __init__(self, log_file: str):\n        self.log_file = log_file\n\n    def pre_fit(self):\n        # Open log file when training starts\n        self.file = open(self.log_file, \"w\")\n        self.file.write(f\"Training started for {self.trainer.experiment}\\n\")\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        # Log loss after each step\n        loss = result.get(\"loss\")\n        if loss is not None:\n            self.file.write(f\"Step {batch_idx}: loss={loss:.4f}\\n\")\n\n    def post_fit(self):\n        # Close file when training ends\n        self.file.write(\"Training completed\\n\")\n        self.file.close()\n</code></pre>"},{"location":"api/callbacks/base/#callback-with-dependencies","title":"Callback with Dependencies","text":"<p>Callbacks can specify dependencies on trainer mixins using generics:</p> <pre><code>from dream_trainer.callbacks import Callback\nfrom dream_trainer.trainer.mixins import EvalMetricMixin, WandBLoggerMixin\n\nclass MetricLoggingCallback(Callback[EvalMetricMixin &amp; WandBLoggerMixin]):\n    \"\"\"Log metrics to WandB after validation.\"\"\"\n\n    def post_validation_epoch(self, result: dict[str, Any]):\n        # Access metrics from EvalMetricMixin\n        metrics = self.trainer.named_metrics()\n\n        # Compute and log metrics using WandBLoggerMixin\n        for name, metric in metrics.items():\n            value = metric.compute()\n            self.trainer.log_scalar(f\"val/{name}\", value)\n</code></pre>"},{"location":"api/callbacks/base/#rank-zero-callback","title":"Rank Zero Callback","text":"<p>For operations that should only run on the main process:</p> <pre><code>from dream_trainer.callbacks import RankZeroCallback\n\nclass ModelSummaryCallback(RankZeroCallback[BaseTrainer]):\n    \"\"\"Print model summary only on rank 0.\"\"\"\n\n    def post_setup(self):\n        # This only runs on rank 0\n        total_params = sum(p.numel() for p in self.trainer.model.parameters())\n        trainable_params = sum(\n            p.numel() for p in self.trainer.model.parameters() if p.requires_grad\n        )\n\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n</code></pre>"},{"location":"api/callbacks/base/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p>The callback system provides hooks at every stage of training:</p>"},{"location":"api/callbacks/base/#initialization-phase","title":"Initialization Phase","text":"<pre><code>class InitializationCallback(Callback[BaseTrainer]):\n    def pre_launch(self):\n        \"\"\"Before distributed world is created.\"\"\"\n        print(\"Setting up environment variables\")\n\n    def pre_configure(self):\n        \"\"\"Before models/optimizers are configured.\"\"\"\n        print(\"Preparing configuration\")\n\n    def post_configure(self):\n        \"\"\"After configuration but before setup.\"\"\"\n        print(\"Configuration complete\")\n\n    def pre_setup(self):\n        \"\"\"Before models are materialized.\"\"\"\n        print(\"Beginning setup\")\n\n    def post_setup(self):\n        \"\"\"After all components are ready.\"\"\"\n        print(\"Setup complete\")\n</code></pre>"},{"location":"api/callbacks/base/#training-phase","title":"Training Phase","text":"<pre><code>class TrainingCallback(Callback[BaseTrainer]):\n    def pre_fit(self):\n        \"\"\"Training is about to start.\"\"\"\n        self.start_time = time.time()\n\n    def pre_epoch(self):\n        \"\"\"Before each epoch (train + val).\"\"\"\n        self.epoch_start = time.time()\n\n    def pre_train_epoch(self):\n        \"\"\"Before training epoch.\"\"\"\n        print(f\"Starting epoch {self.trainer.current_epoch}\")\n\n    def pre_train_step(self, batch: dict[str, Any], batch_idx: int):\n        \"\"\"Before each training step.\"\"\"\n        # Modify batch if needed\n        return batch\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        \"\"\"After training step.\"\"\"\n        # Process results\n        pass\n\n    def post_train_epoch(self, result: dict[str, Any]):\n        \"\"\"After training epoch.\"\"\"\n        print(f\"Epoch {self.trainer.current_epoch} complete\")\n\n    def post_epoch(self):\n        \"\"\"After full epoch (train + val).\"\"\"\n        epoch_time = time.time() - self.epoch_start\n        print(f\"Epoch took {epoch_time:.2f}s\")\n\n    def post_fit(self):\n        \"\"\"Training complete.\"\"\"\n        total_time = time.time() - self.start_time\n        print(f\"Total training time: {total_time:.2f}s\")\n</code></pre>"},{"location":"api/callbacks/base/#optimizer-hooks","title":"Optimizer Hooks","text":"<pre><code>class GradientCallback(Callback[BaseTrainer]):\n    def pre_optimizer_step(self, model: nn.Module, optimizer: Optimizer):\n        \"\"\"Before optimizer step.\"\"\"\n        # Could modify gradients here\n        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        print(f\"Gradient norm: {total_norm:.4f}\")\n\n    def post_optimizer_step(self, model: nn.Module, optimizer: Optimizer):\n        \"\"\"After optimizer step.\"\"\"\n        # Log learning rate\n        lr = optimizer.param_groups[0][\"lr\"]\n        print(f\"Learning rate: {lr}\")\n\n    def pre_optimizer_zero_grad(self, model: nn.Module, optimizer: Optimizer):\n        \"\"\"Before zeroing gradients.\"\"\"\n        pass\n\n    def post_optimizer_zero_grad(self, model: nn.Module, optimizer: Optimizer):\n        \"\"\"After zeroing gradients.\"\"\"\n        pass\n</code></pre>"},{"location":"api/callbacks/base/#context-managers","title":"Context Managers","text":"<p>Callbacks can provide context managers for training/validation:</p> <pre><code>import contextlib\nfrom typing import Any\n\nclass ProfilingCallback(Callback[BaseTrainer]):\n    def __init__(self):\n        self.profiler = torch.profiler.profile(\n            activities=[\n                torch.profiler.ProfilerActivity.CPU,\n                torch.profiler.ProfilerActivity.CUDA,\n            ],\n            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n            on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./logs\"),\n        )\n\n    @contextlib.contextmanager\n    def train_context(self):\n        \"\"\"Context for training steps.\"\"\"\n        with self.profiler:\n            yield\n\n    @contextlib.contextmanager\n    def validation_context(self):\n        \"\"\"Context for validation steps.\"\"\"\n        # No profiling during validation\n        yield\n</code></pre>"},{"location":"api/callbacks/base/#state-management","title":"State Management","text":"<p>Callbacks can save and restore state for resumable training:</p> <pre><code>class StatefulCallback(Callback[BaseTrainer]):\n    def __init__(self):\n        self.step_count = 0\n        self.best_metric = float(\"inf\")\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        self.step_count += 1\n\n    def post_validation_epoch(self, result: dict[str, Any]):\n        metric = result.get(\"val_loss\", float(\"inf\"))\n        self.best_metric = min(self.best_metric, metric)\n\n    def state_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Save callback state.\"\"\"\n        return {\n            \"step_count\": self.step_count,\n            \"best_metric\": self.best_metric,\n        }\n\n    def load_state_dict(self, state_dict: dict[str, Any], trainer: BaseTrainer):\n        \"\"\"Restore callback state.\"\"\"\n        self.step_count = state_dict[\"step_count\"]\n        self.best_metric = state_dict[\"best_metric\"]\n</code></pre>"},{"location":"api/callbacks/base/#using-callbacks","title":"Using Callbacks","text":""},{"location":"api/callbacks/base/#single-callback","title":"Single Callback","text":"<pre><code>from dream_trainer.trainer import DreamTrainer, DreamTrainerConfig\nfrom dream_trainer.callbacks import CallbackCollection\n\nconfig = DreamTrainerConfig(\n    # ... other config ...\n    callbacks=CallbackCollection([\n        SimpleLoggingCallback(\"training.log\")\n    ])\n)\n\ntrainer = MyTrainer(config)\ntrainer.configure()\ntrainer.setup()\ntrainer.fit()\n</code></pre>"},{"location":"api/callbacks/base/#multiple-callbacks","title":"Multiple Callbacks","text":"<pre><code>callbacks = CallbackCollection([\n    ProgressBar(),\n    ModelSummaryCallback(),\n    CheckpointCallback(checkpoint_config),\n    ProfileCallback(profiler),\n    CustomMetricCallback(),\n])\n\nconfig = DreamTrainerConfig(\n    # ... other config ...\n    callbacks=callbacks\n)\n</code></pre>"},{"location":"api/callbacks/base/#adding-callbacks-dynamically","title":"Adding Callbacks Dynamically","text":"<pre><code>trainer = MyTrainer(config)\n\n# Add callback after initialization\ntrainer.callbacks.append(NewCallback())\n\n# Or use dict-style access\ntrainer.callbacks[\"CustomCallback\"] = CustomCallback()\n\n# Remove callback\ndel trainer.callbacks[\"CustomCallback\"]\n</code></pre>"},{"location":"api/callbacks/base/#built-in-callbacks","title":"Built-in Callbacks","text":"<p>Dream Trainer includes several built-in callbacks:</p> <ul> <li>ProgressBar - Display training progress</li> <li>CheckpointCallback - Save and restore checkpoints</li> <li>ProfileCallback - Profile training performance</li> <li>LoggerCallback - Log metrics</li> <li>TrainerSummary - Display trainer configuration</li> <li>FindGraphBreaksCallback - Debug torch.compile</li> <li>OptimizeFSDP - Optimize FSDP performance</li> </ul>"},{"location":"api/callbacks/base/#best-practices","title":"Best Practices","text":""},{"location":"api/callbacks/base/#1-use-type-hints","title":"1. Use Type Hints","text":"<p>Always specify the trainer type for better IDE support:</p> <pre><code>class MyCallback(Callback[DreamTrainer]):\n    # Now self.trainer is typed as DreamTrainer\n    def post_setup(self):\n        models = self.trainer.named_models()  # Type-safe access\n</code></pre>"},{"location":"api/callbacks/base/#2-handle-distributed-training","title":"2. Handle Distributed Training","text":"<p>Be aware of distributed training when designing callbacks:</p> <pre><code>class DistributedAwareCallback(Callback[BaseTrainer]):\n    def post_train_epoch(self, result: dict[str, Any]):\n        # Aggregate metrics across all processes\n        loss = result[\"loss\"]\n        avg_loss = self.trainer.world.all_reduce(loss, op=\"mean\")\n\n        if self.trainer.world.is_global_zero:\n            print(f\"Average loss: {avg_loss}\")\n</code></pre>"},{"location":"api/callbacks/base/#3-avoid-side-effects","title":"3. Avoid Side Effects","text":"<p>Don't modify trainer state in unexpected ways:</p> <pre><code># Bad - modifies training state\nclass BadCallback(Callback[BaseTrainer]):\n    def pre_train_step(self, batch, batch_idx):\n        self.trainer.global_step += 1  # Don't do this!\n\n# Good - only observes state\nclass GoodCallback(Callback[BaseTrainer]):\n    def post_train_step(self, result, batch_idx):\n        print(f\"Current step: {self.trainer.global_step}\")\n</code></pre>"},{"location":"api/callbacks/base/#4-resource-management","title":"4. Resource Management","text":"<p>Clean up resources properly:</p> <pre><code>class ResourceCallback(Callback[BaseTrainer]):\n    def pre_fit(self):\n        self.file = open(\"log.txt\", \"w\")\n        self.connection = create_database_connection()\n\n    def post_fit(self):\n        # Always clean up, even if training fails\n        try:\n            self.file.close()\n        except:\n            pass\n\n        try:\n            self.connection.close()\n        except:\n            pass\n</code></pre>"},{"location":"api/callbacks/base/#see-also","title":"See Also","text":"<ul> <li>CheckpointCallback - Checkpointing implementation</li> <li>Monitoring Callbacks - Progress and logging callbacks</li> <li>Performance Callbacks - Profiling and optimization</li> <li>BaseTrainer - Trainer implementation </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/checkpoint/","title":"Checkpoint Callbacks","text":"<p>Dream Trainer provides comprehensive checkpointing functionality through the <code>CheckpointCallback</code> class and its variants. These callbacks handle saving and restoring training state, enabling resumable training and model preservation.</p>"},{"location":"api/callbacks/checkpoint/#checkpointcallback","title":"CheckpointCallback","text":"<p>The base checkpoint callback that handles synchronous checkpointing:</p>"},{"location":"api/callbacks/checkpoint/#dream_trainer.callbacks.checkpoint.base.CheckpointCallback","title":"CheckpointCallback","text":"<pre><code>CheckpointCallback(config: CheckpointParameters)\n</code></pre> <p>Base checkpoint callback that implements the entire checkpointing workflow except for storage-specific details. Subclasses must supply a storage-specific _setup_paths() implementation and may override the file-open keyword arguments.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/checkpoint/base.py</code> <pre><code>def __init__(self, config: CheckpointParameters):\n    self.config = config\n</code></pre>"},{"location":"api/callbacks/checkpoint/#dream_trainer.callbacks.checkpoint.base.CheckpointCallback-functions","title":"Functions","text":""},{"location":"api/callbacks/checkpoint/#dream_trainer.callbacks.checkpoint.base.CheckpointCallback.save","title":"save","text":"<pre><code>save()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/checkpoint/base.py</code> <pre><code>def save(self):\n    if self._current_metric is None:\n        if self._did_resume:\n            # Skip saving as we just loaded the checkpoint\n            self._did_resume = False\n            return\n        else:\n            raise ValueError(\n                f\"Monitoring {self.config.monitor} but it was not reported in the last epoch\"\n            )\n\n    current_metric = self.trainer.world.all_reduce(self._current_metric, op=\"mean\")\n    assert isinstance(current_metric, Tensor) and current_metric.numel() == 1, (\n        f\"Monitored checkpoint metric must be a scalar tensor, got {current_metric}\"\n    )\n    checkpoint = Checkpoint(step=self.trainer.global_step, metric=current_metric.item())\n\n    self._save(checkpoint)\n    logger.info(f\"Saved checkpoint to {checkpoint.checkpoint_id}\")\n    self._cleanup_checkpoints()\n</code></pre>"},{"location":"api/callbacks/checkpoint/#dream_trainer.callbacks.checkpoint.base.CheckpointCallback.load","title":"load","text":"<pre><code>load()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/checkpoint/base.py</code> <pre><code>def load(self):\n    current_checkpoint = find_current_checkpoint(\n        self._checkpoint_dir, self.config.resume_mode\n    )\n    if current_checkpoint is None:\n        logger.info(f\"Training {self.trainer.experiment} from scratch\")\n        return\n\n    logger.info(\n        f\"Loading checkpoint {current_checkpoint.checkpoint_id} for {self.trainer.experiment}\",\n    )\n\n    state_dict = self.trainer.state_dict()\n    dcp.state_dict_loader.load(\n        state_dict,\n        checkpoint_id=str(self._checkpoint_dir / current_checkpoint.checkpoint_id),\n        process_group=self.pg,\n    )\n\n    self._did_resume = True\n    self._current_metric = None\n\n    self.trainer.world.barrier()\n    logger.info(\n        f\"Resumed {self.trainer.experiment} from step {current_checkpoint.step}\",\n    )\n</code></pre>"},{"location":"api/callbacks/checkpoint/#configuration","title":"Configuration","text":""},{"location":"api/callbacks/checkpoint/#dream_trainer.configs.CheckpointParameters","title":"CheckpointParameters  <code>dataclass</code>","text":"<pre><code>CheckpointParameters(*, enable: bool, root_dir: str | Path, resume_mode: Literal['min', 'max', 'last'] = 'last', monitor: str = 'train/loss', checkpoint_every_n_epochs: int | None = None, checkpoint_every_n_steps: int | None = None, keep_top_k: int = 5, strict_load: bool = False, model_weights_only: bool, exclude_from_loading: list[str], pin_memory: bool = False)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#asynccheckpointcallback","title":"AsyncCheckpointCallback","text":"<p>For improved performance with asynchronous saving:</p>"},{"location":"api/callbacks/checkpoint/#dream_trainer.callbacks.checkpoint.async_checkpoint.AsyncCheckpointCallback","title":"AsyncCheckpointCallback","text":"<pre><code>AsyncCheckpointCallback(config: CheckpointParameters)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/checkpoint/base.py</code> <pre><code>def __init__(self, config: CheckpointParameters):\n    self.config = config\n</code></pre>"},{"location":"api/callbacks/checkpoint/#usage-examples","title":"Usage Examples","text":""},{"location":"api/callbacks/checkpoint/#basic-checkpointing","title":"Basic Checkpointing","text":"<pre><code>from dream_trainer.callbacks import CheckpointCallback\nfrom dream_trainer.configs import CheckpointParameters\n\ncheckpoint_config = CheckpointParameters(\n    root_dir=\"./checkpoints\",\n    monitor=\"val_loss\",  # Metric to monitor\n    mode=\"min\",  # Save when metric is minimized\n    keep_top_k=3,  # Keep best 3 checkpoints\n    save_last=True,  # Always save last checkpoint\n)\n\ncallback = CheckpointCallback(checkpoint_config)\n\n# Add to trainer\nconfig = DreamTrainerConfig(\n    # ... other config ...\n    callbacks=CallbackCollection([callback])\n)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>checkpoint_config = CheckpointParameters(\n    root_dir=\"/mnt/checkpoints\",\n    monitor=\"val_accuracy\",\n    mode=\"max\",  # Save when metric is maximized\n    keep_top_k=5,\n    save_last=True,\n    save_every_n_epochs=5,  # Save every 5 epochs\n    save_every_n_steps=1000,  # Save every 1000 steps\n    resume_mode=\"best\",  # Resume from best checkpoint\n)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#custom-checkpoint-logic","title":"Custom Checkpoint Logic","text":"<pre><code>class CustomCheckpointCallback(CheckpointCallback):\n    def should_save(self) -&gt; bool:\n        \"\"\"Custom logic for when to save.\"\"\"\n        # Save only if validation loss improved by 1%\n        if self._current_metric is None:\n            return False\n\n        if not hasattr(self, \"best_metric\"):\n            self.best_metric = float(\"inf\")\n\n        improvement = (self.best_metric - self._current_metric) / self.best_metric\n        if improvement &gt; 0.01:\n            self.best_metric = self._current_metric\n            return True\n\n        return False\n</code></pre>"},{"location":"api/callbacks/checkpoint/#checkpoint-structure","title":"Checkpoint Structure","text":"<p>Checkpoints contain the complete training state:</p> <pre><code>{\n    \"trainer\": {\n        \"global_step\": 10000,\n        \"current_epoch\": 5,\n        \"callbacks\": {...}  # Callback states\n    },\n    \"models\": {\n        \"model\": {...},  # Model state dict\n        \"auxiliary\": {...}  # Other models\n    },\n    \"optimizers\": {\n        \"optimizer\": {...}  # Optimizer state\n    },\n    \"schedulers\": {\n        \"scheduler\": {...}  # LR scheduler state\n    },\n    \"dataloaders\": {\n        \"train\": {...},  # Dataloader state if stateful\n        \"val\": {...}\n    }\n}\n</code></pre>"},{"location":"api/callbacks/checkpoint/#resuming-training","title":"Resuming Training","text":""},{"location":"api/callbacks/checkpoint/#automatic-resume","title":"Automatic Resume","text":"<pre><code>checkpoint_config = CheckpointParameters(\n    root_dir=\"./checkpoints\",\n    resume_mode=\"latest\",  # Resume from most recent\n)\n\n# Training will automatically resume if checkpoints exist\ntrainer = MyTrainer(config)\ntrainer.configure()\ntrainer.setup()\ntrainer.fit()  # Resumes from checkpoint\n</code></pre>"},{"location":"api/callbacks/checkpoint/#resume-modes","title":"Resume Modes","text":"<ul> <li><code>\"latest\"</code> - Resume from most recent checkpoint</li> <li><code>\"best\"</code> - Resume from best checkpoint (by metric)</li> <li><code>None</code> - Start fresh, ignore existing checkpoints</li> </ul>"},{"location":"api/callbacks/checkpoint/#manual-loading","title":"Manual Loading","text":"<pre><code># Load specific checkpoint\ncheckpoint_path = \"checkpoints/experiment/checkpoint_step_10000\"\nstate_dict = torch.load(checkpoint_path)\ntrainer.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#monitoring-metrics","title":"Monitoring Metrics","text":""},{"location":"api/callbacks/checkpoint/#built-in-metrics","title":"Built-in Metrics","text":"<p>Common metrics to monitor:</p> <pre><code># Minimize these\nmonitor=\"loss\"          # Training loss\nmonitor=\"val_loss\"      # Validation loss\nmonitor=\"val_perplexity\"  # Language model perplexity\n\n# Maximize these\nmonitor=\"accuracy\"      # Classification accuracy\nmonitor=\"val_f1\"        # F1 score\nmonitor=\"val_bleu\"      # Translation quality\n</code></pre>"},{"location":"api/callbacks/checkpoint/#custom-metrics","title":"Custom Metrics","text":"<p>Return custom metrics from training/validation steps:</p> <pre><code>def validation_step(self, batch, batch_idx):\n    # Compute custom metric\n    outputs = self.model(batch[\"input\"])\n    loss = self.criterion(outputs, batch[\"target\"])\n\n    # Custom metric calculation\n    custom_score = self.compute_custom_metric(outputs, batch[\"target\"])\n\n    return {\n        \"val_loss\": loss,\n        \"val_custom_score\": custom_score,  # Can monitor this\n    }\n</code></pre>"},{"location":"api/callbacks/checkpoint/#asynchronous-checkpointing","title":"Asynchronous Checkpointing","text":"<p>For large models, use asynchronous saving:</p> <pre><code>from dream_trainer.callbacks import AsyncCheckpointCallback\n\n# Same configuration, but saves happen in background\ncallback = AsyncCheckpointCallback(checkpoint_config)\n\n# Training continues while checkpoint saves\n</code></pre> <p>Benefits: - Non-blocking saves - Better training throughput - Automatic cleanup in background</p>"},{"location":"api/callbacks/checkpoint/#best-practices","title":"Best Practices","text":""},{"location":"api/callbacks/checkpoint/#1-choose-appropriate-metrics","title":"1. Choose Appropriate Metrics","text":"<pre><code># For generative models\ncheckpoint_config = CheckpointParameters(\n    monitor=\"val_perplexity\",\n    mode=\"min\"\n)\n\n# For classification\ncheckpoint_config = CheckpointParameters(\n    monitor=\"val_accuracy\", \n    mode=\"max\"\n)\n\n# For multi-task\ncheckpoint_config = CheckpointParameters(\n    monitor=\"val_combined_score\",  # Custom metric\n    mode=\"max\"\n)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#2-storage-management","title":"2. Storage Management","text":"<pre><code>checkpoint_config = CheckpointParameters(\n    root_dir=\"/fast_ssd/checkpoints\",  # Fast storage\n    keep_top_k=3,  # Limit storage usage\n    save_last=True,  # Keep recent for resume\n)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#3-checkpoint-frequency","title":"3. Checkpoint Frequency","text":"<pre><code># Balance between safety and performance\ncheckpoint_config = CheckpointParameters(\n    save_every_n_steps=500,  # Frequent for unstable training\n    save_every_n_epochs=1,   # Standard epoch checkpoints\n    keep_top_k=5,            # Keep enough for analysis\n)\n</code></pre>"},{"location":"api/callbacks/checkpoint/#4-distributed-training","title":"4. Distributed Training","text":"<p>Checkpointing works seamlessly with distributed training:</p> <pre><code># Only rank 0 manages checkpoint metadata\n# All ranks participate in saving model shards\n# Automatic synchronization across ranks\n</code></pre>"},{"location":"api/callbacks/checkpoint/#checkpoint-utilities","title":"Checkpoint Utilities","text":""},{"location":"api/callbacks/checkpoint/#finding-checkpoints","title":"Finding Checkpoints","text":"<pre><code>from dream_trainer.callbacks.checkpoint.utils import find_checkpoints\n\n# List all checkpoints\ncheckpoints = find_checkpoints(\n    checkpoint_dir=\"./checkpoints/experiment\",\n    mode=\"best\"  # or \"latest\"\n)\n\nfor ckpt in checkpoints:\n    print(f\"Step: {ckpt.step}, Metric: {ckpt.metric}\")\n</code></pre>"},{"location":"api/callbacks/checkpoint/#checkpoint-inspection","title":"Checkpoint Inspection","text":"<pre><code># Load and inspect checkpoint\nstate_dict = torch.load(\"checkpoint_step_10000\", map_location=\"cpu\")\n\nprint(f\"Global step: {state_dict['trainer']['global_step']}\")\nprint(f\"Epoch: {state_dict['trainer']['current_epoch']}\")\nprint(f\"Model keys: {state_dict['models'].keys()}\")\n</code></pre>"},{"location":"api/callbacks/checkpoint/#integration-with-other-callbacks","title":"Integration with Other Callbacks","text":"<p>Checkpointing integrates with other callbacks:</p> <pre><code>class MetricCheckpointCallback(Callback[BaseTrainer]):\n    \"\"\"Save checkpoint when multiple metrics improve.\"\"\"\n\n    def __init__(self, checkpoint_callback: CheckpointCallback):\n        self.checkpoint_callback = checkpoint_callback\n        self.best_accuracy = 0\n        self.best_f1 = 0\n\n    def post_validation_epoch(self, result: dict[str, Any]):\n        accuracy = result.get(\"val_accuracy\", 0)\n        f1 = result.get(\"val_f1\", 0)\n\n        # Save if both metrics improve\n        if accuracy &gt; self.best_accuracy and f1 &gt; self.best_f1:\n            self.best_accuracy = accuracy\n            self.best_f1 = f1\n\n            # Trigger checkpoint save\n            self.checkpoint_callback._current_metric = accuracy\n            self.checkpoint_callback.save()\n</code></pre>"},{"location":"api/callbacks/checkpoint/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/callbacks/checkpoint/#common-issues","title":"Common Issues","text":"<ol> <li>Checkpoint not found: Check paths and experiment names</li> <li>Resume fails: Ensure model architecture matches</li> <li>Disk space: Monitor available storage</li> <li>Slow saves: Use async checkpointing for large models</li> </ol>"},{"location":"api/callbacks/checkpoint/#debugging","title":"Debugging","text":"<pre><code># Enable verbose logging\nimport logging\nlogging.getLogger(\"dream_trainer.callbacks.checkpoint\").setLevel(logging.DEBUG)\n\n# Check checkpoint contents\nckpt = torch.load(\"checkpoint\", map_location=\"cpu\")\nprint(f\"Checkpoint keys: {ckpt.keys()}\")\nprint(f\"Model state dict size: {len(ckpt['models']['model'])}\")\n</code></pre>"},{"location":"api/callbacks/checkpoint/#see-also","title":"See Also","text":"<ul> <li>Callback System - Base callback documentation</li> <li>Configuration - CheckpointParameters details</li> <li>BaseTrainer - State dict implementation </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/monitoring/","title":"Monitoring Callbacks","text":"<p>Dream Trainer provides several callbacks for monitoring training progress, logging metrics, and displaying training summaries.</p>"},{"location":"api/callbacks/monitoring/#progressbar","title":"ProgressBar","text":"<p>Displays a real-time progress bar for training and validation:</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.ProgressBar","title":"ProgressBar","text":"<pre><code>ProgressBar(smoothing: float = 0.1, metric: str | None = None)\n</code></pre> <p>A callback that displays a progress bar for the training and validation loops.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>smoothing</code> <p>The smoothing factor for the progress bar.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>metric</code> <p>The metric to display in the progress bar. If None, no metric is displayed.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dream_trainer/callbacks/progress_bar.py</code> <pre><code>def __init__(self, smoothing: float = 0.1, metric: str | None = None):\n    super().__init__()\n    self.smoothing = smoothing\n    self.metric = metric\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.ProgressBar-functions","title":"Functions","text":""},{"location":"api/callbacks/monitoring/#usage","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import ProgressBar\n\n# Basic progress bar\nprogress = ProgressBar()\n\n# With custom smoothing and metric display\nprogress = ProgressBar(\n    smoothing=0.1,  # Smoothing factor for speed estimates\n    metric=\"loss\"   # Display loss in progress bar\n)\n\n# Add to trainer\ncallbacks = CallbackCollection([progress])\n</code></pre>"},{"location":"api/callbacks/monitoring/#output-example","title":"Output Example","text":"<pre><code>Epoch:     5/100 [05:23&lt;1:32:45, 58.58s/it]\nTraining:  450/1000 [45%] [02:15&lt;02:45, 3.00it/s, loss=0.453]\nValidation: 50/100 [50%] [00:30&lt;00:30, 1.67it/s]\n</code></pre>"},{"location":"api/callbacks/monitoring/#loggercallback","title":"LoggerCallback","text":"<p>Base callback for logging training metrics:</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback","title":"LoggerCallback","text":"<pre><code>LoggerCallback(log_every_n_train_batches: int | None = 8, log_every_n_val_batches: int | None = None, code_dir: str = './')\n</code></pre> <p>A base callback that logs all returned metrics from training and validation steps and epochs.</p> <p>This callback only logs scalar metrics.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Initialize the logger callback.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>log_every_n_train_batches</code> <p>If set, only log training metrics every N batches. If None, log every batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int | None</code> DEFAULT: <code>8</code> </p> <code>log_every_n_val_batches</code> <p>If set, only log validation metrics every N batches. If None, log every batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>code_dir</code> <p>Directory storing the code to be saved.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> DEFAULT: <code>'./'</code> </p> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>def __init__(\n    self,\n    log_every_n_train_batches: int | None = 8,\n    log_every_n_val_batches: int | None = None,\n    code_dir: str = \"./\",\n):\n    \"\"\"Initialize the logger callback.\n\n    Args:\n        log_every_n_train_batches: If set, only log training metrics every N batches. If None, log every batch.\n        log_every_n_val_batches: If set, only log validation metrics every N batches. If None, log every batch.\n        code_dir: Directory storing the code to be saved.\n    \"\"\"\n    self.log_every_n_train_batches = log_every_n_train_batches\n    self.log_every_n_val_batches = log_every_n_val_batches\n    self.code_dir = code_dir\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback-functions","title":"Functions","text":""},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.pre_configure","title":"pre_configure","text":"<pre><code>pre_configure()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef pre_configure(self):\n    self.trainer.log_config(self.trainer.config)\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.pre_fit","title":"pre_fit","text":"<pre><code>pre_fit()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef pre_fit(self):\n    self.trainer.log_code(\n        self.code_dir, gitignore_path=os.path.join(self.code_dir, \".gitignore\")\n    )\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.post_train_step","title":"post_train_step","text":"<pre><code>post_train_step(result: dict[str, Tensor | int | float], batch_idx: int)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef post_train_step(self, result: dict[str, torch.Tensor | int | float], batch_idx: int):\n    if (\n        self.log_every_n_train_batches is None\n        or batch_idx % self.log_every_n_train_batches == 0\n    ):\n        self.trainer.log_dict(filter_logs(result))\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.post_train_epoch","title":"post_train_epoch","text":"<pre><code>post_train_epoch(result: dict[str, Tensor | int | float])\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef post_train_epoch(self, result: dict[str, torch.Tensor | int | float]):\n    self.trainer.log_dict(filter_logs(result))\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.post_validation_step","title":"post_validation_step","text":"<pre><code>post_validation_step(result: dict[str, Any], batch_idx: int)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef post_validation_step(self, result: dict[str, Any], batch_idx: int):\n    if (\n        self.log_every_n_val_batches is None\n        or batch_idx % self.log_every_n_val_batches == 0\n    ):\n        self.trainer.log_dict(filter_logs(result))\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.LoggerCallback.post_validation_epoch","title":"post_validation_epoch","text":"<pre><code>post_validation_epoch(result: dict[str, Any])\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/base.py</code> <pre><code>@override\ndef post_validation_epoch(self, result: dict[str, Any]):\n    self.trainer.log_dict(filter_logs(result))\n</code></pre>"},{"location":"api/callbacks/monitoring/#usage_1","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import LoggerCallback\n\n# Log every batch\nlogger = LoggerCallback()\n\n# Log every N batches\nlogger = LoggerCallback(\n    log_every_n_train_batches=10,  # Log training every 10 batches\n    log_every_n_val_batches=5,      # Log validation every 5 batches\n    code_dir=\"./\"                   # Directory to save code\n)\n</code></pre>"},{"location":"api/callbacks/monitoring/#medialoggercallback","title":"MediaLoggerCallback","text":"<p>Logs images, videos, and other media during training:</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.MediaLoggerCallback","title":"MediaLoggerCallback","text":"<pre><code>MediaLoggerCallback(num_samples: int = 32, spacing: int = 2, description: str = 'samples', image_key: str = 'samples', caption_key: str | None = None)\n</code></pre> <p>A callback that logs image or video samples and captions to the trainer.</p> <p>If a single batch is given, log_images or log_videos is called on the batch. If a list of batches is given, we concatenate samples along the width (trailing) dimension to convert it into a single batch.</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.MediaLoggerCallback--todo-support-logging-more-types-of-media-audio","title":"TODO: Support logging more types of media (audio, ...)","text":"<p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/loggers/media.py</code> <pre><code>def __init__(\n    self,\n    num_samples: int = 32,\n    spacing: int = 2,\n    description: str = \"samples\",\n    image_key: str = \"samples\",\n    caption_key: str | None = None,\n):\n    self.num_samples = num_samples\n    self.spacing = spacing\n    self.description = description\n    self.image_key = image_key\n    self.caption_key = caption_key\n</code></pre>"},{"location":"api/callbacks/monitoring/#usage_2","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import MediaLoggerCallback\n\nclass CustomMediaLogger(MediaLoggerCallback):\n    def __init__(self, log_every_n_epochs: int = 5):\n        self.log_every_n_epochs = log_every_n_epochs\n\n    def post_validation_epoch(self, result: dict[str, Any]):\n        if self.trainer.current_epoch % self.log_every_n_epochs == 0:\n            # Generate and log sample images\n            with torch.no_grad():\n                samples = self.trainer.model.generate(num_samples=8)\n                self.trainer.log_images(\n                    samples,\n                    desc=\"generated_samples\"\n                )\n\n            # Log attention maps if available\n            if hasattr(self.trainer.model, \"get_attention_maps\"):\n                attention = self.trainer.model.get_attention_maps()\n                self.trainer.log_images(\n                    attention,\n                    desc=\"attention_maps\"\n                )\n</code></pre>"},{"location":"api/callbacks/monitoring/#metricloggercallback","title":"MetricLoggerCallback","text":"<p>Logs torchmetrics values (requires EvalMetricMixin):</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.MetricLoggerCallback","title":"MetricLoggerCallback","text":""},{"location":"api/callbacks/monitoring/#usage_3","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import MetricLoggerCallback\n\n# Automatically log all metrics\nmetric_logger = MetricLoggerCallback()\n\n# Custom metric logging\nclass CustomMetricLogger(MetricLoggerCallback):\n    def post_validation_epoch(self, result: dict[str, Any]):\n        # Log individual metrics\n        for name, metric in self.trainer.named_metrics().items():\n            value = metric.compute()\n            self.trainer.log_scalar(f\"val/metrics/{name}\", value)\n\n        # Log metric visualizations\n        if hasattr(self.trainer, \"confusion_matrix\"):\n            cm = self.trainer.confusion_matrix.compute()\n            fig = self.plot_confusion_matrix(cm)\n            self.trainer.log_plot(fig, desc=\"val/confusion_matrix\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#trainersummary","title":"TrainerSummary","text":"<p>Displays a comprehensive summary of the trainer configuration:</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.TrainerSummary","title":"TrainerSummary","text":""},{"location":"api/callbacks/monitoring/#usage_4","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import TrainerSummary\n\n# Add to callbacks\nsummary = TrainerSummary()\ncallbacks = CallbackCollection([summary])\n</code></pre>"},{"location":"api/callbacks/monitoring/#output-example_1","title":"Output Example","text":"<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Trainer Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                         \u2502\n\u2502 Experiment: llama3-7b-finetune                                          \u2502\n\u2502 Group: language-modeling                                                \u2502\n\u2502 Project: research                                                       \u2502\n\u2502                                                                         \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Model Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e    \u2502\n\u2502 \u2502 Model: LlamaModel                                                \u2502    \u2502\n\u2502 \u2502 Parameters: 6,738,415,616                                        \u2502    \u2502\n\u2502 \u2502 Trainable: 6,738,415,616                                         \u2502    \u2502\n\u2502 \u2502 Non-trainable: 0                                                 \u2502    \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502                                                                         \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Training Config \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e    \u2502\n\u2502 \u2502 Epochs: 3                                                        \u2502    \u2502\n\u2502 \u2502 Batch Size: 8                                                    \u2502    \u2502\n\u2502 \u2502 Gradient Accumulation: 16                                        \u2502    \u2502\n\u2502 \u2502 Learning Rate: 2e-5                                              \u2502    \u2502\n\u2502 \u2502 Warmup Steps: 100                                                \u2502    \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502                                                                         \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Distributed Config \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e    \u2502\n\u2502 \u2502 Strategy: FSDP                                                   \u2502    \u2502\n\u2502 \u2502 World Size: 8                                                    \u2502    \u2502\n\u2502 \u2502 Device: cuda                                                     \u2502    \u2502\n\u2502 \u2502 Mixed Precision: bf16                                            \u2502    \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"api/callbacks/monitoring/#modelwatchcallback","title":"ModelWatchCallback","text":"<p>Tracks model parameters and gradients with WandB:</p>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.ModelWatchCallback","title":"ModelWatchCallback","text":"<pre><code>ModelWatchCallback(log: Literal['gradients', 'parameters', 'all'] | None = None, log_freq: int = 1000)\n</code></pre> <p>A base callback that logs all returned metrics from training and validation steps and epochs.</p> <p>This callback only logs scalar metrics.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/loggers/wandb_watch.py</code> <pre><code>def __init__(\n    self,\n    log: Literal[\"gradients\", \"parameters\", \"all\"] | None = None,\n    log_freq: int = 1000,\n):\n    self.log: Literal[\"gradients\", \"parameters\", \"all\"] | None = log\n    self.log_freq = log_freq\n</code></pre>"},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.ModelWatchCallback-functions","title":"Functions","text":""},{"location":"api/callbacks/monitoring/#dream_trainer.callbacks.ModelWatchCallback.post_setup","title":"post_setup","text":"<pre><code>post_setup()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/loggers/wandb_watch.py</code> <pre><code>@override\ndef post_setup(self):\n    models = [\n        model\n        for model in self.trainer.named_models().values()\n        if any(p.requires_grad for p in model.parameters())\n    ]\n    self.trainer.log_model(models, log=self.log, log_freq=self.log_freq)\n</code></pre>"},{"location":"api/callbacks/monitoring/#usage_5","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import ModelWatchCallback\n\n# Watch all gradients and parameters\nwatch = ModelWatchCallback(\n    log=\"all\",      # Log gradients and parameters\n    log_freq=100    # Log every 100 batches\n)\n\n# Watch only gradients\nwatch = ModelWatchCallback(\n    log=\"gradients\",\n    log_freq=50\n)\n\n# Watch only parameters\nwatch = ModelWatchCallback(\n    log=\"parameters\",\n    log_freq=200\n)\n</code></pre>"},{"location":"api/callbacks/monitoring/#creating-custom-monitoring-callbacks","title":"Creating Custom Monitoring Callbacks","text":""},{"location":"api/callbacks/monitoring/#training-metrics-monitor","title":"Training Metrics Monitor","text":"<pre><code>class TrainingMonitor(Callback[BaseTrainer]):\n    \"\"\"Monitor and log detailed training metrics.\"\"\"\n\n    def __init__(self):\n        self.losses = []\n        self.learning_rates = []\n        self.gradient_norms = []\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        # Collect metrics\n        self.losses.append(result[\"loss\"].item())\n        self.learning_rates.append(\n            self.trainer.optimizer.param_groups[0][\"lr\"]\n        )\n\n        # Log every 100 steps\n        if batch_idx % 100 == 0:\n            self.log_summary()\n\n    def log_summary(self):\n        # Compute statistics\n        recent_losses = self.losses[-100:]\n        avg_loss = sum(recent_losses) / len(recent_losses)\n        loss_std = torch.tensor(recent_losses).std().item()\n\n        print(f\"Average loss (last 100): {avg_loss:.4f} \u00b1 {loss_std:.4f}\")\n        print(f\"Current LR: {self.learning_rates[-1]:.2e}\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#memory-monitor","title":"Memory Monitor","text":"<pre><code>class MemoryMonitor(Callback[BaseTrainer]):\n    \"\"\"Monitor GPU memory usage.\"\"\"\n\n    def __init__(self, log_every: int = 100):\n        self.log_every = log_every\n        self.peak_memory = 0\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        if batch_idx % self.log_every == 0:\n            if torch.cuda.is_available():\n                current = torch.cuda.memory_allocated() / 1024**3\n                self.peak_memory = max(self.peak_memory, current)\n\n                print(f\"GPU Memory: {current:.2f} GB (peak: {self.peak_memory:.2f} GB)\")\n\n                # Log to trainer if available\n                if hasattr(self.trainer, \"log_scalar\"):\n                    self.trainer.log_scalar(\"system/gpu_memory_gb\", current)\n</code></pre>"},{"location":"api/callbacks/monitoring/#validation-improvement-monitor","title":"Validation Improvement Monitor","text":"<pre><code>class ImprovementMonitor(RankZeroCallback[BaseTrainer]):\n    \"\"\"Track validation improvements.\"\"\"\n\n    def __init__(self, metric: str = \"val_loss\", mode: str = \"min\"):\n        self.metric = metric\n        self.mode = mode\n        self.best_value = float(\"inf\") if mode == \"min\" else float(\"-inf\")\n        self.epochs_without_improvement = 0\n\n    def post_validation_epoch(self, result: dict[str, Any]):\n        current_value = result.get(self.metric)\n        if current_value is None:\n            return\n\n        # Check for improvement\n        improved = False\n        if self.mode == \"min\" and current_value &lt; self.best_value:\n            improved = True\n            self.best_value = current_value\n        elif self.mode == \"max\" and current_value &gt; self.best_value:\n            improved = True\n            self.best_value = current_value\n\n        if improved:\n            print(f\"\u2713 New best {self.metric}: {current_value:.4f}\")\n            self.epochs_without_improvement = 0\n        else:\n            self.epochs_without_improvement += 1\n            print(f\"\u2717 No improvement for {self.epochs_without_improvement} epochs\")\n            print(f\"  Best {self.metric}: {self.best_value:.4f}\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#integration-examples","title":"Integration Examples","text":""},{"location":"api/callbacks/monitoring/#complete-monitoring-setup","title":"Complete Monitoring Setup","text":"<pre><code>from dream_trainer.callbacks import (\n    CallbackCollection,\n    ProgressBar,\n    LoggerCallback,\n    ModelWatchCallback,\n    TrainerSummary\n)\n\n# Create comprehensive monitoring\ncallbacks = CallbackCollection([\n    # Display progress\n    ProgressBar(metric=\"loss\"),\n\n    # Log metrics\n    LoggerCallback(\n        log_every_n_train_batches=10,\n        log_every_n_val_batches=1\n    ),\n\n    # Track model\n    ModelWatchCallback(log=\"gradients\", log_freq=100),\n\n    # Show summary\n    TrainerSummary(),\n\n    # Custom monitors\n    MemoryMonitor(log_every=50),\n    ImprovementMonitor(metric=\"val_accuracy\", mode=\"max\"),\n])\n\nconfig = DreamTrainerConfig(\n    # ... other config ...\n    callbacks=callbacks\n)\n</code></pre>"},{"location":"api/callbacks/monitoring/#conditional-monitoring","title":"Conditional Monitoring","text":"<pre><code>class ConditionalMonitor(Callback[BaseTrainer]):\n    \"\"\"Enable detailed monitoring only when needed.\"\"\"\n\n    def __init__(self):\n        self.detailed_logging = False\n        self.loss_threshold = 1.0\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        loss = result[\"loss\"]\n\n        # Enable detailed logging if loss spikes\n        if loss &gt; self.loss_threshold:\n            self.detailed_logging = True\n            print(f\"\u26a0\ufe0f Loss spike detected: {loss:.4f}\")\n\n            # Log additional information\n            self.log_gradients()\n            self.log_activations()\n            self.log_weights()\n\n    def log_gradients(self):\n        for name, param in self.trainer.model.named_parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.norm().item()\n                if grad_norm &gt; 10:\n                    print(f\"Large gradient in {name}: {grad_norm:.2f}\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"api/callbacks/monitoring/#1-efficient-logging","title":"1. Efficient Logging","text":"<pre><code># Log frequently during debugging\ndebug_logger = LoggerCallback(\n    log_every_n_train_batches=1,\n    log_every_n_val_batches=1\n)\n\n# Log less frequently in production\nprod_logger = LoggerCallback(\n    log_every_n_train_batches=100,\n    log_every_n_val_batches=10\n)\n</code></pre>"},{"location":"api/callbacks/monitoring/#2-memory-aware-monitoring","title":"2. Memory-Aware Monitoring","text":"<pre><code>class MemoryAwareLogger(MediaLoggerCallback):\n    def post_validation_epoch(self, result: dict[str, Any]):\n        # Only log media on specific epochs to save memory\n        if self.trainer.current_epoch % 10 == 0:\n            # Log limited samples\n            samples = self.generate_samples(num=4)  # Not 100\n            self.trainer.log_images(samples, desc=\"samples\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#3-distributed-aware-monitoring","title":"3. Distributed-Aware Monitoring","text":"<pre><code>class DistributedMonitor(RankZeroCallback[BaseTrainer]):\n    \"\"\"Only monitor on rank 0 to avoid duplicates.\"\"\"\n\n    def post_train_epoch(self, result: dict[str, Any]):\n        # This only runs on rank 0\n        print(f\"Epoch {self.trainer.current_epoch} summary:\")\n        print(f\"- Average loss: {result.get('loss', 'N/A')}\")\n        print(f\"- Training time: {result.get('epoch_time', 'N/A')}s\")\n</code></pre>"},{"location":"api/callbacks/monitoring/#see-also","title":"See Also","text":"<ul> <li>Callback System - Base callback documentation</li> <li>Logger Mixins - Logging functionality</li> <li>EvalMetricMixin - Metrics integration</li> <li>WandB Integration - WandB logging </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/callbacks/performance/","title":"Performance Callbacks","text":"<p>Dream Trainer provides callbacks for profiling, optimizing, and debugging training performance.</p>"},{"location":"api/callbacks/performance/#profilecallback","title":"ProfileCallback","text":"<p>Profile training performance using PyTorch's profiler:</p>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.ProfileCallback","title":"ProfileCallback","text":"<pre><code>ProfileCallback(profiler: profile)\n</code></pre> <p>Profiles the trainer's training step</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/profile.py</code> <pre><code>def __init__(self, profiler: torch.profiler.profile):\n    self.profiler = profiler\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.ProfileCallback-functions","title":"Functions","text":""},{"location":"api/callbacks/performance/#dream_trainer.callbacks.ProfileCallback.pre_fit","title":"pre_fit","text":"<pre><code>pre_fit()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/profile.py</code> <pre><code>@override\ndef pre_fit(self):\n    original_training_step = self.trainer.training_step\n\n    def profiled_training_step(*args, **kwargs):\n        out = original_training_step(*args, **kwargs)\n        self.profiler.step()\n        return out\n\n    self.trainer.training_step = profiled_training_step\n</code></pre>"},{"location":"api/callbacks/performance/#usage","title":"Usage","text":"<pre><code>import torch.profiler\nfrom dream_trainer.callbacks import ProfileCallback\n\n# Create profiler\nprofiler = torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=3,\n        repeat=2\n    ),\n    on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n)\n\n# Add to callbacks\ncallback = ProfileCallback(profiler)\n</code></pre>"},{"location":"api/callbacks/performance/#analyzing-results","title":"Analyzing Results","text":"<pre><code># View in TensorBoard\n# tensorboard --logdir=./logs\n\n# Or analyze programmatically\nprofiler.export_chrome_trace(\"trace.json\")\nprint(profiler.key_averages().table(sort_by=\"cuda_time_total\"))\n</code></pre>"},{"location":"api/callbacks/performance/#findgraphbreakscallback","title":"FindGraphBreaksCallback","text":"<p>Debug torch.compile graph breaks:</p>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.FindGraphBreaksCallback","title":"FindGraphBreaksCallback","text":"<pre><code>FindGraphBreaksCallback(log_file: str = 'graph_breaks.log', skip: int = 0, fullgraph: bool = False)\n</code></pre> <p>Find graph breaks in the trainer. This will check every compiled function in the trainer and write all the graph breaks to a file.</p> <p>NOTE: We only check for graph breaks in training steps.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>log_file</code> <p>File to write graph breaks to.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> DEFAULT: <code>'graph_breaks.log'</code> </p> <code>skip</code> <p>Number of steps to skip before finding graph breaks.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>src/dream_trainer/callbacks/graph_breaks.py</code> <pre><code>def __init__(\n    self, log_file: str = \"graph_breaks.log\", skip: int = 0, fullgraph: bool = False\n):\n    self.log_file = log_file\n    self.skip = skip\n    self.full_graph = fullgraph\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.FindGraphBreaksCallback-functions","title":"Functions","text":""},{"location":"api/callbacks/performance/#dream_trainer.callbacks.FindGraphBreaksCallback.pre_launch","title":"pre_launch","text":"<pre><code>pre_launch()\n</code></pre> Source code in <code>src/dream_trainer/callbacks/graph_breaks.py</code> <pre><code>@override\ndef pre_launch(self):\n    self.trainer.device_parameters.compile_model = False\n    self.trainer.training_parameters.num_sanity_val_steps = 0\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.FindGraphBreaksCallback.pre_train_step","title":"pre_train_step","text":"<pre><code>pre_train_step(batch: dict[str, Any], batch_idx: int)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/graph_breaks.py</code> <pre><code>@override\ndef pre_train_step(self, batch: dict[str, Any], batch_idx: int):\n    if batch_idx &lt; self.skip:\n        return\n\n    please_find_graph_breaks(\n        trainer=self.trainer,\n        batch=batch,\n        batch_idx=batch_idx,\n        path=self.log_file,\n        fullgraph=self.full_graph,\n    )\n    exit()\n</code></pre>"},{"location":"api/callbacks/performance/#usage_1","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import FindGraphBreaksCallback\n\n# Find all graph breaks\ngraph_breaks = FindGraphBreaksCallback(\n    log_file=\"graph_breaks.log\",\n    skip=0,  # Skip no steps\n    fullgraph=False  # Test without fullgraph\n)\n\n# Test if model can compile with fullgraph=True\nfullgraph_test = FindGraphBreaksCallback(\n    log_file=\"fullgraph_test.log\",\n    skip=0,\n    fullgraph=True\n)\n</code></pre>"},{"location":"api/callbacks/performance/#example-output","title":"Example Output","text":"<pre><code>================================================================================\nmodel.encoder.attention\n/path/to/model.py:45 - Graph break due to:\n  - Dynamic control flow (if statement depending on tensor value)\n  - Suggested fix: Use torch.where or masked operations\n\n================================================================================\nmodel.decoder.generate\n/path/to/model.py:123 - Graph break due to:\n  - Python builtin not supported in graph\n  - Suggested fix: Use torch operations instead of Python list comprehension\n</code></pre>"},{"location":"api/callbacks/performance/#optimizefsdp","title":"OptimizeFSDP","text":"<p>Optimize FSDP performance with advanced prefetching:</p>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.OptimizeFSDP","title":"OptimizeFSDP","text":"<pre><code>OptimizeFSDP(prefetch: int = 1, display: bool = False)\n</code></pre> <p>FSDP optimization callback that improves training performance through intelligent prefetching.</p> <p>This callback optimizes Fully Sharded Data Parallel (FSDP) training by: 1. Tracing the execution order of FSDP modules during the first training step 2. Setting up prefetching for both forward and backward passes based on the traced order 3. Unsharding models asynchronously before each training step</p> <p>The prefetching mechanism overlaps data movement with computation, reducing idle time and improving overall training throughput.</p> <p>Prefetch Behavior: - prefetch=1: Uses singleton lists, providing the same all-gather overlap as   default behavior but issues prefetched all-gathers earlier from the CPU - prefetch&gt;=2: Enables more aggressive overlap with higher memory usage due   to additional reserved memory for prefetched modules</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>prefetch</code> <p>Number of modules to prefetch ahead. Higher values increase memory usage but may improve performance. Must be &gt;= 1. Defaults to 1.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>display</code> <p>Whether to display the tree of FSDP modules after construction. Defaults to True.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> ATTRIBUTE DESCRIPTION <code>prefetch</code> <p>The number of modules to prefetch ahead.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <code>stack</code> <p>List of (module_name, requires_grad) tuples tracking execution order.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[tuple[str, bool, Literal['pre_forward_call', 'post_forward_call', 'prefetch']]]</code> </p> <code>hooks</code> <p>List of registered forward hooks for tracing module execution.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[RemovableHandle]</code> </p> <p>Initialize the FSDP optimization callback.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>prefetch</code> <p>Number of modules to prefetch ahead. Must be &gt;= 1. Values &gt;= 2 enable more aggressive overlap but use more memory.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If prefetch is less than 1.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/optimize_fsdp.py</code> <pre><code>def __init__(self, prefetch: int = 1, display: bool = False):\n    \"\"\"Initialize the FSDP optimization callback.\n\n    Args:\n        prefetch: Number of modules to prefetch ahead. Must be &gt;= 1.\n            Values &gt;= 2 enable more aggressive overlap but use more memory.\n\n    Raises:\n        ValueError: If prefetch is less than 1.\n    \"\"\"\n    if prefetch &lt; 1:\n        raise ValueError(f\"prefetch must be &gt;= 1, got {prefetch}\")\n\n    self.prefetch = prefetch\n    self.display = display\n\n    self.stack: list[\n        tuple[str, bool, Literal[\"pre_forward_call\", \"post_forward_call\", \"prefetch\"]]\n    ] = []\n    self.hooks: list[RemovableHandle] = []\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.OptimizeFSDP-functions","title":"Functions","text":""},{"location":"api/callbacks/performance/#dream_trainer.callbacks.OptimizeFSDP.pre_train_step","title":"pre_train_step","text":"<pre><code>pre_train_step(*_)\n</code></pre> <p>Unshard FSDP models asynchronously before training step.</p> <p>This method is called before each training step. It triggers asynchronous unsharding of the first all-gather of all FSDP model, allowing the unsharding operation to overlap with other computations and reducing the time spent waiting for data movement.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>*_</code> <p>Unused arguments from the trainer callback interface.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> DEFAULT: <code>()</code> </p> Source code in <code>src/dream_trainer/callbacks/optimize_fsdp.py</code> <pre><code>@override\ndef pre_train_step(self, *_):\n    \"\"\"Unshard FSDP models asynchronously before training step.\n\n    This method is called before each training step. It triggers asynchronous unsharding of\n    the first all-gather of all FSDP model, allowing the unsharding operation to overlap\n    with other computations and reducing the time spent waiting for data movement.\n\n    Args:\n        *_: Unused arguments from the trainer callback interface.\n    \"\"\"\n    for _, model in self.trainer.named_models().items():\n        if isinstance(model, FSDPModule):\n            model.unshard(async_op=True)\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.OptimizeFSDP.post_train_step","title":"post_train_step","text":"<pre><code>post_train_step(_, batch_idx: int)\n</code></pre> <p>Set up prefetching based on traced execution order and clean up hooks.</p> <p>This method is called after the first training step. It uses the execution order recorded by the hooks to set up optimal prefetching for both forward and backward passes. After setting up prefetching, it removes all hooks and clears the execution stack since tracing is only needed once.</p> <p>The prefetching setup works by: 1. Using the forward execution order for forward prefetching 2. Using the reverse order (filtering only modules with gradients) for backward prefetching 3. Setting each module to prefetch the next <code>prefetch</code> modules in sequence</p> <p>Prefetch list behavior: - Single module lists (prefetch=1): Same overlap as default, earlier CPU scheduling - Multi-module lists (prefetch&gt;=2): More aggressive overlap, higher memory usage</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>*_</code> <p>Unused arguments from the trainer callback interface.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> Source code in <code>src/dream_trainer/callbacks/optimize_fsdp.py</code> <pre><code>@override\ndef post_train_step(self, _, batch_idx: int):\n    \"\"\"Set up prefetching based on traced execution order and clean up hooks.\n\n    This method is called after the first training step. It uses the execution\n    order recorded by the hooks to set up optimal prefetching for both forward\n    and backward passes. After setting up prefetching, it removes all hooks\n    and clears the execution stack since tracing is only needed once.\n\n    The prefetching setup works by:\n    1. Using the forward execution order for forward prefetching\n    2. Using the reverse order (filtering only modules with gradients) for backward prefetching\n    3. Setting each module to prefetch the next `prefetch` modules in sequence\n\n    Prefetch list behavior:\n    - Single module lists (prefetch=1): Same overlap as default, earlier CPU scheduling\n    - Multi-module lists (prefetch&gt;=2): More aggressive overlap, higher memory usage\n\n    Args:\n        *_: Unused arguments from the trainer callback interface.\n    \"\"\"\n\n    # Add forward prefetching after first training step\n    if self.trainer.local_batches == 0:\n        prefetch_mode = (\n            \"conservative (singleton lists)\"\n            if self.prefetch == 1\n            else \"aggressive (multi-module lists)\"\n        )\n        logger.info(\n            f\"Setting up {prefetch_mode} prefetch for {len(self.stack)} forward and backward calls \"\n            f\"with prefetch factor {self.prefetch}\"\n        )\n\n        # Get the modules in order of execution\n        ordered_forward_modules = cast(\n            list[FSDPModule],\n            [\n                self.trainer.get_module(fqn)\n                for fqn, _, origin in self.stack\n                if origin == \"pre_forward_call\"\n            ],\n        )\n        ordered_backwards_modules = cast(\n            list[FSDPModule],\n            [\n                self.trainer.get_module(fqn)\n                for fqn, requires_grad, origin in self.stack[::-1]\n                if requires_grad and origin == \"pre_forward_call\"\n            ],\n        )\n\n        # Set up prefetching\n        for i, module in enumerate(ordered_forward_modules):\n            if i == 0:\n                module.set_modules_to_forward_prefetch(\n                    ordered_forward_modules[1 : self.prefetch]\n                )\n            else:\n                module.set_modules_to_forward_prefetch(\n                    ordered_forward_modules[i + self.prefetch : i + 1 + self.prefetch]\n                )\n\n        for i, module in enumerate(ordered_backwards_modules):\n            if i == 0:\n                module.set_modules_to_backward_prefetch(\n                    ordered_backwards_modules[1 : self.prefetch]\n                )\n            else:\n                module.set_modules_to_backward_prefetch(\n                    ordered_backwards_modules[i + self.prefetch : i + 1 + self.prefetch]\n                )\n\n        # Clear the stack for second training step\n        self.stack.clear()\n\n        # Add hook to log prefetching\n        FSDPParamGroup._prefetch_unshard = self.append_prefetch()\n\n    # Log a tree inorder of forward calls and prefetching\n    elif self.trainer.local_batches == 1:\n        FSDPParamGroup._prefetch_unshard = staticmethod(_original_prefetch_unshard)\n\n        for hook in self.hooks:\n            hook.remove()\n\n        if self.display:\n            _Node.from_stack(self.stack).print()\n</code></pre>"},{"location":"api/callbacks/performance/#usage_2","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import OptimizeFSDP\n\n# Basic FSDP optimization\nfsdp_opt = OptimizeFSDP(\n    prefetch=1,  # Prefetch 1 module ahead\n    display=False\n)\n\n# Aggressive prefetching\nfsdp_opt = OptimizeFSDP(\n    prefetch=2,  # Prefetch 2 modules ahead\n    display=True  # Show execution order\n)\n</code></pre>"},{"location":"api/callbacks/performance/#how-it-works","title":"How It Works","text":"<ol> <li>Traces execution order during first training step</li> <li>Sets up prefetching based on traced order</li> <li>Optimizes overlap between computation and communication</li> </ol> <p>Benefits: - 10-20% speedup for large models - Better GPU utilization - Reduced communication overhead</p>"},{"location":"api/callbacks/performance/#faulttolerancecallback","title":"FaultToleranceCallback","text":"<p>Enable fault-tolerant training with torchft:</p>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.FaultToleranceCallback","title":"FaultToleranceCallback","text":"<pre><code>FaultToleranceCallback(config: FaultToleranceParameters)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/ft.py</code> <pre><code>def __init__(self, config: FaultToleranceParameters):\n    self.config = config\n</code></pre>"},{"location":"api/callbacks/performance/#usage_3","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import FaultToleranceCallback\nfrom dream_trainer.configs import FaultToleranceParameters\n\nft_config = FaultToleranceParameters(\n    checkpoint_interval=100,  # Checkpoint every 100 steps\n    max_failures=3,          # Tolerate up to 3 failures\n    heartbeat_interval=60,   # Check health every 60 seconds\n)\n\nft_callback = FaultToleranceCallback(ft_config)\n</code></pre>"},{"location":"api/callbacks/performance/#fp8quantization","title":"Fp8Quantization","text":"<p>Enable FP8 training for memory and compute efficiency:</p>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.Fp8Quantization","title":"Fp8Quantization","text":"<pre><code>Fp8Quantization(recipe: str | Float8LinearRecipeName | None = None)\n</code></pre> Source code in <code>src/dream_trainer/callbacks/fp8.py</code> <pre><code>def __init__(self, recipe: str | Float8LinearRecipeName | None = None):\n    if not is_sm89_or_later():\n        raise ValueError(\"Native fp8 is only supported on H100+ GPUs.\")\n\n    self.recipe = Float8LinearRecipeName(recipe) if recipe else None\n</code></pre>"},{"location":"api/callbacks/performance/#dream_trainer.callbacks.Fp8Quantization-functions","title":"Functions","text":""},{"location":"api/callbacks/performance/#dream_trainer.callbacks.Fp8Quantization.post_optimizer_step","title":"post_optimizer_step","text":"<pre><code>post_optimizer_step(model: Module, optimizer: Optimizer)\n</code></pre> <p>Calculate scale dynamically for all float8 parameters. This should be run after the optimizer step. It performs a single all-reduce to compute the scales for all float8 weights.</p> <p>This callback hook assume there is one optimizer per model.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/callbacks/fp8.py</code> <pre><code>@override\ndef post_optimizer_step(self, model: nn.Module, optimizer: Optimizer):\n    \"\"\"\n    Calculate scale dynamically for all float8 parameters.\n    This should be run after the optimizer step. It performs a single all-reduce to compute the\n    scales for all float8 weights.\n\n    This callback hook assume there is one optimizer per model.\n    \"\"\"\n    if isinstance(model, FSDPModule):\n        # TODO: This could be done direclty on the optimizer param groups\n        precompute_float8_dynamic_scale_for_fsdp(cast(nn.Module, model))\n</code></pre>"},{"location":"api/callbacks/performance/#usage_4","title":"Usage","text":"<pre><code>from dream_trainer.callbacks import Fp8Quantization\n\n# Enable FP8 training\nfp8 = Fp8Quantization(\n    enabled=True,\n    amax_history_len=1024,  # History for scaling factors\n    amax_compute_algo=\"most_recent\"  # or \"max\"\n)\n</code></pre>"},{"location":"api/callbacks/performance/#creating-custom-performance-callbacks","title":"Creating Custom Performance Callbacks","text":""},{"location":"api/callbacks/performance/#throughput-monitor","title":"Throughput Monitor","text":"<pre><code>class ThroughputMonitor(Callback[BaseTrainer]):\n    \"\"\"Monitor training throughput.\"\"\"\n\n    def __init__(self, warmup_steps: int = 10):\n        self.warmup_steps = warmup_steps\n        self.start_time = None\n        self.samples_processed = 0\n\n    def pre_train_step(self, batch: dict[str, Any], batch_idx: int):\n        if batch_idx == self.warmup_steps:\n            self.start_time = time.time()\n            self.samples_processed = 0\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        if batch_idx &gt; self.warmup_steps and self.start_time is not None:\n            batch_size = self.trainer.config.batch_size\n            self.samples_processed += batch_size\n\n            # Log every 100 steps\n            if batch_idx % 100 == 0:\n                elapsed = time.time() - self.start_time\n                throughput = self.samples_processed / elapsed\n\n                print(f\"Throughput: {throughput:.2f} samples/sec\")\n\n                if hasattr(self.trainer, \"log_scalar\"):\n                    self.trainer.log_scalar(\"perf/throughput\", throughput)\n</code></pre>"},{"location":"api/callbacks/performance/#compilation-monitor","title":"Compilation Monitor","text":"<pre><code>class CompilationMonitor(Callback[BaseTrainer]):\n    \"\"\"Monitor torch.compile performance.\"\"\"\n\n    def __init__(self):\n        self.compile_times = {}\n        self.graph_breaks = {}\n\n    def pre_setup(self):\n        # Hook into torch.compile\n        original_compile = torch.compile\n\n        def monitored_compile(model, *args, **kwargs):\n            start = time.time()\n            compiled = original_compile(model, *args, **kwargs)\n            elapsed = time.time() - start\n\n            model_name = model.__class__.__name__\n            self.compile_times[model_name] = elapsed\n\n            print(f\"Compiled {model_name} in {elapsed:.2f}s\")\n            return compiled\n\n        torch.compile = monitored_compile\n\n    def post_setup(self):\n        # Report compilation summary\n        total_time = sum(self.compile_times.values())\n        print(f\"\\nCompilation Summary:\")\n        print(f\"Total time: {total_time:.2f}s\")\n\n        for name, time in self.compile_times.items():\n            print(f\"- {name}: {time:.2f}s\")\n</code></pre>"},{"location":"api/callbacks/performance/#memory-profiler","title":"Memory Profiler","text":"<pre><code>class MemoryProfiler(Callback[BaseTrainer]):\n    \"\"\"Profile memory usage patterns.\"\"\"\n\n    def __init__(self, profile_every: int = 100):\n        self.profile_every = profile_every\n        self.memory_stats = []\n\n    def post_train_step(self, result: dict[str, Any], batch_idx: int):\n        if batch_idx % self.profile_every == 0:\n            if torch.cuda.is_available():\n                # Get memory stats\n                allocated = torch.cuda.memory_allocated() / 1024**3\n                reserved = torch.cuda.memory_reserved() / 1024**3\n\n                # Get peak stats\n                peak_allocated = torch.cuda.max_memory_allocated() / 1024**3\n                peak_reserved = torch.cuda.max_memory_reserved() / 1024**3\n\n                stats = {\n                    \"step\": batch_idx,\n                    \"allocated_gb\": allocated,\n                    \"reserved_gb\": reserved,\n                    \"peak_allocated_gb\": peak_allocated,\n                    \"peak_reserved_gb\": peak_reserved,\n                    \"fragmentation\": (reserved - allocated) / reserved\n                }\n\n                self.memory_stats.append(stats)\n\n                # Log if fragmentation is high\n                if stats[\"fragmentation\"] &gt; 0.3:\n                    print(f\"\u26a0\ufe0f High memory fragmentation: {stats['fragmentation']:.1%}\")\n\n    def post_fit(self):\n        # Save detailed report\n        import json\n        with open(\"memory_profile.json\", \"w\") as f:\n            json.dump(self.memory_stats, f, indent=2)\n</code></pre>"},{"location":"api/callbacks/performance/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"api/callbacks/performance/#1-profiling-strategy","title":"1. Profiling Strategy","text":"<pre><code># Profile different phases\ncallbacks = CallbackCollection([\n    # Warmup without profiling\n    WarmupCallback(steps=100),\n\n    # Profile training\n    ProfileCallback(\n        torch.profiler.profile(\n            activities=[ProfilerActivity.CUDA],\n            schedule=schedule(wait=100, warmup=10, active=20)\n        )\n    ),\n\n    # Monitor throughput\n    ThroughputMonitor(warmup_steps=100)\n])\n</code></pre>"},{"location":"api/callbacks/performance/#2-fsdp-optimization","title":"2. FSDP Optimization","text":"<pre><code># Combine FSDP optimizations\ncallbacks = CallbackCollection([\n    # Optimize prefetching\n    OptimizeFSDP(prefetch=2),\n\n    # Monitor memory\n    MemoryProfiler(profile_every=50),\n\n    # Track throughput\n    ThroughputMonitor()\n])\n</code></pre>"},{"location":"api/callbacks/performance/#3-debugging-compilation","title":"3. Debugging Compilation","text":"<pre><code># Debug torch.compile issues\ncallbacks = CallbackCollection([\n    # Find graph breaks\n    FindGraphBreaksCallback(\"breaks.log\"),\n\n    # Monitor compilation\n    CompilationMonitor(),\n\n    # Profile compiled vs eager\n    ProfileCallback(profiler)\n])\n</code></pre>"},{"location":"api/callbacks/performance/#analyzing-performance","title":"Analyzing Performance","text":""},{"location":"api/callbacks/performance/#using-tensorboard","title":"Using TensorBoard","text":"<pre><code># Launch TensorBoard\n# tensorboard --logdir=./logs\n\n# View:\n# - GPU utilization\n# - Kernel timings\n# - Memory usage\n# - Operator breakdown\n</code></pre>"},{"location":"api/callbacks/performance/#creating-reports","title":"Creating Reports","text":"<pre><code>class PerformanceReporter(Callback[BaseTrainer]):\n    \"\"\"Generate performance reports.\"\"\"\n\n    def __init__(self):\n        self.metrics = {\n            \"throughput\": [],\n            \"gpu_util\": [],\n            \"memory\": [],\n            \"compile_time\": 0\n        }\n\n    def post_fit(self):\n        # Generate report\n        report = f\"\"\"\nPerformance Report\n==================\n\nTraining Summary:\n- Total time: {self.total_time:.2f}s\n- Average throughput: {np.mean(self.metrics['throughput']):.2f} samples/s\n- Peak memory: {max(self.metrics['memory']):.2f} GB\n- Compilation time: {self.metrics['compile_time']:.2f}s\n\nGPU Utilization:\n- Average: {np.mean(self.metrics['gpu_util']):.1f}%\n- Min: {min(self.metrics['gpu_util']):.1f}%\n- Max: {max(self.metrics['gpu_util']):.1f}%\n\nRecommendations:\n{self.get_recommendations()}\n\"\"\"\n\n        with open(\"performance_report.txt\", \"w\") as f:\n            f.write(report)\n</code></pre>"},{"location":"api/callbacks/performance/#best-practices","title":"Best Practices","text":""},{"location":"api/callbacks/performance/#1-profile-in-stages","title":"1. Profile in Stages","text":"<pre><code># Don't profile everything at once\n# Stage 1: Data loading\n# Stage 2: Forward pass\n# Stage 3: Backward pass\n# Stage 4: Optimizer step\n</code></pre>"},{"location":"api/callbacks/performance/#2-use-appropriate-tools","title":"2. Use Appropriate Tools","text":"<ul> <li>CPU bottlenecks: cProfile, py-spy</li> <li>GPU bottlenecks: nsight, PyTorch profiler</li> <li>Memory issues: memory_profiler, tracemalloc</li> <li>Compilation: torch._dynamo.explain</li> </ul>"},{"location":"api/callbacks/performance/#3-automate-analysis","title":"3. Automate Analysis","text":"<pre><code>class AutoAnalyzer(Callback[BaseTrainer]):\n    \"\"\"Automatically identify bottlenecks.\"\"\"\n\n    def analyze_profile(self, profile_data):\n        # Find slow operations\n        slow_ops = self.find_slow_operations(profile_data)\n\n        # Detect patterns\n        if self.is_communication_bound(profile_data):\n            print(\"Training is communication bound\")\n            print(\"Consider: larger batch size, gradient accumulation\")\n\n        if self.is_memory_bound(profile_data):\n            print(\"Training is memory bound\")\n            print(\"Consider: activation checkpointing, CPU offload\")\n</code></pre>"},{"location":"api/callbacks/performance/#see-also","title":"See Also","text":"<ul> <li>Callback System - Base callback documentation</li> <li>Optimization Guide - Performance tuning</li> <li>PyTorch Profiler</li> <li>torch.compile Guide </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/device/","title":"Device Configuration","text":"<p>For device and parallelism configuration, see DeviceParameters in the main configuration documentation.</p> <p>Key topics covered: - Single device training - FSDP (Fully Sharded Data Parallel) - HSDP (Hybrid Sharded Data Parallel) - DDP (Distributed Data Parallel) - Tensor parallelism - Pipeline parallelism - Mixed precision settings - Model compilation options </p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/","title":"Configuration Parameters","text":"<p>Dream Trainer uses strongly-typed configuration classes to manage all aspects of training. This ensures type safety, enables IDE autocomplete, and provides clear documentation of all available options.</p>"},{"location":"api/configuration/parameters/#training-configuration","title":"Training Configuration","text":""},{"location":"api/configuration/parameters/#trainingparameters","title":"TrainingParameters","text":"<p>Core training hyperparameters:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.TrainingParameters","title":"TrainingParameters  <code>dataclass</code>","text":"<pre><code>TrainingParameters(n_epochs: int | None = None, train_steps_per_epoch: int | None = None, train_batch_size: int = 1, val_steps_per_epoch: int | None = None, val_frequency: float = 1.0, num_sanity_val_steps: int = 0, gradient_clip_val: float | None = None)\n</code></pre>"},{"location":"api/configuration/parameters/#usage-example","title":"Usage Example","text":"<pre><code>from dream_trainer.configs import TrainingParameters\n\ntraining_params = TrainingParameters(\n    n_epochs=10,\n    train_batch_size=32,\n    gradient_clip_val=1.0,\n    val_frequency=0.5,  # Validate every half epoch\n    num_sanity_val_steps=2\n)\n</code></pre>"},{"location":"api/configuration/parameters/#device-configuration","title":"Device Configuration","text":""},{"location":"api/configuration/parameters/#deviceparameters","title":"DeviceParameters","text":"<p>Configuration for devices and parallelism strategies:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters","title":"DeviceParameters  <code>dataclass</code>","text":"<pre><code>DeviceParameters(*, param_dtype: dtype = torch.bfloat16, reduce_dtype: dtype = torch.float32, cpu_offload: bool = False, checkpoint_activations: bool = False, enable_compiled_autograd: bool = False, compile_model: bool = True, async_tensor_parallel: bool = True, loss_parallel: bool = False, comm: Comm = Comm(), _dp_shard: int | Literal['auto'] = 1, _dp_replicate: int | Literal['auto'] = 'auto', _tensor_parallel: int | Literal['auto'] = 1, _context_parallel: int | Literal['auto'] = 1, _pipeline_parallel: int | Literal['auto'] = 1, context_parallel_rotate_method: Literal['allgather', 'alltoall'] = 'allgather', run_single_device_as_fsdp: bool = False, run_single_device_as_ddp: bool = False)\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters-attributes","title":"Attributes","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.dp_shard","title":"dp_shard  <code>property</code>","text":"<pre><code>dp_shard: int\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.dp_replicate","title":"dp_replicate  <code>property</code>","text":"<pre><code>dp_replicate: int\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.tensor_parallel","title":"tensor_parallel  <code>property</code>","text":"<pre><code>tensor_parallel: int\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.context_parallel","title":"context_parallel  <code>property</code>","text":"<pre><code>context_parallel: int\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.pipeline_parallel","title":"pipeline_parallel  <code>property</code>","text":"<pre><code>pipeline_parallel: int\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters-functions","title":"Functions","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>def __post_init__(self):\n    if self.run_single_device_as_fsdp and self.run_single_device_as_ddp:\n        raise ValueError(\"Cannot run single device as both FSDP and DDP\")\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>def validate(self):\n    parallelism_dimensions = {\n        \"_dp_shard\": self._dp_shard,\n        \"_dp_replicate\": self._dp_replicate,\n        \"_context_parallel\": self._context_parallel,\n        \"_tensor_parallel\": self._tensor_parallel,\n        \"_pipeline_parallel\": self._pipeline_parallel,\n    }\n\n    auto_dims = [dim for dim, degree in parallelism_dimensions.items() if degree == \"auto\"]\n    assert len(auto_dims) &lt;= 1, \"At most one parallelism dimension can be set to 'auto'\"\n    assert all(\n        degree == \"auto\" or int(degree) &gt; 0 for degree in parallelism_dimensions.values()\n    ), \"All parallelism dimensions must be set to 'auto' or a positive integer. Got {self}\"\n\n    world_size = dist_util.core.get_dist_world_size()\n    if len(auto_dims) == 1:\n        remainder = prod(int(d) for d in parallelism_dimensions.values() if d != \"auto\")\n        assert world_size % remainder == 0, (\n            f\"World size, {world_size} must be divisible by the product of the non-auto dimensions {remainder}. Got {self}\"\n        )\n        setattr(self, auto_dims[0], world_size // remainder)\n    else:\n        assert prod(map(int, parallelism_dimensions.values())) == world_size, (\n            f\"The product of the parallelism dimensions must equal the world size, {world_size}. Got {self}\"\n        )\n\n    if self.async_tensor_parallel and not self.compile_model:\n        raise ValueError(\"Async tensor parallelism requires model compilation\")\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.FSDP","title":"FSDP  <code>classmethod</code>","text":"<pre><code>FSDP(tensor_parallel: int | Literal['auto'] = 'auto', dp_shard: int | Literal['auto'] = 'auto', compile_model: bool = True, cpu_offload: bool = False, checkpoint_activations: bool = False, enable_compiled_autograd: bool = False) -&gt; DeviceParameters\n</code></pre> <p>TP within nodes DP-Shard (FSDP) across nodes</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>@classmethod\ndef FSDP(\n    cls,\n    tensor_parallel: int | Literal[\"auto\"] = \"auto\",\n    dp_shard: int | Literal[\"auto\"] = \"auto\",\n    compile_model: bool = True,\n    cpu_offload: bool = False,\n    checkpoint_activations: bool = False,\n    enable_compiled_autograd: bool = False,\n) -&gt; \"DeviceParameters\":\n    \"\"\"\n    TP within nodes\n    DP-Shard (FSDP) across nodes\n    \"\"\"\n    global_world_size = dist_util.core.get_dist_world_size()\n    local_world_size = dist_util.core.get_dist_local_world_size()\n    if tensor_parallel == \"auto\" and dp_shard == \"auto\":\n        logger.info(\n            \"Using FSDP with both tensor_parallel and dp_shard set to auto, \"\n            f\"setting dp_shard=1 and tensor_parallel={local_world_size}\"\n        )\n        tensor_parallel = local_world_size\n        dp_shard = global_world_size // tensor_parallel\n    elif tensor_parallel == \"auto\":\n        assert isinstance(dp_shard, int)\n        tensor_parallel = global_world_size // dp_shard\n        if tensor_parallel &gt; local_world_size:\n            raise ValueError(\n                f\"tensor_parallel={tensor_parallel} is greater than the local world size, {local_world_size}\"\n            )\n        logger.info(\n            f'Got tensor_parallel=\"auto\". Setting tensor_parallel={tensor_parallel}'\n        )\n    elif dp_shard == \"auto\":\n        assert isinstance(tensor_parallel, int)\n        dp_shard = global_world_size // tensor_parallel\n        logger.info(f'Got dp_shard=\"auto\". Setting dp_shard={dp_shard}')\n\n    return cls(\n        _tensor_parallel=tensor_parallel,\n        compile_model=compile_model,\n        cpu_offload=cpu_offload,\n        checkpoint_activations=checkpoint_activations,\n        _dp_shard=dp_shard,\n        _dp_replicate=1,\n        _context_parallel=1,\n        _pipeline_parallel=1,\n        enable_compiled_autograd=enable_compiled_autograd,\n    )\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.HSDP","title":"HSDP  <code>classmethod</code>","text":"<pre><code>HSDP(dp_shard: int, tensor_parallel: int | Literal['auto'] = 'auto', compile_model: bool = True, cpu_offload: bool = False, checkpoint_activations: bool = False, enable_compiled_autograd: bool = False) -&gt; DeviceParameters\n</code></pre> <p>Hybrid Sharding Parallelism</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>@classmethod\ndef HSDP(\n    cls,\n    dp_shard: int,\n    tensor_parallel: int | Literal[\"auto\"] = \"auto\",\n    compile_model: bool = True,\n    cpu_offload: bool = False,\n    checkpoint_activations: bool = False,\n    enable_compiled_autograd: bool = False,\n) -&gt; \"DeviceParameters\":\n    \"\"\"\n    Hybrid Sharding Parallelism\n    \"\"\"\n\n    if tensor_parallel == \"auto\":\n        assert isinstance(dp_shard, int), (\n            \"dp_shard must be an integer if tensor_parallel is auto\"\n        )\n        tensor_parallel = dist_util.core.get_dist_local_world_size()\n\n    return cls(\n        _tensor_parallel=tensor_parallel,\n        compile_model=compile_model,\n        cpu_offload=cpu_offload,\n        checkpoint_activations=checkpoint_activations,\n        _dp_replicate=\"auto\",\n        _dp_shard=dp_shard,\n        _context_parallel=1,\n        _pipeline_parallel=1,\n        enable_compiled_autograd=enable_compiled_autograd,\n    )\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.DDP","title":"DDP  <code>classmethod</code>","text":"<pre><code>DDP(cpu_offload: bool = False, checkpoint_activations: bool = False, compile_model: bool = True, enable_compiled_autograd: bool = False) -&gt; DeviceParameters\n</code></pre> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>@classmethod\ndef DDP(\n    cls,\n    cpu_offload: bool = False,\n    checkpoint_activations: bool = False,\n    compile_model: bool = True,\n    enable_compiled_autograd: bool = False,\n) -&gt; \"DeviceParameters\":\n    return cls(\n        cpu_offload=cpu_offload,\n        checkpoint_activations=checkpoint_activations,\n        compile_model=compile_model,\n        _dp_replicate=\"auto\",\n        _dp_shard=1,\n        _tensor_parallel=1,\n        _context_parallel=1,\n        _pipeline_parallel=1,\n        enable_compiled_autograd=enable_compiled_autograd,\n    )\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.DeviceParameters.SINGLE_DEVICE","title":"SINGLE_DEVICE  <code>classmethod</code>","text":"<pre><code>SINGLE_DEVICE(compile_model: bool = True, cpu_offload: bool = False, checkpoint_activations: bool = False) -&gt; DeviceParameters\n</code></pre> Source code in <code>src/dream_trainer/configs/trainer.py</code> <pre><code>@classmethod\ndef SINGLE_DEVICE(\n    cls,\n    compile_model: bool = True,\n    cpu_offload: bool = False,\n    checkpoint_activations: bool = False,\n) -&gt; \"DeviceParameters\":\n    return cls(\n        compile_model=compile_model,\n        cpu_offload=cpu_offload,\n        checkpoint_activations=checkpoint_activations,\n        _dp_replicate=1,\n        _dp_shard=1,\n        _tensor_parallel=1,\n        _context_parallel=1,\n        _pipeline_parallel=1,\n        async_tensor_parallel=False,\n    )\n</code></pre>"},{"location":"api/configuration/parameters/#usage-examples","title":"Usage Examples","text":""},{"location":"api/configuration/parameters/#single-device","title":"Single Device","text":"<pre><code>from dream_trainer.configs import DeviceParameters\n\n# Single GPU training\ndevice_params = DeviceParameters.SINGLE_DEVICE(\n    compile_model=True,\n    checkpoint_activations=False\n)\n</code></pre>"},{"location":"api/configuration/parameters/#fsdp-fully-sharded-data-parallel","title":"FSDP (Fully Sharded Data Parallel)","text":"<pre><code># Automatic configuration\ndevice_params = DeviceParameters.FSDP()\n\n# Manual configuration\ndevice_params = DeviceParameters.FSDP(\n    tensor_parallel=8,    # TP within nodes\n    dp_shard=4,          # FSDP across nodes\n    cpu_offload=False,\n    checkpoint_activations=True\n)\n</code></pre>"},{"location":"api/configuration/parameters/#hsdp-hybrid-sharded-data-parallel","title":"HSDP (Hybrid Sharded Data Parallel)","text":"<pre><code># HSDP with replication and sharding\ndevice_params = DeviceParameters.HSDP(\n    dp_shard=4,          # Shard within replicas\n    tensor_parallel=8,   # TP degree\n    compile_model=True\n)\n</code></pre>"},{"location":"api/configuration/parameters/#ddp-distributed-data-parallel","title":"DDP (Distributed Data Parallel)","text":"<pre><code># Traditional data parallelism\ndevice_params = DeviceParameters.DDP(\n    compile_model=True,\n    checkpoint_activations=False\n)\n</code></pre>"},{"location":"api/configuration/parameters/#communication-configuration","title":"Communication Configuration","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.Comm","title":"Comm  <code>dataclass</code>","text":"<pre><code>Comm(init_timeout_seconds: int = 300, train_timeout_seconds: int = 300, trace_buf_size: int = 20000)\n</code></pre>"},{"location":"api/configuration/parameters/#dream_trainer.configs.Comm-attributes","title":"Attributes","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.Comm.init_timeout_seconds","title":"init_timeout_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_timeout_seconds: int = 300\n</code></pre> <p>Timeout for communication operations, during initialization and first train step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.Comm.train_timeout_seconds","title":"train_timeout_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_timeout_seconds: int = 300\n</code></pre> <p>Timeout for communication operations after the first train step -- usually a tighter bound than during initialization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.Comm.trace_buf_size","title":"trace_buf_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_buf_size: int = 20000\n</code></pre> <p>Flight recorder ring buffer size, &gt;0 means recording by default, 0 means disabled</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#checkpoint-configuration","title":"Checkpoint Configuration","text":""},{"location":"api/configuration/parameters/#checkpointparameters","title":"CheckpointParameters","text":"<p>Configuration for model checkpointing:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.CheckpointParameters","title":"CheckpointParameters  <code>dataclass</code>","text":"<pre><code>CheckpointParameters(*, enable: bool, root_dir: str | Path, resume_mode: Literal['min', 'max', 'last'] = 'last', monitor: str = 'train/loss', checkpoint_every_n_epochs: int | None = None, checkpoint_every_n_steps: int | None = None, keep_top_k: int = 5, strict_load: bool = False, model_weights_only: bool, exclude_from_loading: list[str], pin_memory: bool = False)\n</code></pre>"},{"location":"api/configuration/parameters/#usage-example_1","title":"Usage Example","text":"<pre><code>from dream_trainer.configs import CheckpointParameters\n\ncheckpoint_params = CheckpointParameters(\n    enable=True,\n    root_dir=\"./checkpoints\",\n    monitor=\"val_loss\",\n    resume_mode=\"min\",  # Resume from best (minimum) loss\n    checkpoint_every_n_epochs=1,\n    keep_top_k=3,\n    model_weights_only=False,\n    exclude_from_loading=[],\n    strict_load=True\n)\n</code></pre>"},{"location":"api/configuration/parameters/#logging-configuration","title":"Logging Configuration","text":""},{"location":"api/configuration/parameters/#loggingparameters","title":"LoggingParameters","text":"<p>Base logging configuration:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.LoggingParameters","title":"LoggingParameters  <code>dataclass</code>","text":"<pre><code>LoggingParameters(*, enabled: bool = True)\n</code></pre>"},{"location":"api/configuration/parameters/#wandbloggingparameters","title":"WandbLoggingParameters","text":"<p>Weights &amp; Biases specific configuration:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.WandbLoggingParameters","title":"WandbLoggingParameters  <code>dataclass</code>","text":"<pre><code>WandbLoggingParameters(*, enabled: bool = True)\n</code></pre>"},{"location":"api/configuration/parameters/#fault-tolerance-configuration","title":"Fault Tolerance Configuration","text":""},{"location":"api/configuration/parameters/#faulttoleranceparameters","title":"FaultToleranceParameters","text":"<p>Configuration for fault-tolerant training:</p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.FaultToleranceParameters","title":"FaultToleranceParameters","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.FaultToleranceParameters-attributes","title":"Attributes","text":""},{"location":"api/configuration/parameters/#dream_trainer.configs.FaultToleranceParameters.replica_prefix","title":"replica_prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>replica_prefix: str | None = None\n</code></pre> <p>Prefix for the replica ID. If None, the prefix will be set with the experiment name.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.FaultToleranceParameters.min_replica_size","title":"min_replica_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_replica_size: int = 1\n</code></pre> <p>Minimum number of replicas to use for fault tolerance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#dream_trainer.configs.FaultToleranceParameters.max_consecutive_failures","title":"max_consecutive_failures  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_consecutive_failures: int = 3\n</code></pre> <p>Maximum number of consecutive failures before stopping the training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/parameters/#usage-example_2","title":"Usage Example","text":"<pre><code>from dream_trainer.configs import FaultToleranceParameters\n\nft_params = FaultToleranceParameters(\n    enable=True,\n    lighthouse_address=\"tcp://lighthouse:8080\",\n    min_replica_size=2,\n    max_consecutive_failures=3\n)\n</code></pre>"},{"location":"api/configuration/parameters/#complete-configuration-example","title":"Complete Configuration Example","text":"<p>Here's how to combine all configuration parameters:</p> <pre><code>from dream_trainer.trainer import DreamTrainerConfig\nfrom dream_trainer.configs import (\n    TrainingParameters,\n    DeviceParameters,\n    CheckpointParameters,\n    LoggingParameters,\n)\nfrom dream_trainer.callbacks import CallbackCollection\n\n# Create complete configuration\nconfig = DreamTrainerConfig(\n    # Basic info\n    seed=42,\n    project=\"my_project\",\n    group=\"experiments\",\n    experiment=\"baseline_v1\",\n\n    # Device setup for multi-GPU\n    device_parameters=DeviceParameters.FSDP(\n        tensor_parallel=4,\n        dp_shard=\"auto\",\n        compile_model=True,\n        checkpoint_activations=True\n    ),\n\n    # Training hyperparameters\n    training_parameters=TrainingParameters(\n        n_epochs=100,\n        train_batch_size=16,\n        gradient_clip_val=1.0,\n        val_frequency=0.2,  # 5 times per epoch\n        num_sanity_val_steps=2\n    ),\n\n    # Model configuration\n    model_config=ModelConfig(\n        hidden_size=768,\n        num_layers=12,\n        num_heads=12,\n        dropout=0.1\n    ),\n\n    # Optimizer configuration\n    optimizer_config=OptimizerConfig(\n        learning_rate=5e-5,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        eps=1e-8\n    ),\n\n    # Scheduler configuration\n    scheduler_config=SchedulerConfig(\n        warmup_steps=1000,\n        scheduler_type=\"cosine\"\n    ),\n\n    # DataLoader configuration\n    dataloader_config=DataLoaderConfig(\n        train_batch_size=16,\n        val_batch_size=32,\n        num_workers=4,\n        prefetch_factor=2\n    ),\n\n    # Logging configuration\n    logger_config=LoggingParameters(\n        enabled=True\n    ),\n\n    # Callbacks\n    callbacks=CallbackCollection([\n        # Add your callbacks here\n    ])\n)\n</code></pre>"},{"location":"api/configuration/parameters/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"api/configuration/parameters/#1-use-factory-methods","title":"1. Use Factory Methods","text":"<pre><code># Good - use factory methods for common patterns\ndevice_params = DeviceParameters.FSDP()\n\n# Avoid - manual configuration is error-prone\ndevice_params = DeviceParameters(\n    _dp_shard=4,\n    _dp_replicate=1,\n    _tensor_parallel=8,\n    # ... many more parameters\n)\n</code></pre>"},{"location":"api/configuration/parameters/#2-validate-early","title":"2. Validate Early","text":"<pre><code># Configurations validate themselves\ntry:\n    device_params = DeviceParameters(\n        _dp_shard=3,\n        _tensor_parallel=5\n    )\n    device_params.validate()  # Will raise if invalid\nexcept ValueError as e:\n    print(f\"Invalid configuration: {e}\")\n</code></pre>"},{"location":"api/configuration/parameters/#3-environment-based-configuration","title":"3. Environment-Based Configuration","text":"<pre><code>import os\n\n# Adjust based on environment\nif os.getenv(\"DISTRIBUTED_TRAINING\"):\n    device_params = DeviceParameters.FSDP()\nelse:\n    device_params = DeviceParameters.SINGLE_DEVICE()\n</code></pre>"},{"location":"api/configuration/parameters/#4-configuration-inheritance","title":"4. Configuration Inheritance","text":"<pre><code>from dataclasses import dataclass, field\n\n@dataclass\nclass MyModelConfig:\n    \"\"\"Custom model configuration.\"\"\"\n    hidden_size: int = 768\n    num_layers: int = 12\n    vocab_size: int = 50257\n\n@dataclass\nclass MyTrainerConfig(DreamTrainerConfig):\n    \"\"\"Extended trainer configuration.\"\"\"\n    model_config: MyModelConfig = field(default_factory=MyModelConfig)\n    custom_param: float = 0.1\n</code></pre>"},{"location":"api/configuration/parameters/#configuration-validation","title":"Configuration Validation","text":"<p>Dream Trainer validates configurations at multiple levels:</p> <ol> <li>Type Checking: Dataclasses ensure type correctness</li> <li>Value Validation: <code>__post_init__</code> methods check constraints</li> <li>Runtime Validation: <code>validate()</code> methods check compatibility</li> </ol> <p>Example validation:</p> <pre><code># This will raise an error\ncheckpoint_params = CheckpointParameters(\n    enable=True,\n    root_dir=\"./checkpoints\",\n    checkpoint_every_n_epochs=None,  # Error: must specify frequency\n    checkpoint_every_n_steps=None\n)\n\n# This will also raise an error\ndevice_params = DeviceParameters(\n    _dp_shard=\"auto\",\n    _tensor_parallel=\"auto\"  # Error: only one can be \"auto\"\n)\n</code></pre>"},{"location":"api/configuration/parameters/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Configuration examples</li> <li>Device Configuration - Detailed parallelism options</li> <li>Training Configuration - Training hyperparameters</li> <li>Trainer Guide - Using configurations </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/configuration/training/","title":"Training Configuration","text":"<p>For training hyperparameter configuration, see TrainingParameters in the main configuration documentation.</p> <p>Key topics covered: - Number of epochs and steps - Batch sizes - Gradient clipping - Validation frequency - Sanity checks </p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/eval_metric/","title":"Evaluation Metrics Mixin","text":"<p>The <code>EvalMetricMixin</code> provides seamless integration with torchmetrics for standardized evaluation during training. It automatically handles metric device placement, distributed synchronization, and lifecycle management.</p>"},{"location":"api/mixins/eval_metric/#overview","title":"Overview","text":"<p>The <code>EvalMetricMixin</code> enables: - Automatic metric registration and tracking - Device placement for GPU/TPU training - Distributed metric synchronization - Integration with any torchmetrics metric - Clean separation of metric definition and usage</p> <p>Installation Required</p> <p>This mixin requires torchmetrics to be installed: <pre><code>pip install dream-trainer[metrics]\n</code></pre></p>"},{"location":"api/mixins/eval_metric/#class-reference","title":"Class Reference","text":""},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricMixin","title":"EvalMetricMixin","text":"<pre><code>EvalMetricMixin(config: AbstractTrainerConfig)\n</code></pre> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricMixin-functions","title":"Functions","text":""},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricMixin.configure_metrics","title":"configure_metrics  <code>abstractmethod</code>","text":"<pre><code>configure_metrics()\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/eval_metric.py</code> <pre><code>@abstractmethod\ndef configure_metrics(self):\n    pass\n</code></pre>"},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricMixin.named_metrics","title":"named_metrics","text":"<pre><code>named_metrics() -&gt; dict[str, MetricCollection]\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/eval_metric.py</code> <pre><code>def named_metrics(self) -&gt; dict[str, MetricCollection]:\n    return {name: getattr(self, name) for name in self._metric_names}\n</code></pre>"},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricMixin.get_metric","title":"get_metric","text":"<pre><code>get_metric(name: str) -&gt; MetricCollection\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/eval_metric.py</code> <pre><code>def get_metric(self, name: str) -&gt; MetricCollection:\n    return getattr(self, name)\n</code></pre>"},{"location":"api/mixins/eval_metric/#configuration","title":"Configuration","text":""},{"location":"api/mixins/eval_metric/#dream_trainer.trainer.mixins.EvalMetricConfigMixin","title":"EvalMetricConfigMixin  <code>dataclass</code>","text":"<pre><code>EvalMetricConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre>"},{"location":"api/mixins/eval_metric/#usage-example","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer.mixins import EvalMetricMixin, EvalMetricConfigMixin\nfrom dream_trainer.trainer import BaseTrainer\nimport torchmetrics\n\nclass MyTrainer(BaseTrainer, EvalMetricMixin):\n    def configure_metrics(self):\n        # Metrics are automatically tracked when assigned as attributes\n        self.accuracy = torchmetrics.Accuracy(\n            task=\"multiclass\",\n            num_classes=10\n        )\n\n        self.top5_accuracy = torchmetrics.Accuracy(\n            task=\"multiclass\",\n            num_classes=10,\n            top_k=5\n        )\n\n        self.f1_score = torchmetrics.F1Score(\n            task=\"multiclass\",\n            num_classes=10,\n            average=\"macro\"\n        )\n\n        # MetricCollection for grouped metrics\n        self.classification_metrics = torchmetrics.MetricCollection({\n            \"precision\": torchmetrics.Precision(task=\"multiclass\", num_classes=10),\n            \"recall\": torchmetrics.Recall(task=\"multiclass\", num_classes=10),\n            \"auroc\": torchmetrics.AUROC(task=\"multiclass\", num_classes=10)\n        })\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x)\n        loss = F.cross_entropy(logits, y)\n\n        # Update metrics during training\n        self.accuracy(logits, y)\n        self.log_scalar(\"train/accuracy\", self.accuracy)\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x)\n        loss = F.cross_entropy(logits, y)\n\n        # Update all metrics\n        self.accuracy(logits, y)\n        self.top5_accuracy(logits, y)\n        self.f1_score(logits, y)\n        self.classification_metrics(logits, y)\n\n        return {\"val_loss\": loss}\n\n    def on_validation_epoch_end(self):\n        # Compute and log final metrics\n        self.log_scalar(\"val/accuracy\", self.accuracy.compute())\n        self.log_scalar(\"val/top5_accuracy\", self.top5_accuracy.compute())\n        self.log_scalar(\"val/f1_score\", self.f1_score.compute())\n\n        # Log collection metrics\n        metrics = self.classification_metrics.compute()\n        for name, value in metrics.items():\n            self.log_scalar(f\"val/{name}\", value)\n\n        # Reset metrics for next epoch\n        self.accuracy.reset()\n        self.top5_accuracy.reset()\n        self.f1_score.reset()\n        self.classification_metrics.reset()\n</code></pre>"},{"location":"api/mixins/eval_metric/#common-metrics","title":"Common Metrics","text":""},{"location":"api/mixins/eval_metric/#classification-metrics","title":"Classification Metrics","text":"<pre><code>def configure_metrics(self):\n    # Accuracy variants\n    self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n    self.balanced_accuracy = torchmetrics.Accuracy(\n        task=\"multiclass\", \n        num_classes=10, \n        average=\"macro\"\n    )\n\n    # Precision, Recall, F1\n    self.precision = torchmetrics.Precision(\n        task=\"multiclass\", \n        num_classes=10,\n        average=\"weighted\"\n    )\n    self.recall = torchmetrics.Recall(\n        task=\"multiclass\",\n        num_classes=10,\n        average=\"weighted\"\n    )\n    self.f1 = torchmetrics.F1Score(\n        task=\"multiclass\",\n        num_classes=10,\n        average=\"weighted\"\n    )\n\n    # ROC and PR curves\n    self.auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=10)\n    self.avg_precision = torchmetrics.AveragePrecision(\n        task=\"multiclass\",\n        num_classes=10\n    )\n</code></pre>"},{"location":"api/mixins/eval_metric/#regression-metrics","title":"Regression Metrics","text":"<pre><code>def configure_metrics(self):\n    # Error metrics\n    self.mae = torchmetrics.MeanAbsoluteError()\n    self.mse = torchmetrics.MeanSquaredError()\n    self.rmse = torchmetrics.MeanSquaredError(squared=False)\n\n    # Correlation metrics\n    self.pearson = torchmetrics.PearsonCorrCoef()\n    self.spearman = torchmetrics.SpearmanCorrCoef()\n\n    # R-squared\n    self.r2 = torchmetrics.R2Score()\n</code></pre>"},{"location":"api/mixins/eval_metric/#nlp-metrics","title":"NLP Metrics","text":"<pre><code>def configure_metrics(self):\n    # BLEU score for translation\n    self.bleu = torchmetrics.BLEUScore()\n\n    # ROUGE for summarization\n    self.rouge = torchmetrics.ROUGEScore()\n\n    # Perplexity for language modeling\n    self.perplexity = torchmetrics.Perplexity()\n\n    # Character/Word error rate\n    self.cer = torchmetrics.CharErrorRate()\n    self.wer = torchmetrics.WordErrorRate()\n</code></pre>"},{"location":"api/mixins/eval_metric/#computer-vision-metrics","title":"Computer Vision Metrics","text":"<pre><code>def configure_metrics(self):\n    # Segmentation metrics\n    self.iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=20)\n    self.dice = torchmetrics.Dice(multiclass=True, num_classes=20)\n\n    # Detection metrics\n    self.map = torchmetrics.detection.MeanAveragePrecision()\n\n    # Image quality metrics\n    self.psnr = torchmetrics.PeakSignalNoiseRatio()\n    self.ssim = torchmetrics.StructuralSimilarityIndexMeasure()\n    self.fid = torchmetrics.image.FrechetInceptionDistance()\n</code></pre>"},{"location":"api/mixins/eval_metric/#metriccollection","title":"MetricCollection","text":"<p>Use <code>MetricCollection</code> to group related metrics:</p> <pre><code>def configure_metrics(self):\n    # Group metrics by task\n    self.train_metrics = torchmetrics.MetricCollection({\n        \"loss\": torchmetrics.MeanMetric(),\n        \"accuracy\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=10),\n        \"learning_rate\": torchmetrics.MeanMetric()\n    })\n\n    self.val_metrics = torchmetrics.MetricCollection({\n        \"loss\": torchmetrics.MeanMetric(),\n        \"accuracy\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=10),\n        \"f1\": torchmetrics.F1Score(task=\"multiclass\", num_classes=10),\n        \"auroc\": torchmetrics.AUROC(task=\"multiclass\", num_classes=10)\n    })\n\n    # Group by metric type\n    self.classification_metrics = torchmetrics.MetricCollection({\n        \"accuracy\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=10),\n        \"precision\": torchmetrics.Precision(task=\"multiclass\", num_classes=10),\n        \"recall\": torchmetrics.Recall(task=\"multiclass\", num_classes=10),\n        \"f1\": torchmetrics.F1Score(task=\"multiclass\", num_classes=10)\n    })\n</code></pre>"},{"location":"api/mixins/eval_metric/#distributed-training","title":"Distributed Training","text":"<p>Metrics are automatically synchronized across devices in distributed training:</p> <pre><code>def validation_step(self, batch, batch_idx):\n    # Metrics automatically sync across GPUs\n    logits = self.model(batch[\"input\"])\n\n    # Update happens on each device\n    self.accuracy(logits, batch[\"target\"])\n\n    # Compute aggregates across all devices\n    if batch_idx == self.num_val_batches - 1:\n        # This automatically reduces across all processes\n        final_accuracy = self.accuracy.compute()\n        self.log_scalar(\"val/accuracy\", final_accuracy)\n</code></pre>"},{"location":"api/mixins/eval_metric/#custom-metrics","title":"Custom Metrics","text":"<p>Create custom metrics by extending <code>torchmetrics.Metric</code>:</p> <pre><code>class CustomMetric(torchmetrics.Metric):\n    def __init__(self):\n        super().__init__()\n        # Define state variables\n        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        # Update state based on predictions\n        self.correct += (preds == target).sum()\n        self.total += target.numel()\n\n    def compute(self):\n        # Compute final metric\n        return self.correct.float() / self.total\n\n# Use in trainer\ndef configure_metrics(self):\n    self.custom_metric = CustomMetric()\n</code></pre>"},{"location":"api/mixins/eval_metric/#best-practices","title":"Best Practices","text":""},{"location":"api/mixins/eval_metric/#1-metric-lifecycle","title":"1. Metric Lifecycle","text":"<pre><code>def on_train_epoch_start(self):\n    # Reset metrics at epoch start\n    self.train_metrics.reset()\n\ndef training_step(self, batch, batch_idx):\n    # Update metrics during training\n    loss = self.compute_loss(batch)\n    self.train_metrics[\"loss\"](loss)\n\ndef on_train_epoch_end(self):\n    # Compute and log final values\n    metrics = self.train_metrics.compute()\n    for name, value in metrics.items():\n        self.log_scalar(f\"train/{name}\", value)\n</code></pre>"},{"location":"api/mixins/eval_metric/#2-memory-management","title":"2. Memory Management","text":"<pre><code>def configure_metrics(self):\n    # For large-scale metrics, compute on CPU\n    self.confusion_matrix = torchmetrics.ConfusionMatrix(\n        task=\"multiclass\",\n        num_classes=1000,\n        compute_on_cpu=True  # Avoid GPU OOM\n    )\n</code></pre>"},{"location":"api/mixins/eval_metric/#3-metric-scheduling","title":"3. Metric Scheduling","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    # Only compute expensive metrics every N batches\n    if batch_idx % 10 == 0:\n        self.expensive_metric(preds, targets)\n\n    # Always compute cheap metrics\n    self.accuracy(preds, targets)\n</code></pre>"},{"location":"api/mixins/eval_metric/#4-multi-task-metrics","title":"4. Multi-Task Metrics","text":"<pre><code>def configure_metrics(self):\n    # Separate metrics for each task\n    self.task_metrics = nn.ModuleDict({\n        \"classification\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=10),\n        \"regression\": torchmetrics.MeanSquaredError(),\n        \"segmentation\": torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=20)\n    })\n\ndef validation_step(self, batch, batch_idx):\n    outputs = self.model(batch[\"input\"])\n\n    # Update task-specific metrics\n    self.task_metrics[\"classification\"](\n        outputs[\"class_logits\"], \n        batch[\"class_labels\"]\n    )\n    self.task_metrics[\"regression\"](\n        outputs[\"regression\"], \n        batch[\"regression_targets\"]\n    )\n</code></pre>"},{"location":"api/mixins/eval_metric/#integration-with-logging","title":"Integration with Logging","text":"<p>The EvalMetricMixin works seamlessly with logger mixins:</p> <pre><code>class MyTrainer(BaseTrainer, EvalMetricMixin, WandBLoggerMixin):\n    def on_validation_epoch_end(self):\n        # Compute all metrics\n        metrics = {\n            \"accuracy\": self.accuracy.compute(),\n            \"f1\": self.f1_score.compute(),\n            \"confusion_matrix\": self.confusion_matrix.compute()\n        }\n\n        # Log scalars\n        for name, value in metrics.items():\n            if isinstance(value, torch.Tensor) and value.numel() == 1:\n                self.log_scalar(f\"val/{name}\", value)\n\n        # Log confusion matrix as image\n        self.log_image(\n            self.plot_confusion_matrix(metrics[\"confusion_matrix\"]),\n            desc=\"val/confusion_matrix\"\n        )\n</code></pre>"},{"location":"api/mixins/eval_metric/#see-also","title":"See Also","text":"<ul> <li>torchmetrics documentation</li> <li>Logger Mixins - For logging metric values</li> <li>BaseTrainer - Core training functionality</li> <li>DreamTrainer - Complete example with metrics </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/loggers/","title":"Logger Mixins","text":"<p>The logger mixins provide standardized interfaces for experiment tracking and metric logging. Dream Trainer includes both a base <code>LoggerMixin</code> interface and specific implementations like <code>WandBLoggerMixin</code> for Weights &amp; Biases integration.</p>"},{"location":"api/mixins/loggers/#overview","title":"Overview","text":"<p>Logger mixins enable:</p> <ul> <li>Experiment tracking and configuration logging</li> <li>Scalar metric logging (loss, accuracy, etc.)</li> <li>Media logging (images, videos, plots)</li> <li>Model artifact tracking</li> <li>Distributed training support (only logs from rank 0)</li> </ul>"},{"location":"api/mixins/loggers/#base-logger-interface","title":"Base Logger Interface","text":""},{"location":"api/mixins/loggers/#loggermixin","title":"LoggerMixin","text":"<p>The base logger interface that all logging implementations should extend:</p>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin","title":"LoggerMixin","text":"<pre><code>LoggerMixin(config: LoggerConfigMixin)\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def __init__(self, config: LoggerConfigMixin):\n    super().__init__(config)\n\n    self.logging_parameters = config.logging_parameters\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin-functions","title":"Functions","text":""},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin.log_config","title":"log_config","text":"<pre><code>log_config(config: dict[str, Any] | Any)\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def log_config(self, config: dict[str, Any] | Any):\n    raise NotImplementedError(\"Please implement `log_config`\")\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin.log_dict","title":"log_dict","text":"<pre><code>log_dict(logs: dict[str, Any], **kwargs: Any)\n</code></pre> <p>Log a dictionary of scalar values (e.g., losses, metrics) to the logger.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>logs</code> <p>Dictionary containing key-value pairs to log.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def log_dict(self, logs: dict[str, Any], **kwargs: Any):\n    \"\"\"\n    Log a dictionary of scalar values (e.g., losses, metrics) to the logger.\n\n    Args:\n        logs (dict[str, Any]): Dictionary containing key-value pairs to log.\n    \"\"\"\n    raise NotImplementedError(\"Please implement `log_dict`\")\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin.log_images","title":"log_images","text":"<pre><code>log_images(images: Tensor, caption: list[str] | None = None, desc: str | None = None, **kwargs: Any)\n</code></pre> <p>Log a single image or a batch of images to the logger.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>image</code> <p>Image tensor of shape (B, C, H, W).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor</code> </p> <code>caption</code> <p>Optional list of captions for each image.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>desc</code> <p>Description/name for the logged images.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def log_images(\n    self,\n    images: torch.Tensor,\n    caption: list[str] | None = None,\n    desc: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Log a single image or a batch of images to the logger.\n\n    Args:\n        image (torch.Tensor): Image tensor of shape (B, C, H, W).\n        caption (list[str] | None): Optional list of captions for each image.\n        desc (str | None): Description/name for the logged images.\n    \"\"\"\n    raise NotImplementedError(\"Please implement `log_image`\")\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin.log_videos","title":"log_videos","text":"<pre><code>log_videos(videos: Tensor, caption: list[str] | None = None, desc: str | None = None, **kwargs: Any)\n</code></pre> <p>Log a single video or a batch of videos to the logger.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>video</code> <p>Video tensor of shape (B, C, T, H, W).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor</code> </p> <code>caption</code> <p>Optional list of captions for each video.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>desc</code> <p>Description/name for the logged videos.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def log_videos(\n    self,\n    videos: torch.Tensor,\n    caption: list[str] | None = None,\n    desc: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Log a single video or a batch of videos to the logger.\n\n    Args:\n        video (torch.Tensor): Video tensor of shape (B, C, T, H, W).\n        caption (list[str] | None): Optional list of captions for each video.\n        desc (str | None): Description/name for the logged videos.\n    \"\"\"\n    raise NotImplementedError(\"Please implement `log_video`\")\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerMixin.log_plot","title":"log_plot","text":"<pre><code>log_plot(plot: Any, desc: str | None = None, **kwargs: Any)\n</code></pre> <p>Log a plot to the logger.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/types.py</code> <pre><code>def log_plot(self, plot: Any, desc: str | None = None, **kwargs: Any):\n    \"\"\"\n    Log a plot to the logger.\n    \"\"\"\n    raise NotImplementedError(\"Please implement `log_plot`\")\n</code></pre>"},{"location":"api/mixins/loggers/#configuration","title":"Configuration","text":""},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.LoggerConfigMixin","title":"LoggerConfigMixin  <code>dataclass</code>","text":"<pre><code>LoggerConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters, logging_parameters: LoggingParameters = LoggingParameters())\n</code></pre>"},{"location":"api/mixins/loggers/#wandb-logger","title":"WandB Logger","text":"<p>The <code>WandBLoggerMixin</code> provides integration with Weights &amp; Biases for comprehensive experiment tracking.</p> <p>Installation Required</p> <p>This mixin requires wandb to be installed: <pre><code>pip install dream-trainer[wandb]\n</code></pre></p>"},{"location":"api/mixins/loggers/#wandbloggermixin","title":"WandBLoggerMixin","text":""},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin","title":"WandBLoggerMixin","text":"<pre><code>WandBLoggerMixin(config: WandBLoggerConfigMixin)\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>def __init__(self, config: WandBLoggerConfigMixin):\n    super().__init__(config)\n\n    self.logging_parameters = config.logging_parameters\n    self._wandb = wandb.init(\n        project=self.project,\n        group=self.group,\n        name=self.experiment,\n        id=self.experiment,  # id == name for resumption\n        resume=\"allow\",\n        mode=\"online\"\n        if (self.logging_parameters.enabled and self.world.is_global_zero)\n        else \"disabled\",\n    )\n\n    assert self._wandb is not None\n    self._wandb.define_metric(\"trainer/global_step\")\n    self._wandb.define_metric(\"*\", step_metric=\"trainer/global_step\", step_sync=True)\n\n    # Monkeypatch annoying wandb.Video \"Encoding video... \" log\n    try:\n        from wandb.sdk.data_types.video import printer_asyncio\n\n        printer_asyncio.run_async_with_spinner = lambda spinner_printer, text, func: func()\n    except ImportError:\n        pass\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin-functions","title":"Functions","text":""},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin.log_model","title":"log_model","text":"<pre><code>log_model(models: list[Module] | None = None, log: Literal['gradients', 'parameters', 'all'] | None = None, log_freq: int = 1000)\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>def log_model(\n    self,\n    models: list[nn.Module] | None = None,\n    log: Literal[\"gradients\", \"parameters\", \"all\"] | None = None,\n    log_freq: int = 1000,\n):\n    self._wandb.watch(\n        models or list(self.named_models().values()),\n        log=log,\n        log_freq=log_freq,\n    )\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin.log_dict","title":"log_dict","text":"<pre><code>log_dict(logs: dict[str, Any])\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>@override\ndef log_dict(self, logs: dict[str, Any]):\n    logs = {\n        k: v.float() if isinstance(v, Tensor) and v.dtype == torch.bfloat16 else v\n        for k, v in logs.items()\n    }\n    self._wandb.log({\"trainer/global_step\": self.global_step, **logs})\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin.log_images","title":"log_images","text":"<pre><code>log_images(images: Tensor | list[Tensor], captions: list[str] | None = None, desc: str = 'media')\n</code></pre> <p>Log images to WandB.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>images</code> <p>Image tensor of shape (B, C, H, W) \u2208 [-1, 1] or list[Tensor] of shape (C, H, W) \u2208 [-1, 1]</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor | list[Tensor]</code> </p> <code>captions</code> <p>Optional list of captions for each image</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>@override\n@background\ndef log_images(\n    self,\n    images: Tensor | list[Tensor],\n    captions: list[str] | None = None,\n    desc: str = \"media\",\n):\n    \"\"\"\n    Log images to WandB.\n\n    Args:\n        images (Tensor | list[Tensor]): Image tensor of shape (B, C, H, W) \u2208 [-1, 1] or list[Tensor] of shape (C, H, W) \u2208 [-1, 1]\n        captions (list[str] | None): Optional list of captions for each image\n    \"\"\"\n    if isinstance(images, Tensor):\n        images = list(images.unbind())\n\n    assert all(image.ndim == 3 for image in images), (\n        \"Images must be 4D tensor (B, C, H, W) or list of 3D tensors (C, H, W)\"\n    )\n    assert all(image.shape[0] in [1, 3] for image in images), (\n        \"Images must have 1 or 3 channels\"\n    )\n\n    # Convert to uint8\n    images = [(image * 128 + 128).clip(0, 255).byte() for image in images]\n    images = [image.detach().cpu() for image in images]\n\n    _images = [\n        wandb.Image(image, caption=caption)\n        for image, caption in zip(images, captions or [None] * len(images))\n    ]\n    self._wandb.log({\"trainer/global_step\": self.global_step, desc: _images})\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin.log_videos","title":"log_videos","text":"<pre><code>log_videos(videos: Tensor | list[Tensor], captions: list[str] | None = None, desc: str = 'media')\n</code></pre> <p>Log videos to WandB.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>videos</code> <p>Video tensor of shape (B, C, T, H, W) \u2208 [-1, 1]</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor</code> </p> <code>captions</code> <p>Optional list of captions for each video</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>desc</code> <p>Description/name for the logged videos</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> DEFAULT: <code>'media'</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>@override\n@background\ndef log_videos(\n    self,\n    videos: Tensor | list[Tensor],\n    captions: list[str] | None = None,\n    desc: str = \"media\",\n):\n    \"\"\"\n    Log videos to WandB.\n\n    Args:\n        videos (Tensor): Video tensor of shape (B, C, T, H, W) \u2208 [-1, 1]\n        captions (list[str] | None): Optional list of captions for each video\n        desc (str): Description/name for the logged videos\n    \"\"\"\n    if isinstance(videos, Tensor):\n        videos = list(videos.unbind())\n\n    assert all(video.ndim == 4 for video in videos), (\n        \"Videos must be 4D tensor (B, C, T, H, W) or list of 4D tensors (C, T, H, W)\"\n    )\n    assert all(video.shape[0] in [1, 3] for video in videos), (\n        \"Videos must have 1 or 3 channels\"\n    )\n\n    # Convert to uint8 and (C, T, H, W) -&gt; (T, C, H, W)\n    videos = [(video * 128 + 128).clip(0, 255).byte() for video in videos]\n    videos = [video.permute(1, 0, 2, 3) for video in videos]\n    videos = [video.to(\"cpu\", non_blocking=True) for video in videos]\n\n    _videos = [\n        wandb.Video(video.numpy(), caption=caption, format=\"gif\")\n        for video, caption in zip(videos, captions or [None] * len(videos))\n    ]\n\n    self._wandb.log({\"trainer/global_step\": self.global_step, desc: _videos})\n</code></pre>"},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerMixin.log_config","title":"log_config","text":"<pre><code>log_config(config: dict[str, Any] | Any)\n</code></pre> Source code in <code>src/dream_trainer/trainer/mixins/loggers/wandb.py</code> <pre><code>def log_config(self, config: dict[str, Any] | Any):\n    if not isinstance(config, dict):\n        config = config_to_dict(config)\n\n    self._wandb.config.update(config, allow_val_change=True)\n</code></pre>"},{"location":"api/mixins/loggers/#configuration_1","title":"Configuration","text":""},{"location":"api/mixins/loggers/#dream_trainer.trainer.mixins.WandBLoggerConfigMixin","title":"WandBLoggerConfigMixin  <code>dataclass</code>","text":"<pre><code>WandBLoggerConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters, logging_parameters: WandbLoggingParameters = WandbLoggingParameters())\n</code></pre>"},{"location":"api/mixins/loggers/#usage-examples","title":"Usage Examples","text":""},{"location":"api/mixins/loggers/#basic-logging","title":"Basic Logging","text":"<pre><code>from dream_trainer.trainer.mixins import WandBLoggerMixin, WandBLoggerConfigMixin\nfrom dream_trainer.trainer import BaseTrainer\n\nclass MyTrainer(BaseTrainer, WandBLoggerMixin):\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_loss(batch)\n\n        # Log scalar values\n        self.log_scalar(\"train/loss\", loss)\n        self.log_scalar(\"train/learning_rate\", self.optimizer.param_groups[0][\"lr\"])\n\n        # Log multiple values at once\n        self.log_dict({\n            \"train/batch_loss\": loss,\n            \"train/grad_norm\": grad_norm,\n            \"train/throughput\": samples_per_sec\n        })\n\n        return {\"loss\": loss}\n</code></pre>"},{"location":"api/mixins/loggers/#image-logging","title":"Image Logging","text":"<pre><code>def validation_step(self, batch, batch_idx):\n    images, labels = batch\n    predictions = self.model(images)\n\n    # Log a batch of images with predictions\n    if batch_idx == 0:  # Log first batch only\n        # Create visualization\n        viz_images = self.create_prediction_viz(images, predictions, labels)\n\n        self.log_images(\n            viz_images,  # (B, C, H, W) tensor\n            caption=[f\"Pred: {p}, True: {l}\" for p, l in zip(predictions, labels)],\n            desc=\"val/predictions\"\n        )\n\n    # Log attention maps\n    if hasattr(self.model, \"get_attention_maps\"):\n        attention_maps = self.model.get_attention_maps()\n        self.log_images(\n            attention_maps,\n            desc=\"val/attention_maps\"\n        )\n</code></pre>"},{"location":"api/mixins/loggers/#video-logging","title":"Video Logging","text":"<pre><code>def on_validation_epoch_end(self):\n    # Generate video samples\n    generated_videos = self.model.generate(num_samples=4)  # (B, C, T, H, W)\n\n    self.log_videos(\n        generated_videos,\n        caption=[\"Sample 1\", \"Sample 2\", \"Sample 3\", \"Sample 4\"],\n        desc=\"val/generated_videos\",\n        fps=30\n    )\n</code></pre>"},{"location":"api/mixins/loggers/#histogram-logging","title":"Histogram Logging","text":"<pre><code>def on_train_epoch_end(self):\n    # Log weight distributions\n    for name, param in self.model.named_parameters():\n        if param.requires_grad:\n            self.log_histogram(\n                param.data,\n                desc=f\"weights/{name}\"\n            )\n\n            if param.grad is not None:\n                self.log_histogram(\n                    param.grad,\n                    desc=f\"gradients/{name}\"\n                )\n</code></pre>"},{"location":"api/mixins/loggers/#table-logging","title":"Table Logging","text":"<pre><code>def on_validation_epoch_end(self):\n    # Create a table of predictions\n    columns = [\"image\", \"prediction\", \"ground_truth\", \"confidence\"]\n    data = []\n\n    for batch in self.val_dataloader:\n        images, labels = batch\n        predictions = self.model(images)\n        confidences = predictions.softmax(dim=-1).max(dim=-1).values\n\n        for img, pred, label, conf in zip(images, predictions, labels, confidences):\n            data.append([\n                wandb.Image(img),\n                self.class_names[pred.argmax()],\n                self.class_names[label],\n                conf.item()\n            ])\n\n        if len(data) &gt;= 100:  # Limit table size\n            break\n\n    self.log_table(\n        columns=columns,\n        data=data,\n        desc=\"val/predictions_table\"\n    )\n</code></pre>"},{"location":"api/mixins/loggers/#plot-logging","title":"Plot Logging","text":"<pre><code>def create_loss_curve(self):\n    import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots()\n    ax.plot(self.train_losses, label=\"Train\")\n    ax.plot(self.val_losses, label=\"Validation\")\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Loss\")\n    ax.legend()\n\n    return fig\n\ndef on_train_end(self):\n    # Log matplotlib figure\n    loss_curve = self.create_loss_curve()\n    self.log_plot(loss_curve, desc=\"training/loss_curve\")\n\n    # Log confusion matrix\n    cm_figure = self.plot_confusion_matrix(self.confusion_matrix)\n    self.log_plot(cm_figure, desc=\"val/confusion_matrix\")\n</code></pre>"},{"location":"api/mixins/loggers/#model-tracking","title":"Model Tracking","text":"<p>WandB can track model weights and gradients:</p> <pre><code>class MyTrainer(BaseTrainer, WandBLoggerMixin):\n    def __init__(self, config):\n        super().__init__(config)\n\n        # Watch model for gradient and parameter tracking\n        self.log_model(\n            models=[self.model],  # List of models to track\n            log=\"all\",  # Log gradients and parameters\n            log_freq=100  # Log every 100 batches\n        )\n</code></pre>"},{"location":"api/mixins/loggers/#configuration-logging","title":"Configuration Logging","text":"<p>Log hyperparameters and configuration:</p> <pre><code>def __init__(self, config):\n    super().__init__(config)\n\n    # Log full configuration\n    self.log_config({\n        \"model\": {\n            \"architecture\": \"transformer\",\n            \"num_layers\": self.config.num_layers,\n            \"hidden_dim\": self.config.hidden_dim,\n            \"num_heads\": self.config.num_heads\n        },\n        \"training\": {\n            \"learning_rate\": self.config.learning_rate,\n            \"batch_size\": self.config.batch_size,\n            \"num_epochs\": self.config.num_epochs,\n            \"optimizer\": \"AdamW\",\n            \"scheduler\": \"CosineAnnealing\"\n        },\n        \"data\": {\n            \"dataset\": self.config.dataset_name,\n            \"num_train_samples\": len(self.train_dataset),\n            \"num_val_samples\": len(self.val_dataset)\n        }\n    })\n</code></pre>"},{"location":"api/mixins/loggers/#custom-logger-implementation","title":"Custom Logger Implementation","text":"<p>Create your own logger by extending <code>LoggerMixin</code>:</p> <pre><code>from dream_trainer.trainer.mixins import LoggerMixin, LoggerConfigMixin\nimport tensorboardX\n\nclass TensorBoardLoggerMixin(LoggerMixin):\n    def __init__(self, config):\n        super().__init__(config)\n\n        if self.world.is_global_zero and self.logging_parameters.enabled:\n            self.writer = tensorboardX.SummaryWriter(\n                log_dir=f\"runs/{self.experiment}\"\n            )\n        else:\n            self.writer = None\n\n    def log_dict(self, logs: dict[str, Any]):\n        if self.writer is None:\n            return\n\n        for key, value in logs.items():\n            if isinstance(value, (int, float, torch.Tensor)):\n                self.writer.add_scalar(key, value, self.global_step)\n\n    def log_images(self, images, caption=None, desc=None):\n        if self.writer is None:\n            return\n\n        # Convert to grid\n        grid = torchvision.utils.make_grid(images)\n        self.writer.add_image(desc or \"images\", grid, self.global_step)\n\n    def log_config(self, config):\n        if self.writer is None:\n            return\n\n        # Log as text\n        self.writer.add_text(\"config\", str(config), 0)\n</code></pre>"},{"location":"api/mixins/loggers/#best-practices","title":"Best Practices","text":""},{"location":"api/mixins/loggers/#1-conditional-logging","title":"1. Conditional Logging","text":"<pre><code>def training_step(self, batch, batch_idx):\n    # Only log every N steps to reduce overhead\n    if self.global_step % self.logging_parameters.log_every_n_steps == 0:\n        self.log_scalar(\"train/loss\", loss)\n\n    # Log expensive metrics less frequently\n    if self.global_step % 100 == 0:\n        self.log_histogram(self.model.embeddings.weight, \"embeddings\")\n</code></pre>"},{"location":"api/mixins/loggers/#2-distributed-training","title":"2. Distributed Training","text":"<p>The logger mixins automatically handle distributed training:</p> <pre><code># Only rank 0 logs to avoid duplicates\nif self.world.is_global_zero:\n    # This check is handled automatically by WandBLoggerMixin\n    self.log_scalar(\"metric\", value)\n</code></pre>"},{"location":"api/mixins/loggers/#3-memory-management","title":"3. Memory Management","text":"<pre><code>def log_large_outputs(self, outputs):\n    # Detach and move to CPU to avoid memory issues\n    if isinstance(outputs, torch.Tensor):\n        outputs = outputs.detach().cpu()\n\n    # Sample large outputs\n    if len(outputs) &gt; 100:\n        indices = torch.randperm(len(outputs))[:100]\n        outputs = outputs[indices]\n\n    self.log_images(outputs, desc=\"samples\")\n</code></pre>"},{"location":"api/mixins/loggers/#4-organizing-logs","title":"4. Organizing Logs","text":"<p>Use hierarchical naming for better organization:</p> <pre><code># Use \"/\" to create sections in WandB\nself.log_scalar(\"train/loss\", train_loss)\nself.log_scalar(\"train/accuracy\", train_acc)\nself.log_scalar(\"train/learning_rate\", lr)\n\nself.log_scalar(\"val/loss\", val_loss)\nself.log_scalar(\"val/accuracy\", val_acc)\n\nself.log_scalar(\"model/num_parameters\", num_params)\nself.log_scalar(\"model/grad_norm\", grad_norm)\n</code></pre>"},{"location":"api/mixins/loggers/#integration-with-callbacks","title":"Integration with Callbacks","text":"<p>Logger mixins work well with callbacks:</p> <pre><code>class LoggingCallback(Callback):\n    def on_train_batch_end(self, trainer, batch, batch_idx, outputs):\n        # Access trainer's logging methods\n        if hasattr(trainer, \"log_scalar\"):\n            trainer.log_scalar(\"callback/batch_time\", time.time() - self.start_time)\n\n    def on_validation_epoch_end(self, trainer):\n        # Log complex visualizations\n        if hasattr(trainer, \"log_plot\"):\n            fig = self.create_visualization(trainer)\n            trainer.log_plot(fig, desc=\"callback/visualization\")\n</code></pre>"},{"location":"api/mixins/loggers/#see-also","title":"See Also","text":"<ul> <li>Weights &amp; Biases Documentation</li> <li>BaseTrainer - Core training functionality</li> <li>EvalMetricMixin - Metric computation</li> <li>Callbacks - Extending functionality </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/quantize/","title":"Quantization Mixin","text":"<p>The <code>QuantizeMixin</code> provides infrastructure for model quantization during training, supporting various quantization strategies including FP8 and INT8 quantization. It enables selective quantization of model components through customizable filters.</p>"},{"location":"api/mixins/quantize/#overview","title":"Overview","text":"<p>The <code>QuantizeMixin</code> enables: - Selective module quantization with filters - FP8 training support - INT8 quantization for inference - Custom quantization strategies - Memory-efficient training of large models</p>"},{"location":"api/mixins/quantize/#class-reference","title":"Class Reference","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeMixin","title":"QuantizeMixin","text":"<pre><code>QuantizeMixin(config: AbstractTrainerConfig)\n</code></pre> <p>Mixin that adds quantization capabilities to a trainer.</p> <p>This mixin provides the infrastructure for quantizing models during training, including tracking which models have been quantized and defining module filters for selective quantization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>_quantized_models</code> <p>List of model names that have been quantized</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str]</code> </p> <p>Initialize the quantization mixin.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>config</code> <p>The trainer configuration</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>AbstractTrainerConfig</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    \"\"\"\n    Initialize the quantization mixin.\n\n    Args:\n        config: The trainer configuration\n    \"\"\"\n    self._quantized_models = []\n    super().__init__(config)\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeMixin-functions","title":"Functions","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeMixin.quantized_models","title":"quantized_models","text":"<pre><code>quantized_models() -&gt; list[str]\n</code></pre> <p>Get the list of models that have been quantized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>list[str]</code> <p>List of model names that have been quantized</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def quantized_models(self) -&gt; list[str]:\n    \"\"\"\n    Get the list of models that have been quantized.\n\n    Returns:\n        List of model names that have been quantized\n    \"\"\"\n    return self._quantized_models\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeMixin.quantize_module_filters","title":"quantize_module_filters  <code>abstractmethod</code>","text":"<pre><code>quantize_module_filters() -&gt; dict[str, QuantizeModuleFilter]\n</code></pre> <p>Define module filters for quantization.</p> <p>This method should return a dictionary mapping model names to their corresponding quantization filters. The filters determine which modules within each model should be quantized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, QuantizeModuleFilter]</code> <p>Dictionary mapping model names to QuantizeModuleFilter instances</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>@abstractmethod\ndef quantize_module_filters(self) -&gt; dict[str, QuantizeModuleFilter]:\n    \"\"\"\n    Define module filters for quantization.\n\n    This method should return a dictionary mapping model names to their\n    corresponding quantization filters. The filters determine which modules\n    within each model should be quantized.\n\n    Returns:\n        Dictionary mapping model names to QuantizeModuleFilter instances\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/mixins/quantize/#module-filters","title":"Module Filters","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeModuleFilter","title":"QuantizeModuleFilter","text":"<p>Abstract class for filtering modules during quantization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeModuleFilter-functions","title":"Functions","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeModuleFilter.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(module: Module, name: str) -&gt; bool\n</code></pre> <p>Determines whether a module should be quantized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>module</code> <p>The module to check</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Module</code> </p> <code>name</code> <p>The fully qualified name of the module</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the module should be quantized, False otherwise</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>@abstractmethod\ndef __call__(self, module: nn.Module, name: str) -&gt; bool:\n    \"\"\"\n    Determines whether a module should be quantized.\n\n    Args:\n        module: The module to check\n        name: The fully qualified name of the module\n\n    Returns:\n        True if the module should be quantized, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeModuleFilter.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validates that the filter was applied correctly. Will be called after quantization is complete.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validates that the filter was applied correctly.\n    Will be called after quantization is complete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.QuantizeModuleFilter.__add__","title":"__add__","text":"<pre><code>__add__(other: QuantizeModuleFilter) -&gt; QuantizeModuleFilter\n</code></pre> <p>Returns a new QuantizeModuleFilter that applies both filters sequentially. The module will be quantized only if both filters return True.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __add__(self, other: \"QuantizeModuleFilter\") -&gt; \"QuantizeModuleFilter\":\n    \"\"\"\n    Returns a new QuantizeModuleFilter that applies both filters sequentially.\n    The module will be quantized only if both filters return True.\n    \"\"\"\n\n    class CombinedQuantizeModuleFilter(QuantizeModuleFilter):\n        def __init__(self, filter1, filter2):\n            self.filter1 = filter1\n            self.filter2 = filter2\n\n        def __call__(self, module: nn.Module, name: str) -&gt; bool:\n            return self.filter1(module, name) and self.filter2(module, name)\n\n        def validate(self):\n            self.filter1.validate()\n            self.filter2.validate()\n\n    return CombinedQuantizeModuleFilter(self, other)\n</code></pre>"},{"location":"api/mixins/quantize/#built-in-filters","title":"Built-in Filters","text":"<p>Dream Trainer provides several pre-built quantization filters:</p>"},{"location":"api/mixins/quantize/#excludemodulebyname","title":"ExcludeModuleByName","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeModuleByName","title":"ExcludeModuleByName","text":"<pre><code>ExcludeModuleByName(exclude: list[str])\n</code></pre> <p>Filter that excludes specific modules from quantization by their fully qualified names.</p> <p>This filter ensures that: - Explicitly excluded modules are not quantized - Only nn.Linear modules are considered for quantization - Linear modules have dimensions divisible by 16 (required for float8 tensorcore)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>exclude</code> <p>Set of module names to exclude from quantization</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <p>Initialize the filter with a list of module names to exclude.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>exclude</code> <p>List of fully qualified module names to exclude from quantization</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __init__(self, exclude: list[str]):\n    \"\"\"\n    Initialize the filter with a list of module names to exclude.\n\n    Args:\n        exclude: List of fully qualified module names to exclude from quantization\n    \"\"\"\n    self.exclude = set(exclude)\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeModuleByName-functions","title":"Functions","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeModuleByName.__call__","title":"__call__","text":"<pre><code>__call__(module: Module, name: str) -&gt; bool\n</code></pre> <p>Determines whether a module should be quantized based on its name and type.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>module</code> <p>The module to check</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Module</code> </p> <code>name</code> <p>The fully qualified name of the module</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the module should be quantized, False otherwise</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the module is not nn.Linear or has dimensions not divisible by 16</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __call__(self, module: nn.Module, name: str) -&gt; bool:\n    \"\"\"\n    Determines whether a module should be quantized based on its name and type.\n\n    Args:\n        module: The module to check\n        name: The fully qualified name of the module\n\n    Returns:\n        True if the module should be quantized, False otherwise\n\n    Raises:\n        ValueError: If the module is not nn.Linear or has dimensions not divisible by 16\n    \"\"\"\n    if name in self.exclude:\n        self.exclude.remove(name)\n        return False\n\n    if not isinstance(module, nn.Linear):\n        raise ValueError(f\"Exclusion list contains a non nn.Linear module: {name}\")\n\n    # All dims must be divisible by 16 due to float8 tensorcore hardware requirements.\n    dims_multiples_of_16 = (\n        module.weight.shape[0] % 16 == 0 and module.weight.shape[1] % 16 == 0\n    )\n    if not dims_multiples_of_16:\n        raise ValueError(\n            f\"Linear layer {name} has in_features or out_features not divisible by 16. \"\n            \"Please explicitly exclude this module from FP8 quantization.\"\n        )\n        return False\n\n    return True\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeModuleByName.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validates that all excluded modules were encountered during filtering.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>AssertionError</code> <p>If any excluded modules were not seen during filtering</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validates that all excluded modules were encountered during filtering.\n\n    Raises:\n        AssertionError: If any excluded modules were not seen during filtering\n    \"\"\"\n    assert len(self.exclude) == 0, (\n        f\"Not all excluded modules were seen. Missing: {self.exclude}\"\n    )\n</code></pre>"},{"location":"api/mixins/quantize/#excludesubmodules","title":"ExcludeSubmodules","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeSubmodules","title":"ExcludeSubmodules","text":"<pre><code>ExcludeSubmodules(exclude: list[str])\n</code></pre> <p>Filter that excludes entire submodule trees from quantization based on module path prefixes.</p> <p>This filter allows excluding all modules under a specific path prefix. For example, excluding 'model.encoder' will exclude 'model.encoder.layer1', 'model.encoder.layer2', etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>exclude</code> <p>Set of module path prefixes to exclude</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <code>_seen</code> <p>Set of prefixes that were actually encountered during filtering</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <p>Initialize the filter with a list of module path prefixes to exclude.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>exclude</code> <p>List of module path prefixes. Any module whose name starts with     these prefixes will be excluded from quantization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __init__(self, exclude: list[str]):\n    \"\"\"\n    Initialize the filter with a list of module path prefixes to exclude.\n\n    Args:\n        exclude: List of module path prefixes. Any module whose name starts with\n                these prefixes will be excluded from quantization.\n    \"\"\"\n    self.exclude = set(exclude)\n    self._seen = set()\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeSubmodules-functions","title":"Functions","text":""},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeSubmodules.__call__","title":"__call__","text":"<pre><code>__call__(module: Module, name: str) -&gt; bool\n</code></pre> <p>Determines whether a module should be quantized based on its path prefix.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>module</code> <p>The module to check</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Module</code> </p> <code>name</code> <p>The fully qualified name of the module</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>False if the module name matches or starts with any excluded prefix,</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <code>bool</code> <p>True otherwise</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def __call__(self, module: nn.Module, name: str) -&gt; bool:\n    \"\"\"\n    Determines whether a module should be quantized based on its path prefix.\n\n    Args:\n        module: The module to check\n        name: The fully qualified name of the module\n\n    Returns:\n        False if the module name matches or starts with any excluded prefix,\n        True otherwise\n    \"\"\"\n    for prefix in self.exclude:\n        if name == prefix or name.startswith(prefix + \".\"):\n            self._seen.add(prefix)\n            return False\n    return True\n</code></pre>"},{"location":"api/mixins/quantize/#dream_trainer.trainer.mixins.ExcludeSubmodules.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validates that all excluded prefixes were encountered during filtering.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>AssertionError</code> <p>If any excluded prefixes were not seen during filtering</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/quantize.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validates that all excluded prefixes were encountered during filtering.\n\n    Raises:\n        AssertionError: If any excluded prefixes were not seen during filtering\n    \"\"\"\n    missing = self.exclude - self._seen\n    assert not missing, f\"Not all excluded module prefixes were seen. Missing: {missing}\"\n</code></pre>"},{"location":"api/mixins/quantize/#usage-examples","title":"Usage Examples","text":""},{"location":"api/mixins/quantize/#basic-quantization-setup","title":"Basic Quantization Setup","text":"<pre><code>from dream_trainer.trainer.mixins import QuantizeMixin, ExcludeModuleByName\nimport torch.nn as nn\n\nclass MyTrainer(BaseTrainer, QuantizeMixin):\n    def quantize_module_filters(self):\n        # Exclude specific modules from quantization\n        return {\n            \"model\": ExcludeModuleByName([\n                \"model.embeddings.word_embeddings\",\n                \"model.embeddings.position_embeddings\",\n                \"model.lm_head\"\n            ])\n        }\n\n    def setup(self):\n        super().setup()\n\n        # Apply FP8 quantization\n        from transformer_engine.pytorch import fp8_autocast\n\n        # Models specified in quantize_module_filters will be quantized\n        with fp8_autocast(enabled=True):\n            # Training will use FP8 for specified modules\n            pass\n</code></pre>"},{"location":"api/mixins/quantize/#using-excludesubmodules-filter","title":"Using ExcludeSubmodules Filter","text":"<pre><code>def quantize_module_filters(self):\n    # Exclude entire submodule trees from quantization\n    return {\n        \"model\": ExcludeSubmodules([\n            \"model.embeddings\",  # Excludes all embedding layers\n            \"model.head\",        # Excludes output head\n            \"model.encoder.layer.0\",  # Exclude first encoder layer\n        ])\n    }\n</code></pre>"},{"location":"api/mixins/quantize/#combining-filters","title":"Combining Filters","text":"<p>Filters can be combined using the <code>+</code> operator:</p> <pre><code>def quantize_module_filters(self):\n    # Combine multiple filters - module must pass ALL filters\n    exclude_by_name = ExcludeModuleByName([\n        \"model.embeddings.word_embeddings\",\n        \"model.lm_head\"\n    ])\n    exclude_submodules = ExcludeSubmodules([\n        \"model.encoder.layer.0\",  # Skip first layer\n        \"model.encoder.layer.23\"  # Skip last layer\n    ])\n\n    combined_filter = exclude_by_name + exclude_submodules\n\n    return {\"model\": combined_filter}\n</code></pre>"},{"location":"api/mixins/quantize/#custom-filter-implementation","title":"Custom Filter Implementation","text":"<p>Create custom filters for specific quantization strategies:</p> <pre><code>class QuantizeByNamePatternFilter(QuantizeModuleFilter):\n    \"\"\"Quantize modules whose names match a pattern.\"\"\"\n\n    def __init__(self, patterns: list[str]):\n        self.patterns = patterns\n        self.quantized_modules = []\n\n    def __call__(self, module: nn.Module, name: str) -&gt; bool:\n        # Check if module name matches any pattern\n        should_quantize = any(pattern in name for pattern in self.patterns)\n\n        if should_quantize:\n            self.quantized_modules.append(name)\n\n        return should_quantize\n\n    def validate(self):\n        # Ensure we quantized at least one module\n        if not self.quantized_modules:\n            raise ValueError(f\"No modules matched patterns: {self.patterns}\")\n\n        print(f\"Quantized {len(self.quantized_modules)} modules\")\n\n# Use the custom filter\nclass MyTrainer(BaseTrainer, QuantizeMixin):\n    def quantize_module_filters(self):\n        # Quantize all attention and MLP layers\n        return {\n            \"model\": QuantizeByNamePatternFilter([\n                \"attention\",\n                \"mlp\",\n                \"feed_forward\"\n            ])\n        }\n</code></pre>"},{"location":"api/mixins/quantize/#layer-wise-quantization","title":"Layer-wise Quantization","text":"<p>Apply different quantization strategies to different layers:</p> <pre><code>class LayerWiseQuantizeFilter(QuantizeModuleFilter):\n    \"\"\"Apply different quantization based on layer depth.\"\"\"\n\n    def __init__(self, early_layers: int, late_layers: int):\n        self.early_layers = early_layers\n        self.late_layers = late_layers\n\n    def __call__(self, module: nn.Module, name: str) -&gt; bool:\n        # Extract layer number from name\n        import re\n        match = re.search(r'layer\\.(\\d+)', name)\n\n        if match:\n            layer_num = int(match.group(1))\n\n            # Don't quantize early or late layers\n            if layer_num &lt; self.early_layers:\n                return False\n            if layer_num &gt;= self.total_layers - self.late_layers:\n                return False\n\n        return True\n\nclass MyTrainer(BaseTrainer, QuantizeMixin):\n    def quantize_module_filters(self):\n        # Skip first 2 and last 2 layers\n        return {\n            \"model\": LayerWiseQuantizeFilter(\n                early_layers=2,\n                late_layers=2\n            )\n        }\n</code></pre>"},{"location":"api/mixins/quantize/#fp8-training-example","title":"FP8 Training Example","text":"<p>Complete example using FP8 quantization for efficient training:</p> <pre><code>from dream_trainer.trainer.mixins import (\n    QuantizeMixin, \n    ExcludeModuleByName,\n    ExcludeSubmodules\n)\nimport transformer_engine.pytorch as te\n\nclass FP8Trainer(BaseTrainer, QuantizeMixin):\n    def configure_models(self):\n        # Use TransformerEngine layers for FP8 support\n        self.model = te.TransformerLayer(\n            hidden_size=4096,\n            num_attention_heads=32,\n            intermediate_size=16384,\n            params_dtype=torch.float16,\n            device=\"cuda\"\n        )\n\n    def quantize_module_filters(self):\n        # Exclude embedding and output layers from quantization\n        return {\n            \"model\": ExcludeModuleByName([\n                \"model.word_embeddings\",\n                \"model.output_layer\"\n            ])\n        }\n\n    def training_step(self, batch, batch_idx):\n        # Enable FP8 autocast for forward and backward\n        with te.fp8_autocast(enabled=True):\n            outputs = self.model(batch[\"input\"])\n            loss = self.criterion(outputs, batch[\"target\"])\n\n            # Backward pass also uses FP8\n            self.backward(loss)\n\n        return {\"loss\": loss}\n</code></pre>"},{"location":"api/mixins/quantize/#int8-quantization-for-inference","title":"INT8 Quantization for Inference","text":"<p>Post-training quantization for deployment:</p> <pre><code>class INT8QuantizationFilter(QuantizeModuleFilter):\n    \"\"\"Filter for INT8 quantization.\"\"\"\n\n    def __init__(self, calibration_batches: int = 100):\n        self.calibration_batches = calibration_batches\n        self.modules_to_quantize = []\n\n    def __call__(self, module: nn.Module, name: str) -&gt; bool:\n        # Quantize compute-intensive layers\n        if isinstance(module, (nn.Linear, nn.Conv2d)):\n            if module.weight.numel() &gt; 100_000:  # &gt;100K parameters\n                self.modules_to_quantize.append((name, module))\n                return True\n        return False\n\n    def apply_int8_quantization(self, model):\n        \"\"\"Apply INT8 quantization after training.\"\"\"\n        import torch.quantization as quantization\n\n        # Prepare model for quantization\n        model.qconfig = quantization.get_default_qconfig('fbgemm')\n        quantization.prepare(model, inplace=True)\n\n        # Calibrate with representative data\n        model.eval()\n        with torch.no_grad():\n            for i, batch in enumerate(calibration_dataloader):\n                if i &gt;= self.calibration_batches:\n                    break\n                model(batch)\n\n        # Convert to INT8\n        quantization.convert(model, inplace=True)\n\n        return model\n</code></pre>"},{"location":"api/mixins/quantize/#memory-efficient-training","title":"Memory-Efficient Training","text":"<p>Combine quantization with other memory-saving techniques:</p> <pre><code>class MemoryEfficientTrainer(BaseTrainer, QuantizeMixin, SetupMixin):\n    def quantize_module_filters(self):\n        # Exclude critical layers from quantization\n        return {\n            \"model\": ExcludeSubmodules([\n                \"model.embeddings\",\n                \"model.layer_norm\",\n                \"model.output\"\n            ])\n        }\n\n    def apply_activation_checkpointing(self):\n        # Combine with activation checkpointing\n        from torch.distributed.fsdp.wrap import checkpoint_wrapper\n\n        for name, module in self.model.named_modules():\n            if isinstance(module, TransformerBlock):\n                wrapped = checkpoint_wrapper(module)\n                setattr(self.model, name, wrapped)\n\n    def configure_optimizers(self):\n        # Use memory-efficient optimizer\n        from bitsandbytes.optim import AdamW8bit\n\n        self.optimizer = AdamW8bit(\n            self.model.parameters(),\n            lr=1e-4,\n            weight_decay=0.1\n        )\n</code></pre>"},{"location":"api/mixins/quantize/#best-practices","title":"Best Practices","text":""},{"location":"api/mixins/quantize/#1-gradual-quantization","title":"1. Gradual Quantization","text":"<p>Start with less aggressive quantization and increase over time:</p> <pre><code>class GradualQuantizationFilter(QuantizeModuleFilter):\n    def __init__(self, start_epoch: int, full_epoch: int):\n        self.start_epoch = start_epoch\n        self.full_epoch = full_epoch\n\n    def __call__(self, module: nn.Module, name: str) -&gt; bool:\n        if self.trainer.current_epoch &lt; self.start_epoch:\n            return False\n\n        # Gradually increase quantization\n        progress = (self.trainer.current_epoch - self.start_epoch) / (\n            self.full_epoch - self.start_epoch\n        )\n        progress = min(1.0, max(0.0, progress))\n\n        # Quantize based on module size and progress\n        size_threshold = 1_000_000 * (1 - progress) + 100_000 * progress\n        return module.weight.numel() &gt; size_threshold\n</code></pre>"},{"location":"api/mixins/quantize/#2-validation-without-quantization","title":"2. Validation Without Quantization","text":"<p>Validate model performance without quantization:</p> <pre><code>def validation_step(self, batch, batch_idx):\n    # Temporarily disable quantization for validation\n    with torch.autocast(device_type='cuda', enabled=False):\n        outputs = self.model(batch[\"input\"])\n        loss = self.criterion(outputs, batch[\"target\"])\n\n    return {\"val_loss\": loss}\n</code></pre>"},{"location":"api/mixins/quantize/#3-monitor-quantization-impact","title":"3. Monitor Quantization Impact","text":"<p>Track the effect of quantization on model performance:</p> <pre><code>def on_train_epoch_end(self):\n    # Log quantization statistics\n    total_params = sum(p.numel() for p in self.model.parameters())\n    quantized_params = sum(\n        p.numel() for name, p in self.model.named_parameters()\n        if name in self.quantized_modules()\n    )\n\n    self.log_scalar(\n        \"quantization/param_ratio\",\n        quantized_params / total_params\n    )\n\n    # Compare FP32 vs quantized performance\n    self.evaluate_quantization_impact()\n</code></pre>"},{"location":"api/mixins/quantize/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/mixins/quantize/#common-issues","title":"Common Issues","text":"<ol> <li>Gradient Underflow: Use gradient scaling with FP8/INT8</li> <li>Accuracy Degradation: Start with less aggressive quantization</li> <li>Incompatible Layers: Not all layers support all quantization types</li> <li>Memory Savings: Ensure quantization is applied before materialization</li> </ol>"},{"location":"api/mixins/quantize/#debugging","title":"Debugging","text":"<pre><code>def validate_quantization(self):\n    \"\"\"Ensure quantization is applied correctly.\"\"\"\n    for model_name in self.quantized_models():\n        model = getattr(self, model_name)\n\n        # Check which modules are quantized\n        for name, module in model.named_modules():\n            if hasattr(module, 'weight'):\n                dtype = module.weight.dtype\n                print(f\"{name}: {dtype}\")\n</code></pre>"},{"location":"api/mixins/quantize/#see-also","title":"See Also","text":"<ul> <li>SetupMixin - Model setup and parallelism</li> <li>BaseTrainer - Core training functionality</li> <li>FP8 Training Guide</li> <li>PyTorch Quantization </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/","title":"Setup Mixins","text":"<p>The setup mixins provide a comprehensive framework for configuring models, optimizers, schedulers, and dataloaders in Dream Trainer. They handle the complete lifecycle from configuration to initialization, including parallelism strategies and memory optimization.</p>"},{"location":"api/mixins/setup/#overview","title":"Overview","text":"<p>The setup system is composed of three main mixins that work together:</p> <ol> <li>ModelSetupMixin - Model configuration, parallelism, and initialization</li> <li>OptimizerAndSchedulerSetupMixin - Optimizer and LR scheduler management</li> <li>DataLoaderSetupMixin - Training and validation dataloader setup</li> </ol> <p>These are combined into a unified SetupMixin that orchestrates the complete setup process.</p>"},{"location":"api/mixins/setup/#setupmixin","title":"SetupMixin","text":"<p>The main setup orchestrator that combines all setup functionality:</p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.SetupMixin","title":"SetupMixin","text":"<pre><code>SetupMixin(config: AbstractTrainerConfig)\n</code></pre> <p>Orchestrates the complete setup process for the trainer.</p> <p>This mixin provides a unified interface for setting up all components required for training, including models, optimizers, schedulers, and dataloaders. It ensures these components are initialized in the correct order and with proper dependencies.</p> <p>The setup process is divided into two main phases: 1. Configuration phase (<code>configure()</code>): Initializes model architecture 2. Setup phase (<code>setup()</code>): Applies parallelism, initializes weights, and    prepares optimizers/dataloaders</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Configuration object containing all setup parameters</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>SetupConfigMixin</code> </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>class MyTrainer(SetupMixin, DreamTrainer):     def configure_models(self):         self.model = MyModel(self.config)</p> <pre><code>def configure_optimizers(self):\n    self.optimizer = torch.optim.Adam(\n        self.model.parameters(),\n        lr=self.config.learning_rate\n    )\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.SetupMixin-functions","title":"Functions","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.SetupMixin.configure","title":"configure","text":"<pre><code>configure()\n</code></pre> <p>Execute the configuration phase of setup.</p> <p>This method handles the initial configuration of models before any parallelism or optimization is applied. It performs the following steps:</p> <ol> <li>Configures models on meta device for memory efficiency</li> <li>Calls post-configuration hooks for additional setup</li> </ol> <p>This method should be called before <code>setup()</code> to ensure models are properly configured before being parallelized and materialized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Note</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Models are created on the meta device during this phase, meaning they don't consume actual memory until materialized in the setup phase.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/setup.py</code> <pre><code>def configure(self):\n    \"\"\"Execute the configuration phase of setup.\n\n    This method handles the initial configuration of models before any\n    parallelism or optimization is applied. It performs the following steps:\n\n    1. Configures models on meta device for memory efficiency\n    2. Calls post-configuration hooks for additional setup\n\n    This method should be called before `setup()` to ensure models are\n    properly configured before being parallelized and materialized.\n\n    Note:\n        Models are created on the meta device during this phase, meaning\n        they don't consume actual memory until materialized in the setup phase.\n    \"\"\"\n    self._configure_models()\n    self.post_configure_models()\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.SetupMixin.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Execute the main setup phase for all training components.</p> <p>This method orchestrates the complete setup process in the correct order:</p> <ol> <li>Model setup: Applies parallelism strategies (TP, PP, FSDP), compiles    models, and initializes weights</li> <li>Optimizer and scheduler setup: Creates optimizers and learning rate    schedulers based on the configured models</li> <li>DataLoader setup: Initializes training and validation dataloaders</li> </ol> <p>The order is important as optimizers depend on model parameters, and dataloaders may need information about model parallelism for proper sharding.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Note</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>This method should be called after <code>configure()</code> and before training begins. It handles all device placement and distributed setup.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/setup.py</code> <pre><code>def setup(self):\n    \"\"\"Execute the main setup phase for all training components.\n\n    This method orchestrates the complete setup process in the correct order:\n\n    1. Model setup: Applies parallelism strategies (TP, PP, FSDP), compiles\n       models, and initializes weights\n    2. Optimizer and scheduler setup: Creates optimizers and learning rate\n       schedulers based on the configured models\n    3. DataLoader setup: Initializes training and validation dataloaders\n\n    The order is important as optimizers depend on model parameters, and\n    dataloaders may need information about model parallelism for proper\n    sharding.\n\n    Note:\n        This method should be called after `configure()` and before training\n        begins. It handles all device placement and distributed setup.\n    \"\"\"\n    self._setup_models()\n    self._setup_optimizers_and_schedulers()\n    self._setup_dataloaders()\n</code></pre>"},{"location":"api/mixins/setup/#configuration","title":"Configuration","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.SetupConfigMixin","title":"SetupConfigMixin  <code>dataclass</code>","text":"<pre><code>SetupConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre> <p>Configuration mixin that combines all setup-related configurations.</p> <p>This dataclass serves as a unified configuration container for all aspects of trainer setup, including: - DataLoader configuration (batch sizes, workers, etc.) - Optimizer and scheduler configuration - Model setup configuration</p> <p>By inheriting from all setup configuration mixins, this class provides a single point of configuration for the entire training setup process.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>@dataclass class MyTrainerConfig(SetupConfigMixin):     learning_rate: float = 1e-4     batch_size: int = 32     model_dim: int = 768</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#modelsetupmixin","title":"ModelSetupMixin","text":"<p>Handles model configuration, parallelism strategies, and weight initialization:</p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin","title":"ModelSetupMixin","text":"<pre><code>ModelSetupMixin(config: AbstractTrainerConfig)\n</code></pre> <p>A mixin that handles the complete lifecycle of model configuration and setup.</p> <p>This mixin provides a comprehensive framework for: - Model configuration and initialization - Applying various parallelism strategies (tensor, pipeline, data) - Model compilation and optimization - Activation checkpointing - Weight initialization</p> <p>The mixin enforces a specific order of operations to ensure models are properly configured before parallelism is applied and weights are initialized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Configuration object containing model setup parameters</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>ModelSetupConfigMixin</code> </p> <code>_model_names</code> <p>List of model attribute names registered during configuration</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>ModelSetupConfigMixin</code> </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>class MyTrainer(ModelSetupMixin):     def configure_models(self):         self.model = MyModel(config)</p> <pre><code>def init_weights(self):\n    self.model.apply(init_weights_fn)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin-functions","title":"Functions","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.named_models","title":"named_models","text":"<pre><code>named_models() -&gt; dict[str, nn.Module]\n</code></pre> <p>Return a dictionary mapping model names to their corresponding modules.</p> <p>This method provides access to all models registered during the configure_models phase. Model names are collected automatically when models are assigned as attributes during configuration.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Module]</code> <p>dict[str, nn.Module]: Dictionary where keys are model attribute names and values are the corresponding nn.Module instances.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>trainer.named_models()</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>@override\ndef named_models(self) -&gt; dict[str, nn.Module]:\n    \"\"\"Return a dictionary mapping model names to their corresponding modules.\n\n    This method provides access to all models registered during the configure_models\n    phase. Model names are collected automatically when models are assigned as\n    attributes during configuration.\n\n    Returns:\n        dict[str, nn.Module]: Dictionary where keys are model attribute names\n            and values are the corresponding nn.Module instances.\n\n    Example:\n        &gt;&gt;&gt; trainer.named_models()\n        {'model': TransformerModel(...), 'encoder': Encoder(...)}\n    \"\"\"\n    return {name: getattr(self, name) for name in self._model_names}\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.get_module","title":"get_module","text":"<pre><code>get_module(fqn: str) -&gt; nn.Module\n</code></pre> <p>Retrieve a module or submodule by its fully qualified name.</p> <p>This method allows access to nested modules using dot notation. The first part of the FQN should be a model name registered during configuration, followed by the submodule path.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>fqn</code> <p>Fully qualified name of the module (e.g., \"model.encoder.layer1\")</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The requested module or submodule</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>AttributeError</code> <p>If the model or submodule doesn't exist</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>trainer.get_module(\"model.encoder.attention\") MultiHeadAttention(...)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>@override\ndef get_module(self, fqn: str) -&gt; nn.Module:\n    \"\"\"Retrieve a module or submodule by its fully qualified name.\n\n    This method allows access to nested modules using dot notation. The first\n    part of the FQN should be a model name registered during configuration,\n    followed by the submodule path.\n\n    Args:\n        fqn: Fully qualified name of the module (e.g., \"model.encoder.layer1\")\n\n    Returns:\n        nn.Module: The requested module or submodule\n\n    Raises:\n        AttributeError: If the model or submodule doesn't exist\n\n    Example:\n        &gt;&gt;&gt; trainer.get_module(\"model.encoder.attention\")\n        MultiHeadAttention(...)\n    \"\"\"\n    model, *submodules = fqn.split(\".\")\n    return getattr(self, model).get_submodule(\".\".join(submodules))\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.configure_models","title":"configure_models  <code>abstractmethod</code>","text":"<pre><code>configure_models()\n</code></pre> <p>Configure and instantiate all models used by the trainer.</p> <p>This method must be implemented by subclasses to define and instantiate all models. Models should be assigned as attributes to the trainer instance. This method is called within a meta device context, so models are created on the meta device for efficient memory usage during configuration.</p> <p>The method is called early in the setup process, before any parallelism or optimization is applied.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def configure_models(self):     self.model = TransformerModel(         vocab_size=self.config.vocab_size,         hidden_dim=self.config.hidden_dim     )     self.encoder = Encoder(self.config.encoder_config)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>@abstractmethod\ndef configure_models(self):\n    \"\"\"Configure and instantiate all models used by the trainer.\n\n    This method must be implemented by subclasses to define and instantiate\n    all models. Models should be assigned as attributes to the trainer instance.\n    This method is called within a meta device context, so models are created\n    on the meta device for efficient memory usage during configuration.\n\n    The method is called early in the setup process, before any parallelism\n    or optimization is applied.\n\n    Example:\n        def configure_models(self):\n            self.model = TransformerModel(\n                vocab_size=self.config.vocab_size,\n                hidden_dim=self.config.hidden_dim\n            )\n            self.encoder = Encoder(self.config.encoder_config)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.post_configure_models","title":"post_configure_models","text":"<pre><code>post_configure_models()\n</code></pre> <p>Optional hook called after model configuration.</p> <p>This method is called after configure_models() but before any parallelism or optimization is applied. It can be used for any additional setup that requires the models to be instantiated but doesn't need them to be on the actual device yet.</p> <p>Common use cases: - Setting up model-specific configurations - Registering custom hooks - Performing model structure validation</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def post_configure_models(self):\n    \"\"\"Optional hook called after model configuration.\n\n    This method is called after configure_models() but before any parallelism\n    or optimization is applied. It can be used for any additional setup that\n    requires the models to be instantiated but doesn't need them to be on\n    the actual device yet.\n\n    Common use cases:\n    - Setting up model-specific configurations\n    - Registering custom hooks\n    - Performing model structure validation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.init_weights","title":"init_weights  <code>abstractmethod</code>","text":"<pre><code>init_weights()\n</code></pre> <p>Initialize model weights after parallelism has been applied.</p> <p>This method must be implemented to define how model weights should be initialized. It is called after all parallelism strategies have been applied and the model has been materialized on the actual device.</p> <p>The method is called within a no_grad context, so gradient computation is automatically disabled.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def init_weights(self):     def init_fn(module):         if isinstance(module, nn.Linear):             nn.init.xavier_uniform_(module.weight)             if module.bias is not None:                 nn.init.zeros_(module.bias)</p> <pre><code>self.model.apply(init_fn)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>@abstractmethod\ndef init_weights(self):\n    \"\"\"Initialize model weights after parallelism has been applied.\n\n    This method must be implemented to define how model weights should be\n    initialized. It is called after all parallelism strategies have been\n    applied and the model has been materialized on the actual device.\n\n    The method is called within a no_grad context, so gradient computation\n    is automatically disabled.\n\n    Example:\n        def init_weights(self):\n            def init_fn(module):\n                if isinstance(module, nn.Linear):\n                    nn.init.xavier_uniform_(module.weight)\n                    if module.bias is not None:\n                        nn.init.zeros_(module.bias)\n\n            self.model.apply(init_fn)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_tensor_parallel","title":"apply_tensor_parallel","text":"<pre><code>apply_tensor_parallel(tp_mesh: DeviceMesh)\n</code></pre> <p>Apply tensor parallelism to the trainer's models.</p> <p>This method should implement the logic to parallelize model layers across the tensor parallel dimension. Typically, this involves splitting linear layers, embeddings, and attention heads across devices.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>tp_mesh</code> <p>The device mesh for tensor parallelism, defining how devices are organized for tensor-level parallelism.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>DeviceMesh</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If tensor parallelism is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_tensor_parallel(self, tp_mesh):     from torch.distributed.tensor.parallel import parallelize_module</p> <pre><code>parallelize_module(\n    self.model,\n    tp_mesh,\n    {\"attention\": ColwiseParallel(), \"mlp\": RowwiseParallel()}\n)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_tensor_parallel(self, tp_mesh: DeviceMesh):\n    \"\"\"\n    Apply tensor parallelism to the trainer's models.\n\n    This method should implement the logic to parallelize model layers across\n    the tensor parallel dimension. Typically, this involves splitting linear\n    layers, embeddings, and attention heads across devices.\n\n    Args:\n        tp_mesh: The device mesh for tensor parallelism, defining how devices\n            are organized for tensor-level parallelism.\n\n    Raises:\n        NotImplementedError: If tensor parallelism is requested but not implemented\n\n    Example:\n        def apply_tensor_parallel(self, tp_mesh):\n            from torch.distributed.tensor.parallel import parallelize_module\n\n            parallelize_module(\n                self.model,\n                tp_mesh,\n                {\"attention\": ColwiseParallel(), \"mlp\": RowwiseParallel()}\n            )\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `apply_tensor_parallel` in your trainer or set device_parameters.tensor_parallel_degree=1\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_pipeline_parallel","title":"apply_pipeline_parallel","text":"<pre><code>apply_pipeline_parallel(pp_mesh: DeviceMesh) -&gt; dict[str, tuple[_PipelineSchedule, list[nn.Module], bool, bool]]\n</code></pre> <p>Apply pipeline parallelism to the trainer's models.</p> <p>This method should implement the logic to split models into pipeline stages and distribute them across the pipeline parallel device mesh. Each model can be split into multiple stages that will be executed in a pipelined fashion.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>pp_mesh</code> <p>The device mesh for pipeline parallelism, defining how devices are organized for pipeline stages.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>DeviceMesh</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary mapping model attribute names to their pipeline configuration: - key: The attribute name of the model on the trainer (e.g., \"model\") - value: A tuple containing:     - pipeline_schedule: The schedule defining how pipeline stages execute     - model_parts: List of nn.Module instances, one per pipeline stage     - has_first_stage: True if this rank owns the first pipeline stage     - has_last_stage: True if this rank owns the last pipeline stage</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, tuple[_PipelineSchedule, list[Module], bool, bool]]</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If pipeline parallelism is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_pipeline_parallel(self, pp_mesh):     # Split model into stages     stages = self.model.split_into_stages()     schedule = create_pipeline_schedule(stages, pp_mesh)</p> <pre><code>return {\n    \"model\": (schedule, stages, rank == 0, rank == world_size - 1)\n}\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_pipeline_parallel(\n    self, pp_mesh: DeviceMesh\n) -&gt; dict[str, tuple[_PipelineSchedule, list[nn.Module], bool, bool]]:\n    \"\"\"\n    Apply pipeline parallelism to the trainer's models.\n\n    This method should implement the logic to split models into pipeline stages\n    and distribute them across the pipeline parallel device mesh. Each model\n    can be split into multiple stages that will be executed in a pipelined fashion.\n\n    Args:\n        pp_mesh: The device mesh for pipeline parallelism, defining how devices\n            are organized for pipeline stages.\n\n    Returns:\n        dict: A dictionary mapping model attribute names to their pipeline configuration:\n            - key: The attribute name of the model on the trainer (e.g., \"model\")\n            - value: A tuple containing:\n                - pipeline_schedule: The schedule defining how pipeline stages execute\n                - model_parts: List of nn.Module instances, one per pipeline stage\n                - has_first_stage: True if this rank owns the first pipeline stage\n                - has_last_stage: True if this rank owns the last pipeline stage\n\n    Raises:\n        NotImplementedError: If pipeline parallelism is requested but not implemented\n\n    Example:\n        def apply_pipeline_parallel(self, pp_mesh):\n            # Split model into stages\n            stages = self.model.split_into_stages()\n            schedule = create_pipeline_schedule(stages, pp_mesh)\n\n            return {\n                \"model\": (schedule, stages, rank == 0, rank == world_size - 1)\n            }\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `apply_pipeline_parallel` in your trainer or set device_parameters.pipeline_parallel_degree=1\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_activation_checkpointing","title":"apply_activation_checkpointing","text":"<pre><code>apply_activation_checkpointing() -&gt; None\n</code></pre> <p>Apply activation checkpointing to reduce memory usage.</p> <p>This method should implement activation checkpointing (gradient checkpointing) for the models. This technique trades computation for memory by not storing intermediate activations during the forward pass and recomputing them during the backward pass.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If activation checkpointing is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_activation_checkpointing(self):     from torch.distributed.algorithms._checkpoint import checkpoint_wrapper</p> <pre><code># Wrap specific layers with checkpointing\nfor layer in self.model.transformer_layers:\n    wrapped = checkpoint_wrapper(layer)\n    setattr(self.model, f\"layer_{i}\", wrapped)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_activation_checkpointing(self) -&gt; None:\n    \"\"\"Apply activation checkpointing to reduce memory usage.\n\n    This method should implement activation checkpointing (gradient checkpointing)\n    for the models. This technique trades computation for memory by not storing\n    intermediate activations during the forward pass and recomputing them during\n    the backward pass.\n\n    Raises:\n        NotImplementedError: If activation checkpointing is requested but not implemented\n\n    Example:\n        def apply_activation_checkpointing(self):\n            from torch.distributed.algorithms._checkpoint import checkpoint_wrapper\n\n            # Wrap specific layers with checkpointing\n            for layer in self.model.transformer_layers:\n                wrapped = checkpoint_wrapper(layer)\n                setattr(self.model, f\"layer_{i}\", wrapped)\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `apply_activation_checkpointing` in your trainer or set training_parameters.checkpoint_activations=False\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_compile","title":"apply_compile","text":"<pre><code>apply_compile()\n</code></pre> <p>Compile models for optimized execution.</p> <p>This method should implement model compilation using torch.compile or similar optimization techniques. Compilation can significantly improve training and inference performance by optimizing the computation graph.</p> <p>The method is called after parallelism is applied but before weight initialization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If compilation is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_compile(self):     import torch._dynamo as dynamo</p> <pre><code>self.model = torch.compile(\n    self.model,\n    mode=\"reduce-overhead\",\n    fullgraph=True\n)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_compile(self):\n    \"\"\"Compile models for optimized execution.\n\n    This method should implement model compilation using torch.compile or\n    similar optimization techniques. Compilation can significantly improve\n    training and inference performance by optimizing the computation graph.\n\n    The method is called after parallelism is applied but before weight\n    initialization.\n\n    Raises:\n        NotImplementedError: If compilation is requested but not implemented\n\n    Example:\n        def apply_compile(self):\n            import torch._dynamo as dynamo\n\n            self.model = torch.compile(\n                self.model,\n                mode=\"reduce-overhead\",\n                fullgraph=True\n            )\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement compile_model or set device_parameters.compile_model=False\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_fully_shard","title":"apply_fully_shard","text":"<pre><code>apply_fully_shard(config: dict[str, Any]) -&gt; None\n</code></pre> <p>Apply Fully Sharded Data Parallel (FSDP) to models.</p> <p>This method should implement FSDP wrapping for the models. FSDP shards model parameters, gradients, and optimizer states across data parallel ranks to reduce memory usage and enable training of larger models.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>config</code> <p>FSDP configuration dictionary containing settings like sharding strategy, backward prefetch, forward prefetch, etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If FSDP is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_fully_shard(self, config):     from torch.distributed.fsdp import fully_shard</p> <pre><code>for layer in self.model.layers:\n    fully_shard(layer, **config)\nfully_shard(self.model, **config)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_fully_shard(self, config: dict[str, Any]) -&gt; None:\n    \"\"\"Apply Fully Sharded Data Parallel (FSDP) to models.\n\n    This method should implement FSDP wrapping for the models. FSDP shards\n    model parameters, gradients, and optimizer states across data parallel\n    ranks to reduce memory usage and enable training of larger models.\n\n    Args:\n        config: FSDP configuration dictionary containing settings like\n            sharding strategy, backward prefetch, forward prefetch, etc.\n\n    Raises:\n        NotImplementedError: If FSDP is requested but not implemented\n\n    Example:\n        def apply_fully_shard(self, config):\n            from torch.distributed.fsdp import fully_shard\n\n            for layer in self.model.layers:\n                fully_shard(layer, **config)\n            fully_shard(self.model, **config)\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `apply_fully_shard` or disable all parallelism but dp_replicate\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.apply_replicate","title":"apply_replicate","text":"<pre><code>apply_replicate(dp_replicate_mesh: DeviceMesh)\n</code></pre> <p>Apply traditional data parallel replication (DDP) to models.</p> <p>This method should implement Distributed Data Parallel (DDP) for the models. Unlike FSDP, DDP replicates the entire model on each device and synchronizes gradients during the backward pass.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>dp_replicate_mesh</code> <p>The device mesh for data parallel replication</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>DeviceMesh</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If DDP is requested but not implemented</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def apply_replicate(self, dp_replicate_mesh):     from torch.distributed._composable.replicate import replicate</p> <pre><code>replicate(self.model, device_mesh=dp_replicate_mesh)\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def apply_replicate(self, dp_replicate_mesh: DeviceMesh):\n    \"\"\"Apply traditional data parallel replication (DDP) to models.\n\n    This method should implement Distributed Data Parallel (DDP) for the models.\n    Unlike FSDP, DDP replicates the entire model on each device and synchronizes\n    gradients during the backward pass.\n\n    Args:\n        dp_replicate_mesh: The device mesh for data parallel replication\n\n    Raises:\n        NotImplementedError: If DDP is requested but not implemented\n\n    Example:\n        def apply_replicate(self, dp_replicate_mesh):\n            from torch.distributed._composable.replicate import replicate\n\n            replicate(self.model, device_mesh=dp_replicate_mesh)\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `apply_replicate` or use non-DDP DeviceParameters.\"\n        \"Ex:\\nfrom torch.distributed._composable.replicate import replicate \\nreplicate(self.model, device_mesh=self.world.get_mesh('dp_replicate'))\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.mark_forward_methods","title":"mark_forward_methods","text":"<pre><code>mark_forward_methods() -&gt; list[str]\n</code></pre> <p>Specify additional forward methods to be wrapped with autocast.</p> <p>By default, only the standard 'forward' method of each model is wrapped with autocast for mixed precision training. This method allows you to specify additional methods that should also be wrapped.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: List of method names in dot notation (e.g., [\"model.generate\", \"model.decode\"]). Each string should specify the path from the trainer to the method.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def mark_forward_methods(self):     return [\"model.generate\", \"model.encode\", \"decoder.decode_step\"]</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def mark_forward_methods(self) -&gt; list[str]:\n    \"\"\"Specify additional forward methods to be wrapped with autocast.\n\n    By default, only the standard 'forward' method of each model is wrapped\n    with autocast for mixed precision training. This method allows you to\n    specify additional methods that should also be wrapped.\n\n    Returns:\n        list[str]: List of method names in dot notation (e.g., [\"model.generate\",\n            \"model.decode\"]). Each string should specify the path from the trainer\n            to the method.\n\n    Example:\n        def mark_forward_methods(self):\n            return [\"model.generate\", \"model.encode\", \"decoder.decode_step\"]\n    \"\"\"\n    return []\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupMixin.context_parallel_buffers","title":"context_parallel_buffers","text":"<pre><code>context_parallel_buffers() -&gt; list[torch.Tensor]\n</code></pre> <p>Return buffers that need to be synchronized across context parallel ranks.</p> <p>This method should return a list of buffers (like positional embeddings or frequency tensors) that need to be kept in sync across different context parallel ranks when using context parallelism.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>list[Tensor]</code> <p>list[torch.Tensor]: List of tensor buffers to synchronize</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented and context parallelism is used</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def context_parallel_buffers(self):     return [self.model.freqs_cis, self.model.position_embeddings]</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/models.py</code> <pre><code>def context_parallel_buffers(self) -&gt; list[torch.Tensor]:\n    \"\"\"Return buffers that need to be synchronized across context parallel ranks.\n\n    This method should return a list of buffers (like positional embeddings\n    or frequency tensors) that need to be kept in sync across different\n    context parallel ranks when using context parallelism.\n\n    Returns:\n        list[torch.Tensor]: List of tensor buffers to synchronize\n\n    Raises:\n        NotImplementedError: If not implemented and context parallelism is used\n\n    Example:\n        def context_parallel_buffers(self):\n            return [self.model.freqs_cis, self.model.position_embeddings]\n    \"\"\"\n    raise NotImplementedError(\n        \"Please implement `context_parallel_buffers` in your trainer. Return buffers like freq_cis\"\n    )\n</code></pre>"},{"location":"api/mixins/setup/#configuration_1","title":"Configuration","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.ModelSetupConfigMixin","title":"ModelSetupConfigMixin  <code>dataclass</code>","text":"<pre><code>ModelSetupConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre> <p>Configuration mixin for model setup functionality.</p> <p>This class serves as a base configuration for trainers that need model setup capabilities. It inherits from AbstractTrainerConfig and can be extended with additional configuration parameters specific to model initialization and setup.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#usage-example","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer.mixins import ModelSetupMixin\nimport torch.nn as nn\n\nclass MyTrainer(ModelSetupMixin):\n    def configure_models(self):\n        # Models are automatically tracked when assigned as attributes\n        self.model = nn.TransformerModel(\n            d_model=768,\n            nhead=12,\n            num_layers=12\n        )\n        self.auxiliary_model = nn.Linear(768, 10)\n\n    def init_weights(self):\n        # Initialize weights after parallelism is applied\n        def init_fn(module):\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, std=0.02)\n\n        self.model.apply(init_fn)\n        self.auxiliary_model.apply(init_fn)\n\n    def apply_tensor_parallel(self, tp_mesh):\n        # Apply tensor parallelism to split model across devices\n        from torch.distributed.tensor.parallel import parallelize_module\n\n        plan = {\n            \"attention\": ColwiseParallel(),\n            \"mlp\": RowwiseParallel(),\n            \"embedding\": RowwiseParallel()\n        }\n        parallelize_module(self.model, tp_mesh, plan)\n\n    def apply_activation_checkpointing(self):\n        # Enable gradient checkpointing to save memory\n        from torch.distributed.fsdp.wrap import (\n            enable_checkpointing,\n            checkpoint_wrapper\n        )\n\n        enable_checkpointing(\n            self.model,\n            checkpoint_wrapper,\n            checkpoint_impl=torch.utils.checkpoint.checkpoint\n        )\n</code></pre>"},{"location":"api/mixins/setup/#parallelism-strategies","title":"Parallelism Strategies","text":"<p>The ModelSetupMixin supports multiple parallelism strategies:</p>"},{"location":"api/mixins/setup/#1-tensor-parallelism-tp","title":"1. Tensor Parallelism (TP)","text":"<pre><code>def apply_tensor_parallel(self, tp_mesh):\n    # Split layers across tensor parallel dimension\n    parallelize_module(\n        self.model,\n        tp_mesh,\n        {\"attention.wqkv\": ColwiseParallel(), \"mlp.w1\": RowwiseParallel()}\n    )\n</code></pre>"},{"location":"api/mixins/setup/#2-pipeline-parallelism-pp","title":"2. Pipeline Parallelism (PP)","text":"<pre><code>def apply_pipeline_parallel(self, pp_mesh):\n    # Split model into pipeline stages\n    stages = [\n        self.model.embeddings,\n        self.model.encoder[:6],\n        self.model.encoder[6:],\n        self.model.output_layer\n    ]\n    schedule = PipelineSchedule(stages, pp_mesh)\n    return {\"model\": (schedule, stages, True, True)}\n</code></pre>"},{"location":"api/mixins/setup/#3-fully-sharded-data-parallel-fsdp","title":"3. Fully Sharded Data Parallel (FSDP)","text":"<pre><code>def apply_fully_shard(self, fsdp_config):\n    # Configure which layers to wrap with FSDP\n    from torch.distributed.fsdp import wrap\n\n    # Wrap each transformer layer\n    for layer in self.model.layers:\n        wrap(layer, fsdp_config)\n\n    # Wrap the entire model\n    wrap(self.model, fsdp_config)\n</code></pre>"},{"location":"api/mixins/setup/#4-data-parallel-ddp","title":"4. Data Parallel (DDP)","text":"<pre><code>def apply_replicate(self, dp_mesh):\n    # Apply traditional data parallelism\n    from torch.distributed._composable.replicate import replicate\n    replicate(self.model, device_mesh=dp_mesh)\n</code></pre>"},{"location":"api/mixins/setup/#optimizerandschedulersetupmixin","title":"OptimizerAndSchedulerSetupMixin","text":"<p>Manages optimizer and learning rate scheduler configuration:</p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin","title":"OptimizerAndSchedulerSetupMixin","text":"<pre><code>OptimizerAndSchedulerSetupMixin(config: AbstractTrainerConfig)\n</code></pre> <p>Mixin that handles optimizer and learning rate scheduler configuration and setup.</p> <p>This mixin provides a framework for configuring and managing optimizers and schedulers in a trainer. It automatically tracks optimizer and scheduler instances, manages their relationships, and provides convenient access methods.</p> <p>The mixin enforces a two-phase setup: 1. Configuration: Define optimizers and schedulers 2. Setup: Initialize and link optimizers with their schedulers</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Configuration for optimizers/schedulers</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>OptimizerAndSchedulerSetupConfigMixin</code> </p> <code>_optimizer_names</code> <p>Names of optimizer attributes</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str]</code> </p> <code>_scheduler_names</code> <p>Names of scheduler attributes</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>list[str]</code> </p> <code>_optimizer_scheduler_map</code> <p>Maps optimizer names to their controlling scheduler names</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, str | None]</code> </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin-functions","title":"Functions","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin.named_optimizers","title":"named_optimizers","text":"<pre><code>named_optimizers() -&gt; dict[str, Optimizer]\n</code></pre> <p>Return a dictionary mapping optimizer names to their instances.</p> <p>This method provides access to all optimizers registered during the configure_optimizers phase. Optimizer names are collected automatically when optimizers are assigned as attributes during configuration.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Optimizer]</code> <p>dict[str, Optimizer]: Dictionary where keys are optimizer attribute names and values are the corresponding Optimizer instances.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>trainer.named_optimizers()</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/optimizers.py</code> <pre><code>@override\ndef named_optimizers(self) -&gt; dict[str, Optimizer]:\n    \"\"\"Return a dictionary mapping optimizer names to their instances.\n\n    This method provides access to all optimizers registered during the\n    configure_optimizers phase. Optimizer names are collected automatically\n    when optimizers are assigned as attributes during configuration.\n\n    Returns:\n        dict[str, Optimizer]: Dictionary where keys are optimizer attribute names\n            and values are the corresponding Optimizer instances.\n\n    Example:\n        &gt;&gt;&gt; trainer.named_optimizers()\n        {'optimizer': Adam(...), 'discriminator_opt': SGD(...)}\n    \"\"\"\n    return {name: getattr(self, name) for name in self._optimizer_names}\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin.named_schedulers","title":"named_schedulers","text":"<pre><code>named_schedulers() -&gt; dict[str, LRScheduler]\n</code></pre> <p>Return a dictionary mapping scheduler names to their instances.</p> <p>This method provides access to all schedulers registered during the configure_schedulers phase. Scheduler names are collected automatically when schedulers are assigned as attributes during configuration.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, LRScheduler]</code> <p>dict[str, LRScheduler]: Dictionary where keys are scheduler attribute names and values are the corresponding LRScheduler instances.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>trainer.named_schedulers()</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/optimizers.py</code> <pre><code>@override\ndef named_schedulers(self) -&gt; dict[str, LRScheduler]:\n    \"\"\"Return a dictionary mapping scheduler names to their instances.\n\n    This method provides access to all schedulers registered during the\n    configure_schedulers phase. Scheduler names are collected automatically\n    when schedulers are assigned as attributes during configuration.\n\n    Returns:\n        dict[str, LRScheduler]: Dictionary where keys are scheduler attribute names\n            and values are the corresponding LRScheduler instances.\n\n    Example:\n        &gt;&gt;&gt; trainer.named_schedulers()\n        {'scheduler': CosineAnnealingLR(...), 'warmup': LinearWarmup(...)}\n    \"\"\"\n    return {name: getattr(self, name) for name in self._scheduler_names}\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin.configure_optimizers","title":"configure_optimizers  <code>abstractmethod</code>","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure and instantiate all optimizers used by the trainer.</p> <p>This method must be implemented by subclasses to define and instantiate all optimizers. Optimizers should be assigned as attributes to the trainer instance. The method is called after models have been set up, so model parameters are available.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def configure_optimizers(self):     self.optimizer = torch.optim.AdamW(         self.model.parameters(),         lr=self.config.learning_rate,         weight_decay=self.config.weight_decay     )     self.discriminator_opt = torch.optim.SGD(         self.discriminator.parameters(),         lr=0.01     )</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/optimizers.py</code> <pre><code>@abstractmethod\ndef configure_optimizers(self):\n    \"\"\"Configure and instantiate all optimizers used by the trainer.\n\n    This method must be implemented by subclasses to define and instantiate\n    all optimizers. Optimizers should be assigned as attributes to the trainer\n    instance. The method is called after models have been set up, so model\n    parameters are available.\n\n    Example:\n        def configure_optimizers(self):\n            self.optimizer = torch.optim.AdamW(\n                self.model.parameters(),\n                lr=self.config.learning_rate,\n                weight_decay=self.config.weight_decay\n            )\n            self.discriminator_opt = torch.optim.SGD(\n                self.discriminator.parameters(),\n                lr=0.01\n            )\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupMixin.configure_schedulers","title":"configure_schedulers","text":"<pre><code>configure_schedulers()\n</code></pre> <p>Configure and instantiate learning rate schedulers.</p> <p>This optional method can be overridden to define learning rate schedulers for the optimizers. Schedulers should be assigned as attributes and must be initialized with their corresponding optimizer instances.</p> <p>The method is called after configure_optimizers(), so optimizer instances are available via self.optimizer_name or self.get_optimizer().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def configure_schedulers(self):     self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(         self.optimizer,         T_max=self.config.max_steps     )     self.warmup = WarmupScheduler(         self.optimizer,         warmup_steps=self.config.warmup_steps     )</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/optimizers.py</code> <pre><code>def configure_schedulers(self):\n    \"\"\"Configure and instantiate learning rate schedulers.\n\n    This optional method can be overridden to define learning rate schedulers\n    for the optimizers. Schedulers should be assigned as attributes and must\n    be initialized with their corresponding optimizer instances.\n\n    The method is called after configure_optimizers(), so optimizer instances\n    are available via self.optimizer_name or self.get_optimizer().\n\n    Example:\n        def configure_schedulers(self):\n            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                self.optimizer,\n                T_max=self.config.max_steps\n            )\n            self.warmup = WarmupScheduler(\n                self.optimizer,\n                warmup_steps=self.config.warmup_steps\n            )\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#configuration_2","title":"Configuration","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.OptimizerAndSchedulerSetupConfigMixin","title":"OptimizerAndSchedulerSetupConfigMixin  <code>dataclass</code>","text":"<pre><code>OptimizerAndSchedulerSetupConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre> <p>Configuration mixin for optimizer and scheduler setup.</p> <p>This class serves as a base configuration for trainers that need optimizer and learning rate scheduler setup capabilities. It inherits from AbstractTrainerConfig and can be extended with optimizer-specific parameters like learning rates, weight decay, scheduler configurations, etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>@dataclass class MyTrainerConfig(OptimizerAndSchedulerSetupConfigMixin):     learning_rate: float = 1e-4     weight_decay: float = 0.1     warmup_steps: int = 1000</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#usage-example_1","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer.mixins import OptimizerAndSchedulerSetupMixin\nimport torch.optim as optim\n\nclass MyTrainer(OptimizerAndSchedulerSetupMixin):\n    def configure_optimizers(self):\n        # Optimizers are automatically tracked when assigned as attributes\n        self.optimizer = optim.AdamW(\n            self.model.parameters(),\n            lr=1e-4,\n            weight_decay=0.1,\n            betas=(0.9, 0.95)\n        )\n\n        # Multiple optimizers for different models\n        self.discriminator_opt = optim.SGD(\n            self.discriminator.parameters(),\n            lr=1e-3,\n            momentum=0.9\n        )\n\n    def configure_schedulers(self):\n        # Schedulers are automatically tracked and linked to optimizers\n        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=self.config.num_epochs,\n            eta_min=1e-6\n        )\n\n        # Warmup scheduler\n        self.warmup_scheduler = optim.lr_scheduler.LinearLR(\n            self.optimizer,\n            start_factor=0.1,\n            total_iters=self.config.warmup_steps\n        )\n</code></pre>"},{"location":"api/mixins/setup/#advanced-optimizer-configuration","title":"Advanced Optimizer Configuration","text":""},{"location":"api/mixins/setup/#parameter-groups","title":"Parameter Groups","text":"<pre><code>def configure_optimizers(self):\n    # Different learning rates for different parts\n    param_groups = [\n        {\"params\": self.model.embeddings.parameters(), \"lr\": 1e-5},\n        {\"params\": self.model.encoder.parameters(), \"lr\": 1e-4},\n        {\"params\": self.model.output_layer.parameters(), \"lr\": 2e-4}\n    ]\n\n    self.optimizer = optim.AdamW(\n        param_groups,\n        weight_decay=0.01,\n        betas=(0.9, 0.999)\n    )\n</code></pre>"},{"location":"api/mixins/setup/#multiple-optimizers","title":"Multiple Optimizers","text":"<pre><code>def configure_optimizers(self):\n    # Separate optimizers for generator and discriminator\n    self.gen_optimizer = optim.Adam(\n        self.generator.parameters(),\n        lr=2e-4,\n        betas=(0.5, 0.999)\n    )\n\n    self.disc_optimizer = optim.Adam(\n        self.discriminator.parameters(),\n        lr=2e-4,\n        betas=(0.5, 0.999)\n    )\n</code></pre>"},{"location":"api/mixins/setup/#dataloadersetupmixin","title":"DataLoaderSetupMixin","text":"<p>Handles training and validation dataloader configuration:</p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin","title":"DataLoaderSetupMixin","text":"<pre><code>DataLoaderSetupMixin(config: AbstractTrainerConfig)\n</code></pre> <p>Mixin that handles dataloader configuration and setup for training.</p> <p>This mixin provides a framework for configuring training and validation dataloaders in a trainer. It manages dataloader lifecycle and provides convenient property access to the configured dataloaders.</p> <p>The mixin expects subclasses to implement configure_dataloaders() to define how dataloaders should be created based on the configuration.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Configuration for dataloaders</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>DataLoaderSetupConfigMixin</code> </p> <code>_train_dataloader</code> <p>Internal training dataloader instance</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <code>_val_dataloader</code> <p>Internal validation dataloader instance</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>class MyTrainer(DataLoaderSetupMixin):     def configure_dataloaders(self):         train_dataset = MyDataset(self.config.train_path)         val_dataset = MyDataset(self.config.val_path)</p> <pre><code>    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=False\n    )\n\n    return train_loader, val_loader\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin-attributes","title":"Attributes","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin.train_dataloader","title":"train_dataloader  <code>property</code>","text":"<pre><code>train_dataloader: Iterable\n</code></pre> <p>Access the training dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>Iterable</code> <p>The configured training dataloader instance. This is typically a torch.utils.data.DataLoader but can be any iterable that yields batches of training data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <p>Note</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>The dataloader is created during the setup phase by calling configure_dataloaders(). Accessing this property before setup will raise an AttributeError.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin.val_dataloader","title":"val_dataloader  <code>property</code>","text":"<pre><code>val_dataloader: Iterable\n</code></pre> <p>Access the validation dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>Iterable</code> <p>The configured validation dataloader instance. This is typically a torch.utils.data.DataLoader but can be any iterable that yields batches of validation data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <p>Note</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>The dataloader is created during the setup phase by calling configure_dataloaders(). Accessing this property before setup will raise an AttributeError.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin-functions","title":"Functions","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupMixin.configure_dataloaders","title":"configure_dataloaders  <code>abstractmethod</code>","text":"<pre><code>configure_dataloaders() -&gt; tuple[Iterable, Iterable]\n</code></pre> <p>Configure and create training and validation dataloaders.</p> <p>This method must be implemented by subclasses to define how dataloaders are created. It should instantiate and return both training and validation dataloaders based on the trainer's configuration.</p> <p>The method is called during the setup phase after models and optimizers have been configured, allowing dataloaders to be aware of model parallelism settings if needed.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>tuple[Iterable, Iterable]</code> <p>tuple[Iterable, Iterable]: A tuple containing (train_dataloader, val_dataloader). Both should be iterables that yield batches of data. Typically these are torch.utils.data.DataLoader instances.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>def configure_dataloaders(self):     # Create datasets     train_dataset = TextDataset(         self.config.train_path,         tokenizer=self.tokenizer,         max_length=self.config.max_seq_len     )     val_dataset = TextDataset(         self.config.val_path,         tokenizer=self.tokenizer,         max_length=self.config.max_seq_len     )</p> <pre><code># Create dataloaders with distributed sampling if needed\ntrain_sampler = DistributedSampler(\n    train_dataset,\n    num_replicas=self.world.size,\n    rank=self.world.rank\n) if self.world.size &gt; 1 else None\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=self.config.batch_size,\n    sampler=train_sampler,\n    shuffle=(train_sampler is None),\n    num_workers=self.config.num_workers,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=self.config.batch_size,\n    shuffle=False,\n    num_workers=self.config.num_workers,\n    pin_memory=True\n)\n\nreturn train_loader, val_loader\n</code></pre> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/mixins/setup/dataloader.py</code> <pre><code>@abstractmethod\ndef configure_dataloaders(self) -&gt; tuple[Iterable, Iterable]:\n    \"\"\"Configure and create training and validation dataloaders.\n\n    This method must be implemented by subclasses to define how dataloaders\n    are created. It should instantiate and return both training and validation\n    dataloaders based on the trainer's configuration.\n\n    The method is called during the setup phase after models and optimizers\n    have been configured, allowing dataloaders to be aware of model parallelism\n    settings if needed.\n\n    Returns:\n        tuple[Iterable, Iterable]: A tuple containing (train_dataloader, val_dataloader).\n            Both should be iterables that yield batches of data. Typically these are\n            torch.utils.data.DataLoader instances.\n\n    Example:\n        def configure_dataloaders(self):\n            # Create datasets\n            train_dataset = TextDataset(\n                self.config.train_path,\n                tokenizer=self.tokenizer,\n                max_length=self.config.max_seq_len\n            )\n            val_dataset = TextDataset(\n                self.config.val_path,\n                tokenizer=self.tokenizer,\n                max_length=self.config.max_seq_len\n            )\n\n            # Create dataloaders with distributed sampling if needed\n            train_sampler = DistributedSampler(\n                train_dataset,\n                num_replicas=self.world.size,\n                rank=self.world.rank\n            ) if self.world.size &gt; 1 else None\n\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=self.config.batch_size,\n                sampler=train_sampler,\n                shuffle=(train_sampler is None),\n                num_workers=self.config.num_workers,\n                pin_memory=True\n            )\n\n            val_loader = DataLoader(\n                val_dataset,\n                batch_size=self.config.batch_size,\n                shuffle=False,\n                num_workers=self.config.num_workers,\n                pin_memory=True\n            )\n\n            return train_loader, val_loader\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mixins/setup/#configuration_3","title":"Configuration","text":""},{"location":"api/mixins/setup/#dream_trainer.trainer.mixins.DataLoaderSetupConfigMixin","title":"DataLoaderSetupConfigMixin  <code>dataclass</code>","text":"<pre><code>DataLoaderSetupConfigMixin(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre> <p>Configuration mixin for dataloader setup functionality.</p> <p>This class serves as a base configuration for trainers that need dataloader setup capabilities. It inherits from AbstractTrainerConfig and can be extended with dataloader-specific parameters like batch sizes, number of workers, prefetch settings, and dataset configurations.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>@dataclass class MyTrainerConfig(DataLoaderSetupConfigMixin):     batch_size: int = 32     num_workers: int = 4     prefetch_factor: int = 2     train_dataset_path: str = \"data/train\"     val_dataset_path: str = \"data/val\"</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/mixins/setup/#usage-example_2","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer.mixins import DataLoaderSetupMixin\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nclass MyTrainer(DataLoaderSetupMixin):\n    def configure_dataloaders(self):\n        # Create datasets\n        train_dataset = MyDataset(\n            self.config.train_path,\n            transform=self.train_transform\n        )\n        val_dataset = MyDataset(\n            self.config.val_path,\n            transform=self.val_transform\n        )\n\n        # Create distributed samplers if needed\n        train_sampler = None\n        if self.world.size &gt; 1:\n            train_sampler = DistributedSampler(\n                train_dataset,\n                num_replicas=self.world.size,\n                rank=self.world.rank,\n                shuffle=True\n            )\n\n        # Create dataloaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            sampler=train_sampler,\n            shuffle=(train_sampler is None),\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.val_batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n        return train_loader, val_loader\n</code></pre>"},{"location":"api/mixins/setup/#advanced-dataloader-features","title":"Advanced DataLoader Features","text":""},{"location":"api/mixins/setup/#custom-collate-functions","title":"Custom Collate Functions","text":"<pre><code>def configure_dataloaders(self):\n    def collate_fn(batch):\n        # Custom batching logic\n        texts = [item[\"text\"] for item in batch]\n        labels = torch.stack([item[\"label\"] for item in batch])\n\n        # Tokenize and pad\n        encoded = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoded[\"input_ids\"],\n            \"attention_mask\": encoded[\"attention_mask\"],\n            \"labels\": labels\n        }\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=self.config.batch_size,\n        collate_fn=collate_fn,\n        num_workers=self.config.num_workers\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/mixins/setup/#streaming-datasets","title":"Streaming Datasets","text":"<pre><code>def configure_dataloaders(self):\n    # For large datasets that don't fit in memory\n    train_dataset = StreamingDataset(\n        self.config.train_urls,\n        batch_size=self.config.batch_size\n    )\n\n    # No need for DistributedSampler with streaming\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=None,  # Dataset returns batches\n        num_workers=1\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/mixins/setup/#setup-order-and-dependencies","title":"Setup Order and Dependencies","text":"<p>The setup process follows a specific order to ensure dependencies are satisfied:</p> <pre><code>graph TD\n    A[configure] --&gt; B[configure_models]\n    B --&gt; C[post_configure_models]\n    C --&gt; D[setup]\n    D --&gt; E[Apply Parallelism]\n    E --&gt; F[Materialize Models]\n    F --&gt; G[init_weights]\n    G --&gt; H[configure_optimizers]\n    H --&gt; I[configure_schedulers]\n    I --&gt; J[configure_dataloaders]</code></pre>"},{"location":"api/mixins/setup/#best-practices","title":"Best Practices","text":""},{"location":"api/mixins/setup/#1-model-configuration","title":"1. Model Configuration","text":"<ul> <li>Create models on meta device (automatic in configure_models)</li> <li>Apply parallelism before weight initialization</li> <li>Use post_configure_models for model-specific setup</li> </ul>"},{"location":"api/mixins/setup/#2-optimizer-configuration","title":"2. Optimizer Configuration","text":"<ul> <li>Configure optimizers after models are set up</li> <li>Use parameter groups for different learning rates</li> <li>Link schedulers to their optimizers properly</li> </ul>"},{"location":"api/mixins/setup/#3-dataloader-configuration","title":"3. DataLoader Configuration","text":"<ul> <li>Use DistributedSampler for multi-GPU training</li> <li>Enable pin_memory for GPU training</li> <li>Use persistent_workers to reduce overhead</li> </ul>"},{"location":"api/mixins/setup/#4-memory-optimization","title":"4. Memory Optimization","text":"<ul> <li>Apply activation checkpointing for large models</li> <li>Use gradient accumulation to simulate larger batches</li> <li>Enable CPU offloading for very large models</li> </ul>"},{"location":"api/mixins/setup/#see-also","title":"See Also","text":"<ul> <li>AbstractTrainer - Base trainer interface</li> <li>DreamTrainer - Complete example using all setup mixins</li> <li>Device Configuration - Parallelism configuration</li> <li>Training Configuration - Training hyperparameters </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/trainers/abstract/","title":"AbstractTrainer","text":"<p>The <code>AbstractTrainer</code> class is the foundation of the Dream Trainer framework. It defines the interface that all trainer implementations must follow.</p>"},{"location":"api/trainers/abstract/#overview","title":"Overview","text":"<p><code>AbstractTrainer</code> provides: - Core training loop abstraction - Distributed training world management - Model, optimizer, and scheduler management - State serialization interface - Training lifecycle hooks</p>"},{"location":"api/trainers/abstract/#class-reference","title":"Class Reference","text":""},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer","title":"AbstractTrainer","text":"<pre><code>AbstractTrainer(config: AbstractTrainerConfig)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all trainer implementations.</p> <p>This class defines the interface and common functionality that all trainers must implement. It encapsulates the core components needed for training including distributed world management, state tracking, and abstract methods for model, optimizer, and dataloader access.</p> <p>The trainer maintains global training state and provides utility methods for managing models, optimizers, and schedulers by name.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Configuration object containing training parameters.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>AbstractTrainerConfig</code> </p> <code>world</code> <p>DistributedWorld object managing distributed and parallel training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <code>global_step</code> <p>Number of optimizer steps taken across all epochs.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <code>local_batches</code> <p>Number of batches processed since program start.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> <code>current_epoch</code> <p>Current epoch number (0-indexed).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def __init__(self, config: AbstractTrainerConfig):\n    self.config = config\n\n    self.seed = config.seed or random.randint(0, 1000)\n\n    self.project = config.project\n    self.group = config.group\n    self.experiment = config.experiment\n\n    self.device_parameters = config.device_parameters\n\n    self.world = DistributedWorld(config.device_parameters)\n\n    # Trainer State:  NOTE: Keep track of these yourself\n    self.global_step = 0  # Number of optimizer steps taken\n    self.local_batches = 0  # Number of batches processed since program start\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer-attributes","title":"Attributes","text":""},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.train_dataloader","title":"train_dataloader  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>train_dataloader: Iterable\n</code></pre> <p>Return the training dataloader.</p> <p>This property should provide access to the dataloader used for training. The dataloader should yield batches of data that will be passed to the training_step method.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>Iterable</code> <p>An iterable that yields training batches. Typically a PyTorch DataLoader, but can be any iterable that produces batches of training data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.val_dataloader","title":"val_dataloader  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>val_dataloader: Iterable\n</code></pre> <p>Return the validation dataloader.</p> <p>This property should provide access to the dataloader used for validation. The dataloader should yield batches of data that will be passed to the validation_step method.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>Iterable</code> <p>An iterable that yields validation batches. Typically a PyTorch DataLoader, but can be any iterable that produces batches of validation data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer-functions","title":"Functions","text":""},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.named_models","title":"named_models  <code>abstractmethod</code>","text":"<pre><code>named_models() -&gt; dict[str, nn.Module]\n</code></pre> <p>Return a dictionary mapping model names to their corresponding modules.</p> <p>This method should return all models used in training, organized by unique string identifiers. These names are used throughout the training process for logging, checkpointing, and model-specific operations.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Module]</code> <p>dict[str, nn.Module]: Dictionary mapping model names to PyTorch modules. For example: {\"encoder\": encoder_model, \"decoder\": decoder_model}</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef named_models(self) -&gt; dict[str, \"nn.Module\"]:\n    \"\"\"\n    Return a dictionary mapping model names to their corresponding modules.\n\n    This method should return all models used in training, organized by\n    unique string identifiers. These names are used throughout the training\n    process for logging, checkpointing, and model-specific operations.\n\n    Returns:\n        dict[str, nn.Module]: Dictionary mapping model names to PyTorch modules.\n            For example: {\"encoder\": encoder_model, \"decoder\": decoder_model}\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.named_optimizers","title":"named_optimizers  <code>abstractmethod</code>","text":"<pre><code>named_optimizers() -&gt; dict[str, Optimizer]\n</code></pre> <p>Return a dictionary mapping optimizer names to their corresponding optimizers.</p> <p>This method should return all optimizers used in training, organized by unique string identifiers. Each optimizer should correspond to one or more models returned by named_models().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Optimizer]</code> <p>dict[str, Optimizer]: Dictionary mapping optimizer names to PyTorch optimizers. For example: {\"adam\": adam_optimizer, \"sgd\": sgd_optimizer}</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef named_optimizers(self) -&gt; dict[str, \"Optimizer\"]:\n    \"\"\"\n    Return a dictionary mapping optimizer names to their corresponding optimizers.\n\n    This method should return all optimizers used in training, organized by\n    unique string identifiers. Each optimizer should correspond to one or more\n    models returned by named_models().\n\n    Returns:\n        dict[str, Optimizer]: Dictionary mapping optimizer names to PyTorch optimizers.\n            For example: {\"adam\": adam_optimizer, \"sgd\": sgd_optimizer}\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.named_schedulers","title":"named_schedulers  <code>abstractmethod</code>","text":"<pre><code>named_schedulers() -&gt; dict[str, LRScheduler] | None\n</code></pre> <p>Return a dictionary mapping scheduler names to their corresponding schedulers.</p> <p>This method should return all learning rate schedulers used in training, organized by unique string identifiers. Each scheduler should be associated with an optimizer from named_optimizers(). Return None if no schedulers are used.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, LRScheduler] | None</code> <p>dict[str, LRScheduler] | None: Dictionary mapping scheduler names to PyTorch schedulers, or None if no schedulers are used. For example: {\"cosine\": cosine_scheduler, \"linear\": linear_scheduler}</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef named_schedulers(self) -&gt; dict[str, \"LRScheduler\"] | None:\n    \"\"\"\n    Return a dictionary mapping scheduler names to their corresponding schedulers.\n\n    This method should return all learning rate schedulers used in training,\n    organized by unique string identifiers. Each scheduler should be associated\n    with an optimizer from named_optimizers(). Return None if no schedulers\n    are used.\n\n    Returns:\n        dict[str, LRScheduler] | None: Dictionary mapping scheduler names to\n            PyTorch schedulers, or None if no schedulers are used.\n            For example: {\"cosine\": cosine_scheduler, \"linear\": linear_scheduler}\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.get_module","title":"get_module  <code>abstractmethod</code>","text":"<pre><code>get_module(fqn: str) -&gt; nn.Module\n</code></pre> <p>Retrieve a module by its fully qualified name (FQN).</p> <p>This method provides access to nested modules within the trainer's models using dot-separated paths. It's primarily used for accessing specific layers or components for callbacks, analysis, or targeted operations.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>fqn</code> <p>Fully qualified name of the module, using dot notation. For example: \"encoder.layer1.conv1\" or \"decoder.attention\"</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The requested module.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>AttributeError</code> <p>If the module with the given FQN doesn't exist.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef get_module(self, fqn: str) -&gt; \"nn.Module\":\n    \"\"\"\n    Retrieve a module by its fully qualified name (FQN).\n\n    This method provides access to nested modules within the trainer's models\n    using dot-separated paths. It's primarily used for accessing specific\n    layers or components for callbacks, analysis, or targeted operations.\n\n    Args:\n        fqn: Fully qualified name of the module, using dot notation.\n            For example: \"encoder.layer1.conv1\" or \"decoder.attention\"\n\n    Returns:\n        nn.Module: The requested module.\n\n    Raises:\n        AttributeError: If the module with the given FQN doesn't exist.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return the complete state dictionary of the trainer.</p> <p>This method should capture all necessary state for resuming training, including model parameters, optimizer states, scheduler states, trainer metadata, and any other stateful components.</p> <p>The returned dictionary should be serializable and contain all information needed to exactly resume training from the current point.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: Complete state dictionary containing all trainer state. Typically includes keys like \"models\", \"optimizers\", \"schedulers\", \"trainer\", and \"dataloaders\".</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the complete state dictionary of the trainer.\n\n    This method should capture all necessary state for resuming training,\n    including model parameters, optimizer states, scheduler states, trainer\n    metadata, and any other stateful components.\n\n    The returned dictionary should be serializable and contain all information\n    needed to exactly resume training from the current point.\n\n    Returns:\n        dict[str, Any]: Complete state dictionary containing all trainer state.\n            Typically includes keys like \"models\", \"optimizers\", \"schedulers\",\n            \"trainer\", and \"dataloaders\".\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load a complete state dictionary into the trainer.</p> <p>This method should restore all trainer state from a checkpoint, including model parameters, optimizer states, scheduler states, trainer metadata, and any other stateful components.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>state_dict</code> <p>Complete state dictionary to load, typically obtained from a previous call to state_dict(). Should contain all necessary components to resume training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Load a complete state dictionary into the trainer.\n\n    This method should restore all trainer state from a checkpoint, including\n    model parameters, optimizer states, scheduler states, trainer metadata,\n    and any other stateful components.\n\n    Args:\n        state_dict: Complete state dictionary to load, typically obtained\n            from a previous call to state_dict(). Should contain all\n            necessary components to resume training.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit()\n</code></pre> <p>Execute the complete training pipeline.</p> <p>This is the main entry point for training. It should handle the entire training lifecycle including initialization, training loops, validation, checkpointing, and cleanup.</p> <p>Implementations should ensure proper setup and teardown of distributed training resources.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef fit(self):\n    \"\"\"\n    Execute the complete training pipeline.\n\n    This is the main entry point for training. It should handle the entire\n    training lifecycle including initialization, training loops, validation,\n    checkpointing, and cleanup.\n\n    Implementations should ensure proper setup and teardown of distributed\n    training resources.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.setup","title":"setup  <code>abstractmethod</code>","text":"<pre><code>setup()\n</code></pre> <p>Set up the trainer components after configuration.</p> <p>This method is called after configure() and should perform any setup that requires the trainer to be fully configured. This includes initializing models, optimizers, dataloaders, and any other components that depend on the configuration.</p> <p>This is where heavy initialization should occur, such as loading pretrained weights, setting up distributed training, or preparing datasets.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef setup(self):\n    \"\"\"\n    Set up the trainer components after configuration.\n\n    This method is called after configure() and should perform any setup\n    that requires the trainer to be fully configured. This includes\n    initializing models, optimizers, dataloaders, and any other components\n    that depend on the configuration.\n\n    This is where heavy initialization should occur, such as loading\n    pretrained weights, setting up distributed training, or preparing datasets.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.configure","title":"configure  <code>abstractmethod</code>","text":"<pre><code>configure()\n</code></pre> <p>Configure the trainer components.</p> <p>This method is called early in the training pipeline and should set up the basic configuration for training. This includes defining models, optimizers, schedulers, and other training components.</p> <p>This method should be lightweight and primarily focused on defining the training components rather than initializing them.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>@abstractmethod\ndef configure(self):\n    \"\"\"\n    Configure the trainer components.\n\n    This method is called early in the training pipeline and should set up\n    the basic configuration for training. This includes defining models,\n    optimizers, schedulers, and other training components.\n\n    This method should be lightweight and primarily focused on defining\n    the training components rather than initializing them.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.get_name_by_model","title":"get_name_by_model","text":"<pre><code>get_name_by_model(model: Module) -&gt; str\n</code></pre> <p>Get the name associated with a given model instance.</p> <p>This utility method performs a reverse lookup to find the name key for a given model object in the named_models() dictionary.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>model</code> <p>The model instance to look up.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The name associated with the model.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model is not found in named_models().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def get_name_by_model(self, model: \"nn.Module\") -&gt; str:\n    \"\"\"\n    Get the name associated with a given model instance.\n\n    This utility method performs a reverse lookup to find the name key\n    for a given model object in the named_models() dictionary.\n\n    Args:\n        model: The model instance to look up.\n\n    Returns:\n        str: The name associated with the model.\n\n    Raises:\n        ValueError: If the model is not found in named_models().\n    \"\"\"\n    name = next((name for name, m in self.named_models().items() if m is model), None)\n    if name is None:\n        raise ValueError(f\"Model {model} not found in {self.named_models()}\")\n    return name\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.get_name_by_optimizer","title":"get_name_by_optimizer","text":"<pre><code>get_name_by_optimizer(optimizer: Optimizer) -&gt; str\n</code></pre> <p>Get the name associated with a given optimizer instance.</p> <p>This utility method performs a reverse lookup to find the name key for a given optimizer object in the named_optimizers() dictionary.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The optimizer instance to look up.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Optimizer</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The name associated with the optimizer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the optimizer is not found in named_optimizers().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def get_name_by_optimizer(self, optimizer: \"Optimizer\") -&gt; str:\n    \"\"\"\n    Get the name associated with a given optimizer instance.\n\n    This utility method performs a reverse lookup to find the name key\n    for a given optimizer object in the named_optimizers() dictionary.\n\n    Args:\n        optimizer: The optimizer instance to look up.\n\n    Returns:\n        str: The name associated with the optimizer.\n\n    Raises:\n        ValueError: If the optimizer is not found in named_optimizers().\n    \"\"\"\n    name = next(\n        (name for name, o in self.named_optimizers().items() if o is optimizer),\n        None,\n    )\n    if name is None:\n        raise ValueError(f\"Optimizer {optimizer} not found in {self.named_optimizers()}\")\n    return name\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.get_name_by_scheduler","title":"get_name_by_scheduler","text":"<pre><code>get_name_by_scheduler(scheduler: LRScheduler) -&gt; str\n</code></pre> <p>Get the name associated with a given scheduler instance.</p> <p>This utility method performs a reverse lookup to find the name key for a given scheduler object in the named_schedulers() dictionary.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>scheduler</code> <p>The scheduler instance to look up.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>LRScheduler</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The name associated with the scheduler.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the scheduler is not found in named_schedulers() or if named_schedulers() returns None.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def get_name_by_scheduler(self, scheduler: \"LRScheduler\") -&gt; str:\n    \"\"\"\n    Get the name associated with a given scheduler instance.\n\n    This utility method performs a reverse lookup to find the name key\n    for a given scheduler object in the named_schedulers() dictionary.\n\n    Args:\n        scheduler: The scheduler instance to look up.\n\n    Returns:\n        str: The name associated with the scheduler.\n\n    Raises:\n        ValueError: If the scheduler is not found in named_schedulers() or\n            if named_schedulers() returns None.\n    \"\"\"\n    name = next(\n        (name for name, s in (self.named_schedulers() or {}).items() if s is scheduler),\n        None,\n    )\n    if name is None:\n        raise ValueError(f\"Scheduler {scheduler} not found in {self.named_schedulers()}\")\n    return name\n</code></pre>"},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainer.get_scheduler_from_optimizer","title":"get_scheduler_from_optimizer","text":"<pre><code>get_scheduler_from_optimizer(optimizer: Optimizer) -&gt; LRScheduler | None\n</code></pre> <p>Find the scheduler associated with a given optimizer.</p> <p>This utility method searches through all schedulers to find one that is configured to work with the specified optimizer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The optimizer to find the associated scheduler for.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Optimizer</code> </p> RETURNS DESCRIPTION <code>LRScheduler | None</code> <p>LRScheduler | None: The scheduler associated with the optimizer, or None if no scheduler is found or if named_schedulers() returns None.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/abstract.py</code> <pre><code>def get_scheduler_from_optimizer(self, optimizer: \"Optimizer\") -&gt; \"LRScheduler | None\":\n    \"\"\"\n    Find the scheduler associated with a given optimizer.\n\n    This utility method searches through all schedulers to find one that\n    is configured to work with the specified optimizer.\n\n    Args:\n        optimizer: The optimizer to find the associated scheduler for.\n\n    Returns:\n        LRScheduler | None: The scheduler associated with the optimizer,\n            or None if no scheduler is found or if named_schedulers() returns None.\n    \"\"\"\n    for scheduler in (self.named_schedulers() or {}).values():\n        if scheduler.optimizer is optimizer:\n            return scheduler\n    return None\n</code></pre>"},{"location":"api/trainers/abstract/#configuration","title":"Configuration","text":""},{"location":"api/trainers/abstract/#dream_trainer.trainer.AbstractTrainerConfig","title":"AbstractTrainerConfig  <code>dataclass</code>","text":"<pre><code>AbstractTrainerConfig(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters)\n</code></pre> <p>Abstract configuration class for trainers.</p> <p>This dataclass defines the base configuration parameters that all trainer implementations must provide. It serves as the foundation for specific trainer configurations.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>seed</code> <p>Random seed for reproducibility. If None, a random seed between 0 and 1000 will be generated.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int | None</code> </p> <code>project</code> <p>Name of the project for experiment tracking and organization.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> <code>group</code> <p>Group name for categorizing related experiments.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> <code>experiment</code> <p>Unique name for this specific experiment run.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>str</code> </p> <code>device_parameters</code> <p>Configuration for device and distributed training setup.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>DeviceParameters</code> </p>"},{"location":"api/trainers/abstract/#usage-example","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer import AbstractTrainer\nfrom dream_trainer.configs import DeviceParameters\n\nclass MyTrainer(AbstractTrainer):\n    def configure(self):\n        # Define models, optimizers, etc.\n        self.model = torch.nn.Linear(10, 1)\n        self.optimizer = torch.optim.Adam(self.model.parameters())\n\n    def setup(self):\n        # Initialize components\n        self.model = self.model.to(self.world.device)\n\n    def named_models(self):\n        return {\"main\": self.model}\n\n    def named_optimizers(self):\n        return {\"adam\": self.optimizer}\n\n    def named_schedulers(self):\n        return None  # No schedulers in this example\n\n    def get_module(self, fqn: str):\n        # Implement module lookup\n        parts = fqn.split(\".\")\n        module = self.named_models()[parts[0]]\n        for part in parts[1:]:\n            module = getattr(module, part)\n        return module\n\n    @property\n    def train_dataloader(self):\n        return self._train_dataloader\n\n    @property\n    def val_dataloader(self):\n        return self._val_dataloader\n\n    def state_dict(self):\n        return {\n            \"models\": {name: model.state_dict() for name, model in self.named_models().items()},\n            \"optimizers\": {name: opt.state_dict() for name, opt in self.named_optimizers().items()},\n            \"trainer\": {\"global_step\": self.global_step, \"current_epoch\": self.current_epoch}\n        }\n\n    def load_state_dict(self, state_dict):\n        for name, model in self.named_models().items():\n            model.load_state_dict(state_dict[\"models\"][name])\n        for name, opt in self.named_optimizers().items():\n            opt.load_state_dict(state_dict[\"optimizers\"][name])\n        self.global_step = state_dict[\"trainer\"][\"global_step\"]\n        self.current_epoch = state_dict[\"trainer\"][\"current_epoch\"]\n\n    def fit(self):\n        # Implement training loop\n        pass\n</code></pre>"},{"location":"api/trainers/abstract/#key-concepts","title":"Key Concepts","text":""},{"location":"api/trainers/abstract/#world-management","title":"World Management","text":"<p>The <code>world</code> attribute provides access to distributed training utilities:</p> <pre><code># Access device\ndevice = trainer.world.device\n\n# Check if distributed\nif trainer.world.size &gt; 1:\n    print(f\"Running on {trainer.world.size} processes\")\n\n# Get rank\nrank = trainer.world.rank\n</code></pre>"},{"location":"api/trainers/abstract/#named-components","title":"Named Components","text":"<p>The trainer uses a naming system for all components:</p> <pre><code># Access models by name\nmodels = trainer.named_models()  # {\"encoder\": encoder, \"decoder\": decoder}\n\n# Access optimizers by name\noptimizers = trainer.named_optimizers()  # {\"adam\": adam_opt}\n\n# Access schedulers by name\nschedulers = trainer.named_schedulers()  # {\"cosine\": cosine_scheduler}\n</code></pre>"},{"location":"api/trainers/abstract/#state-management","title":"State Management","text":"<p>The trainer tracks training progress:</p> <pre><code># Global step (optimizer steps)\nprint(f\"Step: {trainer.global_step}\")\n\n# Current epoch\nprint(f\"Epoch: {trainer.current_epoch}\")\n\n# Local batches (since program start)\nprint(f\"Batches: {trainer.local_batches}\")\n</code></pre>"},{"location":"api/trainers/abstract/#see-also","title":"See Also","text":"<ul> <li>BaseTrainer - Default implementation with common functionality</li> <li>DreamTrainer - Production-ready trainer with all features</li> <li>World Management - Distributed training utilities </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/trainers/base/","title":"BaseTrainer","text":"<p>The <code>BaseTrainer</code> class provides a complete implementation of the training loop with support for gradient accumulation, validation, callbacks, and distributed training.</p>"},{"location":"api/trainers/base/#overview","title":"Overview","text":"<p><code>BaseTrainer</code> extends <code>AbstractTrainer</code> with: - Complete training and validation loops - Gradient accumulation and clipping - Callback integration - Mixed precision training support - Distributed training synchronization - Learning rate scheduling</p>"},{"location":"api/trainers/base/#class-reference","title":"Class Reference","text":""},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer","title":"BaseTrainer","text":"<pre><code>BaseTrainer(config: BaseTrainerConfig, *args, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractTrainer</code></p> <p>An implementation of a basic training loop, taking into account gradient accumulation, validation, callbacks, and contains bindings for backwards calls and optimizer steps.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Initialize the BaseTrainer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing training parameters and callbacks.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>BaseTrainerConfig</code> </p> <code>*args</code> <p>Additional positional arguments passed to parent class.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional keyword arguments passed to parent class.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def __init__(self, config: BaseTrainerConfig, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the BaseTrainer.\n\n    Args:\n        config: Configuration object containing training parameters and callbacks.\n        *args: Additional positional arguments passed to parent class.\n        **kwargs: Additional keyword arguments passed to parent class.\n    \"\"\"\n    super().__init__(config, *args, **kwargs)\n\n    self.training_parameters = config.training_parameters\n\n    if config.callbacks is None:\n        from dream_trainer.callbacks import CallbackCollection\n\n        config.callbacks = CallbackCollection()\n\n    self.callbacks = config.callbacks\n    self.callbacks.initialize(self)\n\n    self.training = False\n    self._local_step = 0\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer-attributes","title":"Attributes","text":""},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.is_accumulating_gradients","title":"is_accumulating_gradients  <code>property</code>","text":"<pre><code>is_accumulating_gradients: bool\n</code></pre> <p>Check if currently accumulating gradients.</p> <p>Returns True if the current step is a gradient accumulation step (i.e., gradients are being accumulated but not yet applied). Returns False if this is the step where accumulated gradients will be applied, or if we're on the last training batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>bool</code> <p>True if accumulating gradients, False if applying them.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer-functions","title":"Functions","text":""},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return the complete state dictionary of the trainer.</p> <p>This method captures the entire training state including: - Trainer metadata (global step, current epoch, callbacks state) - All model states - All optimizer states - All scheduler states - Dataloader states (if stateful)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the complete trainer state that can be used to resume training from a checkpoint.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the complete state dictionary of the trainer.\n\n    This method captures the entire training state including:\n    - Trainer metadata (global step, current epoch, callbacks state)\n    - All model states\n    - All optimizer states\n    - All scheduler states\n    - Dataloader states (if stateful)\n\n    Returns:\n        dict[str, Any]: A dictionary containing the complete trainer state\n            that can be used to resume training from a checkpoint.\n    \"\"\"\n    return {\n        \"trainer\": {\n            \"global_step\": self.global_step,\n            \"current_epoch\": self.current_epoch,\n            \"callbacks\": self.callbacks.state_dict(),\n        },\n        \"models\": {name: model.state_dict() for name, model in self.named_models().items()},\n        \"optimizers\": {\n            name: optimizer.state_dict()\n            for name, optimizer in self.named_optimizers().items()\n        },\n        \"schedulers\": {\n            name: scheduler.state_dict()\n            for name, scheduler in (self.named_schedulers() or {}).items()\n        },\n        \"dataloaders\": {\n            \"train\": getattr(self.train_dataloader, \"state_dict\", lambda: {})(),\n            \"val\": getattr(self.val_dataloader, \"state_dict\", lambda: {})(),\n        },\n    }\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: dict[str, Any], strict: bool = True) -&gt; None\n</code></pre> <p>Load a complete state dictionary into the trainer.</p> <p>This method restores the entire training state from a checkpoint, including trainer metadata, model states, optimizer states, scheduler states, and dataloader states.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>state_dict</code> <p>Dictionary containing the complete trainer state, typically obtained from a previous call to state_dict().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>strict</code> <p>If True, raises ValueError when state_dict contains keys that don't match the current trainer setup. If False, logs warnings for mismatched keys instead.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If strict=True and state_dict contains unexpected keys.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any], strict: bool = True) -&gt; None:\n    \"\"\"\n    Load a complete state dictionary into the trainer.\n\n    This method restores the entire training state from a checkpoint,\n    including trainer metadata, model states, optimizer states, scheduler states,\n    and dataloader states.\n\n    Args:\n        state_dict: Dictionary containing the complete trainer state,\n            typically obtained from a previous call to state_dict().\n        strict: If True, raises ValueError when state_dict contains keys\n            that don't match the current trainer setup. If False, logs\n            warnings for mismatched keys instead.\n\n    Raises:\n        ValueError: If strict=True and state_dict contains unexpected keys.\n    \"\"\"\n    # Load Trainer State\n    trainer_state = state_dict.pop(\"trainer\")\n    self.global_step = trainer_state.pop(\"global_step\")\n    self.current_epoch = trainer_state.pop(\"current_epoch\")\n    self.callbacks.load_state_dict(trainer_state.pop(\"callbacks\"), self)\n\n    # Load Model State\n    for name, model in self.named_models().items():\n        model.load_state_dict(state_dict.pop(\"models\")[name], strict=strict)\n\n    # Load Optimizer State\n    for name, optimizer in self.named_optimizers().items():\n        optimizer.load_state_dict(state_dict.pop(\"optimizers\")[name])\n\n    # Load Scheduler State\n    for name, scheduler in (self.named_schedulers() or {}).items():\n        scheduler.load_state_dict(state_dict.pop(\"schedulers\")[name])\n\n    # Load Dataloader State\n    dataloader_state = state_dict.pop(\"dataloaders\")\n    getattr(self.train_dataloader, \"load_state_dict\", lambda _: None)(\n        dataloader_state[\"train\"]\n    )\n    getattr(self.val_dataloader, \"load_state_dict\", lambda _: None)(dataloader_state[\"val\"])\n\n    if state_dict:\n        if strict:\n            raise ValueError(f\"Missing keys in state_dict: {state_dict.keys()}\")\n        else:\n            logger.warning(f\"Missing keys in state_dict: {state_dict.keys()}\")\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.fit","title":"fit","text":"<pre><code>fit()\n</code></pre> <p>Execute the complete training pipeline.</p> <p>This is the main entry point for training. It handles the entire training lifecycle including setup, training loops, validation, and cleanup.</p> <p>The method ensures proper cleanup by destroying the distributed process group in the finally block, even if training is interrupted.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@override\ndef fit(self):\n    \"\"\"\n    Execute the complete training pipeline.\n\n    This is the main entry point for training. It handles the entire training\n    lifecycle including setup, training loops, validation, and cleanup.\n\n    The method ensures proper cleanup by destroying the distributed process\n    group in the finally block, even if training is interrupted.\n    \"\"\"\n    try:\n        self._fit()\n    finally:\n        # TODO: close the checkpointer\n\n        if torch.distributed.is_initialized():\n            torch.distributed.destroy_process_group()\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.training_step","title":"training_step  <code>abstractmethod</code>","text":"<pre><code>training_step(batch: dict[str, Any], batch_idx: int) -&gt; dict[str, Any]\n</code></pre> <p>Execute a single training step.</p> <p>This method should implement the forward pass, loss computation, and backward pass for a single batch of training data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>batch</code> <p>Dictionary containing the batch data, typically with keys like 'input', 'target', etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>batch_idx</code> <p>Index of the current batch within the epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing at minimum the computed loss and any other metrics or values to log.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@abstractmethod\ndef training_step(self, batch: dict[str, Any], batch_idx: int) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute a single training step.\n\n    This method should implement the forward pass, loss computation,\n    and backward pass for a single batch of training data.\n\n    Args:\n        batch: Dictionary containing the batch data, typically with keys\n            like 'input', 'target', etc.\n        batch_idx: Index of the current batch within the epoch.\n\n    Returns:\n        dict[str, Any]: Dictionary containing at minimum the computed loss\n            and any other metrics or values to log.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.validation_step","title":"validation_step  <code>abstractmethod</code>","text":"<pre><code>validation_step(batch: dict[str, Any], batch_idx: int) -&gt; dict[str, Any]\n</code></pre> <p>Execute a single validation step.</p> <p>This method should implement the forward pass and metric computation for a single batch of validation data. No gradients should be computed.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>batch</code> <p>Dictionary containing the batch data, typically with keys like 'input', 'target', etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>batch_idx</code> <p>Index of the current batch within the validation epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing validation metrics and any other values to log.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@abstractmethod\ndef validation_step(self, batch: dict[str, Any], batch_idx: int) -&gt; dict[str, Any]:\n    \"\"\"\n    Execute a single validation step.\n\n    This method should implement the forward pass and metric computation\n    for a single batch of validation data. No gradients should be computed.\n\n    Args:\n        batch: Dictionary containing the batch data, typically with keys\n            like 'input', 'target', etc.\n        batch_idx: Index of the current batch within the validation epoch.\n\n    Returns:\n        dict[str, Any]: Dictionary containing validation metrics and any\n            other values to log.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.eval","title":"eval","text":"<pre><code>eval()\n</code></pre> <p>Set the trainer and all models to evaluation mode.</p> <p>This method: - Sets the trainer's training flag to False - Calls eval() on all registered models</p> <p>This should be called before validation or inference to disable dropout, batch normalization updates, and other training-specific behaviors.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Set the trainer and all models to evaluation mode.\n\n    This method:\n    - Sets the trainer's training flag to False\n    - Calls eval() on all registered models\n\n    This should be called before validation or inference to disable\n    dropout, batch normalization updates, and other training-specific\n    behaviors.\n    \"\"\"\n    self.training = False\n    for model in self.named_models().values():\n        model.eval()\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.train","title":"train","text":"<pre><code>train()\n</code></pre> <p>Set the trainer and models to training mode.</p> <p>This method: - Sets the trainer's training flag to True - Calls train() on all registered models that have trainable parameters</p> <p>Models without any parameters requiring gradients are kept in eval mode to avoid unnecessary computation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def train(self):\n    \"\"\"\n    Set the trainer and models to training mode.\n\n    This method:\n    - Sets the trainer's training flag to True\n    - Calls train() on all registered models that have trainable parameters\n\n    Models without any parameters requiring gradients are kept in eval mode\n    to avoid unnecessary computation.\n    \"\"\"\n    self.training = True\n    for model in self.named_models().values():\n        if any(p.requires_grad for p in model.parameters()):\n            model.train()\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.step","title":"step","text":"<pre><code>step(model: Module, optimizer: Optimizer) -&gt; torch.Tensor\n</code></pre> <p>Performs a single optimization step for the given model and optimizer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>This method</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <ul> <li>Computes the total gradient norm for all parameters with gradients.</li> <li>Clips gradients to the configured norm.</li> <li>Calls pre- and post-optimizer step callbacks.</li> <li>Performs the optimizer step.</li> <li>Calls pre- and post-optimizer zero_grad callbacks.</li> <li>Zeros the gradients.</li> <li>Steps the learning rate scheduler if one is associated with the optimizer.</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>The step is performed with autocast disabled to ensure numerical stability.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>model</code> <p>The model whose parameters are being optimized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer used to update the model parameters.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Optimizer</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor: The total norm of the gradients before clipping.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def step(self, model: nn.Module, optimizer: Optimizer) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single optimization step for the given model and optimizer.\n\n    This method:\n        - Computes the total gradient norm for all parameters with gradients.\n        - Clips gradients to the configured norm.\n        - Calls pre- and post-optimizer step callbacks.\n        - Performs the optimizer step.\n        - Calls pre- and post-optimizer zero_grad callbacks.\n        - Zeros the gradients.\n        - Steps the learning rate scheduler if one is associated with the optimizer.\n\n    The step is performed with autocast disabled to ensure numerical stability.\n\n    Args:\n        model (nn.Module): The model whose parameters are being optimized.\n        optimizer (Optimizer): The optimizer used to update the model parameters.\n\n    Returns:\n        torch.Tensor: The total norm of the gradients before clipping.\n    \"\"\"\n    parameters = [p for p in model.parameters() if p.grad is not None]\n    total_norm = self.total_gradient_norm(parameters, p=2, foreach=True)\n    self.clip_gradient_norm(parameters, total_norm, foreach=True)\n\n    self.callbacks.pre_optimizer_step(self, model, optimizer)\n    optimizer.step()\n    self.callbacks.post_optimizer_step(self, model, optimizer)\n\n    self.callbacks.pre_optimizer_zero_grad(self, model, optimizer)\n    optimizer.zero_grad()\n    self.callbacks.post_optimizer_zero_grad(self, model, optimizer)\n\n    if (scheduler := self.get_scheduler_from_optimizer(optimizer)) is not None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            scheduler.step()\n\n    return total_norm\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.loss_parallel","title":"loss_parallel","text":"<pre><code>loss_parallel()\n</code></pre> <p>Context manager for loss parallelism.</p> <p>This provides a context where loss computation can be parallelized with tensor parallelism on dim=-1.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> YIELDS DESCRIPTION <code>None</code> <p>Context manager for loss parallel computation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>with self.loss_parallel():     loss = self.compute_loss(outputs, targets)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@contextlib.contextmanager\ndef loss_parallel(self):\n    \"\"\"\n    Context manager for loss parallelism.\n\n    This provides a context where loss computation can be parallelized\n    with tensor parallelism on dim=-1.\n\n    Yields:\n        None: Context manager for loss parallel computation.\n\n    Example:\n        with self.loss_parallel():\n            loss = self.compute_loss(outputs, targets)\n    \"\"\"\n    with self.world.loss_parallel():\n        yield\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.backward","title":"backward","text":"<pre><code>backward(loss: Tensor)\n</code></pre> <p>Backward pass for loss, with gradient accumulation scaling and autocast disabled.</p> <p>This function is intended to be called inside a training step that is already wrapped in autocast (mixed precision). We explicitly disable autocast here to avoid calling backward in an autocast context, which can cause issues.</p> <p>The loss is divided by the number of gradient accumulation steps to ensure correct gradient scaling when using gradient accumulation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>loss</code> <p>The computed loss tensor to backpropagate.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def backward(self, loss: torch.Tensor):\n    \"\"\"\n    Backward pass for loss, with gradient accumulation scaling and autocast disabled.\n\n    This function is intended to be called inside a training step that is already\n    wrapped in autocast (mixed precision). We explicitly disable autocast here to\n    avoid calling backward in an autocast context, which can cause issues.\n\n    The loss is divided by the number of gradient accumulation steps to ensure\n    correct gradient scaling when using gradient accumulation.\n\n    Args:\n        loss (torch.Tensor): The computed loss tensor to backpropagate.\n    \"\"\"\n    (loss / self._num_gradient_accumulation_steps).backward()\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.no_gradient_sync","title":"no_gradient_sync","text":"<pre><code>no_gradient_sync(*models: Module)\n</code></pre> <p>Disable gradient sync during accumulation steps and mark the final backward for FSDP.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Usage</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>with self.no_gradient_sync(self.model):     loss.backward()</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@contextlib.contextmanager\ndef no_gradient_sync(self, *models: nn.Module):\n    \"\"\"\n    Disable gradient sync during accumulation steps\n    and mark the final backward for FSDP.\n\n    Usage:\n        with self.no_gradient_sync(self.model):\n            loss.backward()\n    \"\"\"\n    if self.world.world_size == 1 or self._num_gradient_accumulation_steps == 1:\n        # If no gradient accumulation or in single process environment, don't sync gradients\n        yield\n        return\n\n    assert all(isinstance(model, (FSDPModule, DDPModule)) for model in models), (\n        f\"Expected all modules to be FSDPModule or DDPModule, got {[type(model).__name__ for model in models]}\"\n    )\n    distributed_modules = cast(tuple[FSDPModule | DDPModule, ...], models)\n\n    current_accumulation_step = (\n        self.local_batches + 1\n    ) % self._num_gradient_accumulation_steps\n\n    # Only update flags when transitioning between states\n    is_first_accumulation_step = (\n        current_accumulation_step == 1 and not self._is_last_training_batch\n    )\n    is_last_accumulation_step = (\n        current_accumulation_step == 0 or self._is_last_training_batch\n    )\n\n    if is_first_accumulation_step:\n        # Set requires_gradient_sync to False only on first accumulation step (unless last batch)\n        for model in distributed_modules:\n            model.set_requires_gradient_sync(False)\n\n    # Set is_last_backward to True on second-to-last step OR if it's the last training batch\n    if is_last_accumulation_step:\n        for model in distributed_modules:\n            model.set_requires_gradient_sync(True)\n            if isinstance(model, FSDPModule):\n                model.set_is_last_backward(True)\n\n    yield\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.total_gradient_norm","title":"total_gradient_norm","text":"<pre><code>total_gradient_norm(parameters: Iterable[Tensor], p=2, error_if_nonfinite=False, foreach: bool | None = None)\n</code></pre> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@torch.no_grad()\ndef total_gradient_norm(\n    self,\n    parameters: Iterable[torch.Tensor],\n    p=2,\n    error_if_nonfinite=False,\n    foreach: bool | None = None,\n):\n    grads = [param for param in parameters if param.grad is not None]\n    return self.world.get_total_norm(\n        parameters=grads,\n        norm_type=p,\n        error_if_nonfinite=error_if_nonfinite,\n        foreach=foreach,\n    )\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.clip_gradient_norm","title":"clip_gradient_norm","text":"<pre><code>clip_gradient_norm(parameters: Iterable[Tensor], total_norm: Tensor, foreach: bool | None = None)\n</code></pre> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@torch.no_grad()\ndef clip_gradient_norm(\n    self,\n    parameters: Iterable[torch.Tensor],\n    total_norm: torch.Tensor,\n    foreach: bool | None = None,\n):\n    if self.training_parameters.gradient_clip_val is None:\n        return\n\n    torch.nn.utils.clip_grads_with_norm_(\n        parameters=parameters,\n        max_norm=self.training_parameters.gradient_clip_val,\n        total_norm=total_norm,\n        foreach=foreach,\n    )\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.train_context","title":"train_context","text":"<pre><code>train_context()\n</code></pre> <p>Create a stacked context manager for training.</p> <p>This method combines the world's training context with any additional contexts provided by callbacks into a single stacked context manager.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <p>contextlib.ExitStack: A stacked context manager that applies all training-related contexts.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def train_context(self):\n    \"\"\"\n    Create a stacked context manager for training.\n\n    This method combines the world's training context with any additional\n    contexts provided by callbacks into a single stacked context manager.\n\n    Returns:\n        contextlib.ExitStack: A stacked context manager that applies all\n            training-related contexts.\n    \"\"\"\n    return stacked_context(\n        [self.world.train_context()] + self.callbacks.train_context(self)\n    )\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.perform_training_epoch","title":"perform_training_epoch","text":"<pre><code>perform_training_epoch()\n</code></pre> <p>Execute a complete training epoch.</p> <p>This method: 1. Sets the trainer to training mode 2. Iterates through the training dataloader 3. Executes training steps with gradient accumulation 4. Manages callbacks before/after each step 5. Performs validation at specified intervals 6. Updates process group timeouts after the first step</p> <p>The method handles gradient accumulation by only incrementing the global step when gradients are applied (not during accumulation).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If fewer batches are received than expected, which may indicate data loading issues in distributed training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def perform_training_epoch(self):\n    \"\"\"\n    Execute a complete training epoch.\n\n    This method:\n    1. Sets the trainer to training mode\n    2. Iterates through the training dataloader\n    3. Executes training steps with gradient accumulation\n    4. Manages callbacks before/after each step\n    5. Performs validation at specified intervals\n    6. Updates process group timeouts after the first step\n\n    The method handles gradient accumulation by only incrementing the\n    global step when gradients are applied (not during accumulation).\n\n    Raises:\n        RuntimeError: If fewer batches are received than expected, which\n            may indicate data loading issues in distributed training.\n    \"\"\"\n    if self._num_train_steps &lt;= 0:\n        return\n\n    self.train()\n    self.callbacks.pre_train_epoch(self)\n\n    batch_idx = 0\n    for batch in self.train_dataloader:\n        if batch_idx &gt;= self._num_train_steps:\n            break\n\n        self._is_last_training_batch = batch_idx == self._num_train_steps - 1\n\n        # Move batch to device, non-blocking\n        batch = apply_to_collection(\n            cast(Batch, batch),\n            function=lambda t: t.to(self.world.device, non_blocking=True),\n            dtype=torch.Tensor,\n        )\n\n        # Train Step\n        self.callbacks.pre_train_step(self, batch, batch_idx)\n        with self.train_context():\n            result = self.training_step(batch, batch_idx)\n\n        self.callbacks.post_train_step(self, result, batch_idx)\n        self.local_batches += 1\n\n        if not self.is_accumulating_gradients:\n            self._local_step += 1\n            self.global_step += 1\n        batch_idx += 1\n\n        # Reduce timeout after first train step for faster signal\n        # (assuming lazy init and compilation are finished)\n        if self._local_step == 1 and not self.is_accumulating_gradients:\n            self.world.set_pg_timeouts(\n                timeout=dt.timedelta(\n                    seconds=self.device_parameters.comm.train_timeout_seconds,\n                ),\n            )\n\n        # Validation Epoch\n        if (\n            self.global_step\n            % int(self._num_train_steps * self.training_parameters.val_frequency)\n        ) == 0 and not self.is_accumulating_gradients:\n            self.perform_validation_epoch()\n            self.train()\n\n    if (batch_idx + 1) &lt; self._num_train_steps:\n        raise RuntimeError(\n            f\"Worker {self.world.world_mesh.get_rank() if self.world.world_mesh is not None else 'unknown'} received fewer training batches than expected. \"\n            f\"Expected {self._num_train_steps} batches, received {batch_idx + 1}\"\n        )\n\n    self.callbacks.post_train_epoch(self, result)\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.perform_validation_epoch","title":"perform_validation_epoch","text":"<pre><code>perform_validation_epoch()\n</code></pre> <p>Execute a complete validation epoch.</p> <p>This method: 1. Sets the trainer to evaluation mode 2. Disables gradient computation with @torch.no_grad() 3. Iterates through the validation dataloader 4. Executes validation steps 5. Manages callbacks before/after each step and epoch</p> <p>All operations are performed without gradient computation for efficiency.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If fewer batches are received than expected, which may indicate data loading issues in distributed training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>@torch.no_grad()\ndef perform_validation_epoch(self):\n    \"\"\"\n    Execute a complete validation epoch.\n\n    This method:\n    1. Sets the trainer to evaluation mode\n    2. Disables gradient computation with @torch.no_grad()\n    3. Iterates through the validation dataloader\n    4. Executes validation steps\n    5. Manages callbacks before/after each step and epoch\n\n    All operations are performed without gradient computation for efficiency.\n\n    Raises:\n        RuntimeError: If fewer batches are received than expected, which\n            may indicate data loading issues in distributed training.\n    \"\"\"\n    if self._num_val_steps &lt;= 0:\n        return\n\n    self.eval()\n\n    # Validation Epoch Start\n    self.callbacks.pre_validation_epoch(self)\n\n    # Validation Epoch Loop\n    batch_idx = 0\n    for batch in self.val_dataloader:\n        if batch_idx &gt;= self._num_val_steps:\n            break\n\n        # Move batch to device, non-blocking\n        batch = apply_to_collection(\n            cast(Batch, batch),\n            function=lambda t: t.to(self.world.device, non_blocking=True),\n            dtype=torch.Tensor,\n        )\n\n        self.callbacks.pre_validation_step(self, batch, batch_idx)\n\n        with stacked_context(self.callbacks.validation_context(self)):\n            result = self.validation_step(batch, batch_idx)\n\n        self.callbacks.post_validation_step(self, result, batch_idx)\n        batch_idx += 1\n\n    if (batch_idx + 1) &lt; self._num_val_steps:\n        raise RuntimeError(\n            f\"Worker {self.world.world_mesh.get_rank() if self.world.world_mesh is not None else 'unknown'} received fewer validation batches than expected. \"\n            f\"Expected {self._num_val_steps} batches, received {batch_idx + 1}\"\n        )\n\n    # Validation Epoch End\n    self.callbacks.post_validation_epoch(self, result)\n</code></pre>"},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainer.perform_sanity_validation_steps","title":"perform_sanity_validation_steps","text":"<pre><code>perform_sanity_validation_steps()\n</code></pre> <p>Perform sanity validation steps before training begins.</p> <p>This method runs a limited number of validation steps at the start of training to ensure the validation pipeline is working correctly. It temporarily overrides the number of validation steps with the configured number of sanity validation steps.</p> <p>Sanity validation is only performed on the first epoch (epoch 0) and is skipped when resuming training from a checkpoint.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/base.py</code> <pre><code>def perform_sanity_validation_steps(self):\n    \"\"\"\n    Perform sanity validation steps before training begins.\n\n    This method runs a limited number of validation steps at the start\n    of training to ensure the validation pipeline is working correctly.\n    It temporarily overrides the number of validation steps with the\n    configured number of sanity validation steps.\n\n    Sanity validation is only performed on the first epoch (epoch 0)\n    and is skipped when resuming training from a checkpoint.\n    \"\"\"\n    # Don't perform sanity validation on resumption\n    if self.current_epoch &gt; 0:\n        return\n\n    # Store num val steps &amp; temporarily override to num sanity val steps\n    num_val_steps = self._num_val_steps\n    self._num_val_steps = self._num_sanity_val_steps\n\n    # Call validation epoch normally &amp; restore num val steps\n    self.perform_validation_epoch()\n    self._num_val_steps = num_val_steps\n</code></pre>"},{"location":"api/trainers/base/#configuration","title":"Configuration","text":""},{"location":"api/trainers/base/#dream_trainer.trainer.BaseTrainerConfig","title":"BaseTrainerConfig  <code>dataclass</code>","text":"<pre><code>BaseTrainerConfig(*, seed: int | None = 42, project: str, group: str, experiment: str, device_parameters: DeviceParameters, training_parameters: TrainingParameters, callbacks: CallbackCollection = cast('CallbackCollection', None))\n</code></pre> <p>               Bases: <code>AbstractTrainerConfig</code></p> <p>Configuration class for BaseTrainer.</p> <p>This dataclass holds all configuration parameters needed to initialize and run a BaseTrainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> ATTRIBUTE DESCRIPTION <code>training_parameters</code> <p>Configuration for training hyperparameters including epochs, batch size, gradient accumulation, validation frequency, etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>TrainingParameters</code> </p> <code>callbacks</code> <p>Collection of callbacks to execute during training lifecycle. If None, an empty CallbackCollection will be created.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>CallbackCollection</code> </p>"},{"location":"api/trainers/base/#usage-example","title":"Usage Example","text":"<pre><code>from dream_trainer.trainer import BaseTrainer, BaseTrainerConfig\nfrom dream_trainer.configs import TrainingParameters, DeviceParameters\nfrom dream_trainer.callbacks import CallbackCollection\n\nclass MyTrainer(BaseTrainer):\n    def configure(self):\n        # Define models\n        self.model = torch.nn.Linear(10, 1)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n\n        # Setup dataloaders\n        self._train_dataloader = DataLoader(train_dataset, batch_size=32)\n        self._val_dataloader = DataLoader(val_dataset, batch_size=32)\n\n    def setup(self):\n        # Move model to device\n        self.model = self.model.to(self.world.device)\n\n    def named_models(self):\n        return {\"main\": self.model}\n\n    def named_optimizers(self):\n        return {\"adam\": self.optimizer}\n\n    def named_schedulers(self):\n        return None\n\n    def get_module(self, fqn: str):\n        return getattr(self.model, fqn)\n\n    @property\n    def train_dataloader(self):\n        return self._train_dataloader\n\n    @property\n    def val_dataloader(self):\n        return self._val_dataloader\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch[\"input\"], batch[\"target\"]\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n\n        # Use no_gradient_sync for gradient accumulation\n        with self.no_gradient_sync(self.model):\n            self.backward(loss)\n\n        # Step optimizer when not accumulating\n        if not self.is_accumulating_gradients:\n            self.step(self.model, self.optimizer)\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch[\"input\"], batch[\"target\"]\n        y_hat = self.model(x)\n        loss = F.mse_loss(y_hat, y)\n        return {\"val_loss\": loss}\n\n# Create and run trainer\nconfig = BaseTrainerConfig(\n    seed=42,\n    project=\"my_project\",\n    group=\"experiments\",\n    experiment=\"baseline\",\n    device_parameters=DeviceParameters(),\n    training_parameters=TrainingParameters(\n        num_epochs=10,\n        gradient_accumulation_steps=4,\n        gradient_clip_val=1.0,\n        val_check_interval=100\n    ),\n    callbacks=CallbackCollection()\n)\n\ntrainer = MyTrainer(config)\ntrainer.configure()\ntrainer.setup()\ntrainer.fit()\n</code></pre>"},{"location":"api/trainers/base/#key-features","title":"Key Features","text":""},{"location":"api/trainers/base/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>BaseTrainer handles gradient accumulation automatically:</p> <pre><code># Configure in TrainingParameters\ntraining_params = TrainingParameters(\n    gradient_accumulation_steps=4,  # Accumulate over 4 batches\n    gradient_clip_val=1.0\n)\n\n# In training_step, use no_gradient_sync\ndef training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n\n    # Automatically handles gradient sync\n    with self.no_gradient_sync(self.model):\n        self.backward(loss)\n\n    # Only step when not accumulating\n    if not self.is_accumulating_gradients:\n        self.step(self.model, self.optimizer)\n</code></pre>"},{"location":"api/trainers/base/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>BaseTrainer integrates with PyTorch's autocast:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # Forward pass is automatically in autocast context\n    outputs = self.model(batch[\"input\"])\n\n    # Loss computation with loss parallelism\n    with self.loss_parallel():\n        loss = self.criterion(outputs, batch[\"target\"])\n\n    # Backward automatically handles mixed precision\n    self.backward(loss)\n</code></pre>"},{"location":"api/trainers/base/#validation","title":"Validation","text":"<p>Validation runs automatically based on configuration:</p> <pre><code>training_params = TrainingParameters(\n    val_check_interval=100,  # Validate every 100 steps\n    limit_val_batches=50,    # Use only 50 validation batches\n    num_sanity_val_steps=2   # Run 2 sanity check steps\n)\n</code></pre>"},{"location":"api/trainers/base/#callbacks","title":"Callbacks","text":"<p>BaseTrainer provides extensive callback hooks:</p> <pre><code># Callbacks are called at these points:\n# - pre/post_train_epoch\n# - pre/post_train_step\n# - pre/post_validation_epoch\n# - pre/post_validation_step\n# - pre/post_optimizer_step\n# - pre/post_optimizer_zero_grad\n</code></pre>"},{"location":"api/trainers/base/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/trainers/base/#custom-gradient-clipping","title":"Custom Gradient Clipping","text":"<pre><code>def step(self, model, optimizer):\n    # Compute gradient norm\n    parameters = [p for p in model.parameters() if p.grad is not None]\n    total_norm = self.total_gradient_norm(parameters)\n\n    # Custom clipping logic\n    if total_norm &gt; self.training_parameters.gradient_clip_val:\n        self.clip_gradient_norm(parameters, total_norm)\n\n    # Continue with optimizer step\n    optimizer.step()\n</code></pre>"},{"location":"api/trainers/base/#distributed-training","title":"Distributed Training","text":"<p>BaseTrainer automatically handles distributed synchronization:</p> <pre><code># No gradient sync during accumulation\nwith self.no_gradient_sync(self.model):\n    self.backward(loss)\n\n# Automatic gradient sync on last accumulation step\nif not self.is_accumulating_gradients:\n    # Gradients are synchronized here\n    self.step(self.model, self.optimizer)\n</code></pre>"},{"location":"api/trainers/base/#see-also","title":"See Also","text":"<ul> <li>AbstractTrainer - Base interface</li> <li>DreamTrainer - Full-featured trainer</li> <li>TrainingParameters - Training configuration</li> <li>CallbackCollection - Callback system </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/trainers/dream/","title":"DreamTrainer","text":"<p>The <code>DreamTrainer</code> class is a production-ready trainer that demonstrates the full power of Dream Trainer's mixin architecture. It combines all major features into a single, cohesive training system.</p>"},{"location":"api/trainers/dream/#overview","title":"Overview","text":"<p><code>DreamTrainer</code> combines: - Base training functionality from <code>BaseTrainer</code> - Model setup and parallelism from <code>SetupMixin</code> - Evaluation metrics from <code>EvalMetricMixin</code> - Weights &amp; Biases logging from <code>WandBLoggerMixin</code></p> <p>This trainer serves as both a ready-to-use solution and a reference implementation for creating custom trainers.</p>"},{"location":"api/trainers/dream/#class-reference","title":"Class Reference","text":""},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer","title":"DreamTrainer","text":"<pre><code>DreamTrainer(config: DreamTrainerConfig)\n</code></pre> <p>               Bases: <code>BaseTrainer</code>, <code>EvalMetricMixin</code>, <code>SetupMixin</code>, <code>WandBLoggerMixin</code></p> <p>Proprietary DreamTrainer demonstrating the mixin architecture pattern.</p> <p>This trainer showcases how to compose functionality using mixins to create a fully-featured PyTorch training system. Each mixin adds specific capabilities while maintaining clean separation of concerns.</p>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--mixin-architecture-overview","title":"Mixin Architecture Overview","text":"<p>The DreamTrainer uses multiple inheritance to combine functionality from various mixins:</p> <ul> <li> <p>BaseTrainer: Provides the core training loop, gradient accumulation, validation,   and callback system. This is always the foundation of any custom trainer.</p> </li> <li> <p>SetupMixin: Handles model initialization, parallelism (FSDP, DDP, TP, PP),   optimizer/scheduler setup, and dataloader configuration. Composed of:</p> </li> <li><code>ModelSetupMixin</code>: Model configuration, weight init, parallelism, compilation</li> <li><code>OptimizerAndSchedulerSetupMixin</code>: Optimizer and LR scheduler management</li> <li> <p><code>DataLoaderSetupMixin</code>: Train/validation dataloader setup</p> </li> <li> <p>EvalMetricMixin: Integrates torchmetrics for standardized evaluation metrics   with automatic device placement and distributed synchronization.</p> </li> <li> <p>WandBLoggerMixin: Adds Weights &amp; Biases logging with support for scalars,   images, videos, plots, and model watching.</p> </li> </ul>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--creating-your-own-trainer","title":"Creating Your Own Trainer","text":"<p>To create a custom trainer, follow these steps:</p>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--1-define-your-configuration","title":"1. Define Your Configuration","text":"<pre><code>@dataclass(kw_only=True)\nclass MyTrainerConfig(\n    BaseTrainerConfig,\n    SetupConfigMixin,\n    LoggerConfigMixin,\n):\n    my_param: float = 1.0\n</code></pre>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--2-create-your-trainer-class","title":"2. Create Your Trainer Class","text":"<pre><code>class MyTrainer(BaseTrainer, SetupMixin, LoggerMixin):\n    config: MyTrainerConfig\n\n    def __init__(self, config: MyTrainerConfig):\n        super().__init__(config)\n</code></pre>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--3-implement-required-methods","title":"3. Implement Required Methods","text":"<p>Each mixin requires certain methods to be implemented:</p> <p>From BaseTrainer (always required): - <code>training_step(batch, batch_idx)</code>: Forward pass and loss computation - <code>validation_step(batch, batch_idx)</code>: Validation forward pass</p> <p>From SetupMixin sub-components: - <code>configure_models()</code>: Create model instances as attributes - <code>init_weights()</code>: Initialize model weights - <code>configure_dataloaders()</code>: Return (train_loader, val_loader) tuple - <code>configure_optimizers()</code>: Create optimizer instances as attributes - <code>configure_schedulers()</code>: (Optional) Create LR schedulers</p> <p>From SetupMixin (optional, for advanced features): - <code>apply_tensor_parallel(tp_mesh)</code>: Apply tensor parallelism - <code>apply_pipeline_parallel(pp_mesh)</code>: Apply pipeline parallelism - <code>apply_activation_checkpointing()</code>: Enable gradient checkpointing - <code>apply_compile()</code>: Compile models with torch.compile - <code>apply_fully_shard(config)</code>: Apply FSDP sharding - <code>apply_replicate(dp_mesh)</code>: Apply DDP replication</p> <p>From EvalMetricMixin (if used): - <code>configure_metrics()</code>: Create torchmetrics instances as attributes</p> <p>From LoggerMixin variants (if used): - Various logging methods are provided, override as needed</p>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--4-example-implementation","title":"4. Example Implementation","text":"<pre><code>class MyTrainer(BaseTrainer, SetupMixin, EvalMetricMixin):\n    config: MyTrainerConfig\n\n    def configure_models(self):\n        # Models become attributes and are auto-tracked\n        self.model = MyModel(self.config.model_config)\n\n    def init_weights(self):\n        # Initialize weights after model creation\n        self.model.apply(self._init_weights_fn)\n\n    def configure_optimizers(self):\n        # Optimizers become attributes and are auto-tracked\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.config.learning_rate\n        )\n\n    def configure_dataloaders(self):\n        train_loader = DataLoader(train_dataset, ...)\n        val_loader = DataLoader(val_dataset, ...)\n        return train_loader, val_loader\n\n    def configure_metrics(self):\n        # Metrics become attributes and are auto-tracked\n        self.accuracy = torchmetrics.Accuracy()\n        self.f1 = torchmetrics.F1Score()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x)\n        loss = F.cross_entropy(logits, y)\n\n        # Use self.backward() for proper gradient scaling\n        self.backward(loss)\n\n        # Step optimizer at the right frequency\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self.model(x)\n        loss = F.cross_entropy(logits, y)\n\n        # Update metrics\n        self.accuracy(logits, y)\n        self.f1(logits, y)\n\n        return {\"val_loss\": loss}\n</code></pre>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--mixin-composition-rules","title":"Mixin Composition Rules","text":"<ol> <li>Order matters: Place BaseTrainer first, then mixins in order of dependencies</li> <li>Config alignment: Trainer mixins must match their config counterparts</li> <li>Method resolution: Later mixins override earlier ones (Python MRO)</li> <li>Setup order: The <code>setup()</code> method calls setup in a specific order:</li> <li>Models \u2192 Optimizers/Schedulers \u2192 Dataloaders \u2192 Metrics</li> </ol>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainer--benefits-of-the-mixin-architecture","title":"Benefits of the Mixin Architecture","text":"<ul> <li>Modularity: Pick only the features you need</li> <li>Extensibility: Easy to add new mixins for custom functionality</li> <li>Reusability: Share common patterns across different trainers</li> <li>Separation of Concerns: Each mixin handles one aspect of training</li> <li>Type Safety: Config classes ensure required parameters are provided</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/dream.py</code> <pre><code>def __init__(self, config: DreamTrainerConfig):\n    super().__init__(config)\n</code></pre>"},{"location":"api/trainers/dream/#configuration","title":"Configuration","text":""},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainerConfig","title":"DreamTrainerConfig  <code>dataclass</code>","text":"<pre><code>DreamTrainerConfig(*, seed: int | None = 42, project: str, group: str, experiment: str = get_experiment_name(), device_parameters: DeviceParameters, logging_parameters: WandbLoggingParameters = WandbLoggingParameters(), training_parameters: TrainingParameters, callbacks: CallbackCollection = cast('CallbackCollection', None))\n</code></pre> <p>               Bases: <code>BaseTrainerConfig</code>, <code>EvalMetricConfigMixin</code>, <code>SetupConfigMixin</code>, <code>WandBLoggerConfigMixin</code></p> <p>Configuration class for DreamTrainer using the mixin architecture.</p> <p>This class demonstrates how to compose trainer configurations using mixins. Each mixin provides specific configuration fields and functionality:</p> <ul> <li>BaseTrainerConfig: Core training parameters (epochs, batch size, callbacks, etc.)</li> <li>EvalMetricConfigMixin: Configuration for evaluation metrics (requires torchmetrics)</li> <li>SetupConfigMixin: Combines model, optimizer, scheduler, and dataloader setup configs</li> <li>WandBLoggerConfigMixin: Weights &amp; Biases logging configuration</li> </ul>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainerConfig--creating-custom-trainer-configurations","title":"Creating Custom Trainer Configurations","text":"<p>To create your own trainer configuration, inherit from <code>BaseTrainerConfig</code> and mix in the configuration classes for the features you need:</p> <pre><code>@dataclass(kw_only=True)\nclass MyTrainerConfig(\n    BaseTrainerConfig,          # Always required as the base\n    SetupConfigMixin,           # For model/optimizer/dataloader setup\n    LoggerConfigMixin,          # For generic logging capabilities\n    QuantizeConfigMixin,        # For quantization support (optional)\n):\n    # Add any custom configuration fields here\n    my_custom_param: float = 0.1\n</code></pre>"},{"location":"api/trainers/dream/#dream_trainer.trainer.DreamTrainerConfig--available-configuration-mixins","title":"Available Configuration Mixins","text":"<ul> <li><code>SetupConfigMixin</code>: Aggregates model, optimizer, scheduler, and dataloader configs</li> <li><code>EvalMetricConfigMixin</code>: Adds torchmetrics support for evaluation</li> <li><code>LoggerConfigMixin</code>: Base logging configuration</li> <li><code>WandBLoggerConfigMixin</code>: Weights &amp; Biases specific logging</li> <li><code>QuantizeConfigMixin</code>: Model quantization configuration</li> </ul> <p>The order of inheritance doesn't matter for configs since they only add fields.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/trainers/dream/#complete-example","title":"Complete Example","text":"<p>Here's a full example using DreamTrainer for training a vision model:</p> <pre><code>from dream_trainer.trainer import DreamTrainer, DreamTrainerConfig\nfrom dream_trainer.configs import (\n    DeviceParameters,\n    TrainingParameters,\n    ModelConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n    DataLoaderConfig,\n    LoggerConfig,\n)\nfrom dream_trainer.callbacks import CallbackCollection\nimport torch\nimport torch.nn as nn\nimport torchmetrics\nfrom torch.utils.data import DataLoader\n\nclass VisionTrainer(DreamTrainer):\n    def configure_models(self):\n        # Model is automatically tracked by SetupMixin\n        self.model = torchvision.models.resnet50(\n            pretrained=self.config.model_config.pretrained\n        )\n        self.model.fc = nn.Linear(2048, self.config.model_config.num_classes)\n\n    def init_weights(self):\n        # Initialize the new classifier head\n        nn.init.xavier_uniform_(self.model.fc.weight)\n        nn.init.zeros_(self.model.fc.bias)\n\n    def configure_optimizers(self):\n        # Optimizer is automatically tracked\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.config.optimizer_config.learning_rate,\n            weight_decay=self.config.optimizer_config.weight_decay\n        )\n\n    def configure_schedulers(self):\n        # Scheduler is automatically tracked\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=self.config.training_parameters.num_epochs\n        )\n\n    def configure_dataloaders(self):\n        train_dataset = ImageDataset(split=\"train\", transform=train_transforms)\n        val_dataset = ImageDataset(split=\"val\", transform=val_transforms)\n\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.config.dataloader_config.train_batch_size,\n            shuffle=True,\n            num_workers=self.config.dataloader_config.num_workers,\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.dataloader_config.val_batch_size,\n            shuffle=False,\n            num_workers=self.config.dataloader_config.num_workers,\n            pin_memory=True\n        )\n\n        return train_loader, val_loader\n\n    def configure_metrics(self):\n        # Metrics are automatically tracked and moved to device\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n        self.top5_accuracy = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=10, top_k=5\n        )\n        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=10)\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch[\"image\"], batch[\"label\"]\n\n        # Forward pass\n        logits = self.model(images)\n        loss = F.cross_entropy(logits, labels)\n\n        # Backward with gradient accumulation\n        with self.no_gradient_sync(self.model):\n            self.backward(loss)\n\n        # Step optimizer when not accumulating\n        if not self.is_accumulating_gradients:\n            grad_norm = self.step(self.model, self.optimizer)\n\n            # Log gradient norm\n            self.log_scalar(\"train/grad_norm\", grad_norm)\n\n        # Log loss\n        self.log_scalar(\"train/loss\", loss)\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch[\"image\"], batch[\"label\"]\n\n        # Forward pass\n        logits = self.model(images)\n        loss = F.cross_entropy(logits, labels)\n\n        # Update metrics\n        self.accuracy(logits, labels)\n        self.top5_accuracy(logits, labels)\n        self.f1(logits, labels)\n\n        # Log loss\n        self.log_scalar(\"val/loss\", loss)\n\n        return {\"val_loss\": loss}\n\n# Configure and run\nconfig = DreamTrainerConfig(\n    # Basic info\n    seed=42,\n    project=\"vision_classification\",\n    group=\"resnet50\",\n    experiment=\"baseline\",\n\n    # Device setup\n    device_parameters=DeviceParameters(\n        devices=\"cuda\",\n        num_nodes=1,\n        strategy=\"fsdp\",\n        fsdp_config={\"sharding_strategy\": \"FULL_SHARD\"}\n    ),\n\n    # Training parameters\n    training_parameters=TrainingParameters(\n        num_epochs=90,\n        gradient_accumulation_steps=4,\n        gradient_clip_val=1.0,\n        val_check_interval=1000,\n        limit_val_batches=100\n    ),\n\n    # Model config\n    model_config=ModelConfig(\n        pretrained=True,\n        num_classes=10,\n        compile_mode=\"reduce-overhead\"\n    ),\n\n    # Optimizer config\n    optimizer_config=OptimizerConfig(\n        learning_rate=1e-3,\n        weight_decay=0.01\n    ),\n\n    # DataLoader config\n    dataloader_config=DataLoaderConfig(\n        train_batch_size=32,\n        val_batch_size=64,\n        num_workers=4\n    ),\n\n    # WandB config\n    logger_config=LoggerConfig(\n        enabled=True,\n        log_every_n_steps=10,\n        log_model=\"all\"  # Log model at all checkpoints\n    ),\n\n    # Callbacks\n    callbacks=CallbackCollection([\n        # Add your callbacks here\n    ])\n)\n\n# Create and run trainer\ntrainer = VisionTrainer(config)\ntrainer.configure()\ntrainer.setup()\ntrainer.fit()\n</code></pre>"},{"location":"api/trainers/dream/#mixin-features","title":"Mixin Features","text":""},{"location":"api/trainers/dream/#setupmixin-features","title":"SetupMixin Features","text":"<p>The <code>SetupMixin</code> provides:</p> <ol> <li>Model Setup</li> <li>Automatic model registration via attributes</li> <li>Weight initialization hooks</li> <li>Parallelism strategies (FSDP, DDP, TP, PP)</li> <li>Model compilation with torch.compile</li> <li> <p>Activation checkpointing</p> </li> <li> <p>Optimizer &amp; Scheduler Setup</p> </li> <li>Automatic optimizer registration</li> <li>Learning rate scheduler management</li> <li> <p>Parameter group configuration</p> </li> <li> <p>DataLoader Setup</p> </li> <li>Train and validation dataloader configuration</li> <li>Automatic device placement</li> <li>Distributed sampling</li> </ol>"},{"location":"api/trainers/dream/#evalmetricmixin-features","title":"EvalMetricMixin Features","text":"<p>The <code>EvalMetricMixin</code> provides:</p> <ol> <li>Metric Management</li> <li>Automatic metric registration via attributes</li> <li>Device placement for metrics</li> <li>Distributed synchronization</li> <li> <p>Metric computation and logging</p> </li> <li> <p>Integration with torchmetrics</p> </li> <li>Support for all torchmetrics modules</li> <li>Automatic reset between epochs</li> <li>Proper aggregation across devices</li> </ol>"},{"location":"api/trainers/dream/#wandbloggermixin-features","title":"WandBLoggerMixin Features","text":"<p>The <code>WandBLoggerMixin</code> provides:</p> <ol> <li>Logging Capabilities</li> <li>Scalar logging with <code>log_scalar</code></li> <li>Image logging with <code>log_image</code></li> <li>Video logging with <code>log_video</code></li> <li>Histogram logging with <code>log_histogram</code></li> <li> <p>Model watching and gradient tracking</p> </li> <li> <p>Experiment Tracking</p> </li> <li>Automatic run initialization</li> <li>Hyperparameter logging</li> <li>System metrics tracking</li> <li>Artifact management</li> </ol>"},{"location":"api/trainers/dream/#creating-custom-trainers","title":"Creating Custom Trainers","text":"<p>To create your own trainer based on DreamTrainer:</p> <ol> <li> <p>Choose Your Mixins <pre><code>class MyTrainer(BaseTrainer, SetupMixin, EvalMetricMixin):\n    pass\n</code></pre></p> </li> <li> <p>Match Configuration <pre><code>@dataclass\nclass MyTrainerConfig(BaseTrainerConfig, SetupConfigMixin, EvalMetricConfigMixin):\n    pass\n</code></pre></p> </li> <li> <p>Implement Required Methods</p> </li> <li>From SetupMixin: <code>configure_models</code>, <code>configure_optimizers</code>, <code>configure_dataloaders</code></li> <li>From EvalMetricMixin: <code>configure_metrics</code></li> <li> <p>From BaseTrainer: <code>training_step</code>, <code>validation_step</code></p> </li> <li> <p>Use Provided Features</p> </li> <li>Call <code>self.backward()</code> for proper gradient scaling</li> <li>Use <code>self.no_gradient_sync()</code> for gradient accumulation</li> <li>Call <code>self.step()</code> for optimizer updates</li> <li>Use <code>self.log_scalar()</code> for metrics logging</li> </ol>"},{"location":"api/trainers/dream/#see-also","title":"See Also","text":"<ul> <li>BaseTrainer - Core training functionality</li> <li>SetupMixin - Model and optimizer setup</li> <li>EvalMetricMixin - Metrics integration</li> <li>WandBLoggerMixin - WandB logging </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/utilities/data/","title":"Data Utilities","text":"<p>Dream Trainer provides utilities for working with data loaders and datasets in distributed training environments.</p>"},{"location":"api/utilities/data/#dataloader-utilities","title":"DataLoader Utilities","text":""},{"location":"api/utilities/data/#get_epoch_length","title":"get_epoch_length","text":"<p>Calculate the length of an epoch from a dataloader:</p>"},{"location":"api/utilities/data/#dream_trainer.utils.dataloader.get_epoch_length","title":"get_epoch_length","text":"<pre><code>get_epoch_length(dataloader: Iterable, length: int | None) -&gt; int\n</code></pre> Source code in <code>src/dream_trainer/utils/dataloader.py</code> <pre><code>def get_epoch_length(dataloader: Iterable, length: int | None) -&gt; int:\n    if length is not None:\n        return length\n\n    try:\n        return len(dataloader)  # type: ignore\n    except TypeError:\n        raise ValueError(\n            f\"The underlying dataset of {dataloader} does not have __len__ defined. \"\n            f\"Please specify training_parameters.{{stage}}_steps_per_epoch instead. \"\n        )\n</code></pre>"},{"location":"api/utilities/data/#usage","title":"Usage","text":"<pre><code>from dream_trainer.utils.dataloader import get_epoch_length\n\n# Automatically determine length\nepoch_length = get_epoch_length(train_dataloader, length=None)\n\n# Or specify manually\nepoch_length = get_epoch_length(train_dataloader, length=1000)\n</code></pre>"},{"location":"api/utilities/data/#get_train_dataloader_steps","title":"get_train_dataloader_steps","text":"<p>Calculate the number of training steps:</p>"},{"location":"api/utilities/data/#dream_trainer.utils.dataloader.get_train_dataloader_steps","title":"get_train_dataloader_steps","text":"<pre><code>get_train_dataloader_steps(dataloader: Iterable, train_steps_per_epoch: int | None, train_batch_size: int = 1, dp_size: int = 1) -&gt; tuple[int, int, int]\n</code></pre> <p>Calculate training dataloader steps, effective minibatch size, and gradient accumulation steps.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>dataloader</code> <p>The training dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <code>train_steps_per_epoch</code> <p>Number of training steps per epoch. If None, uses the length of the dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int | None</code> </p> <code>train_batch_size</code> <p>The total batch size for training (across all devices/processes).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>dp_size</code> <p>Data parallel size (number of processes/devices).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>(train_batch_size, num_train_steps, gradient_accumulation_steps) - train_batch_size (int): The total batch size for training. - num_train_steps (int): Number of training steps per epoch (possibly adjusted for gradient accumulation). - gradient_accumulation_steps (int): Number of steps to accumulate gradients before optimizer step.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>tuple[int, int, int]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If batch size cannot be determined, or if effective minibatch size is invalid.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/utils/dataloader.py</code> <pre><code>def get_train_dataloader_steps(\n    dataloader: Iterable,\n    train_steps_per_epoch: int | None,\n    train_batch_size: int = 1,\n    dp_size: int = 1,\n) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Calculate training dataloader steps, effective minibatch size, and gradient accumulation steps.\n\n    Args:\n        dataloader (Iterable): The training dataloader.\n        train_steps_per_epoch (int | None): Number of training steps per epoch. If None, uses the length of the dataloader.\n        train_batch_size (int): The total batch size for training (across all devices/processes).\n        dp_size (int): Data parallel size (number of processes/devices).\n\n    Returns:\n        tuple: (train_batch_size, num_train_steps, gradient_accumulation_steps)\n            - train_batch_size (int): The total batch size for training.\n            - num_train_steps (int): Number of training steps per epoch (possibly adjusted for gradient accumulation).\n            - gradient_accumulation_steps (int): Number of steps to accumulate gradients before optimizer step.\n\n    Raises:\n        ValueError: If batch size cannot be determined, or if effective minibatch size is invalid.\n    \"\"\"\n    num_train_steps: int = get_epoch_length(dataloader, train_steps_per_epoch)\n\n    if train_steps_per_epoch is not None and train_steps_per_epoch &gt; num_train_steps:\n        logger.warning(\n            f\"train_steps_per_epoch, {train_steps_per_epoch}, \"\n            f\"is greater than the number of batches in the dataloader, {num_train_steps}. \",\n        )\n\n    dataloader_batch_size: int | None = getattr(dataloader, \"batch_size\", None)\n    if dataloader_batch_size is None:\n        dataloader_batch_size = getattr(getattr(dataloader, \"dataset\", {}), \"batch_size\", None)\n\n    if dataloader_batch_size is None:\n        raise ValueError(\n            \"Neither dataloader nor dataloader.dataset has non-None 'batch_size' attribute. \"\n            \"Please ensure one or the other specifies an integer batch size \"\n            \"to correctly compute the effective minibatch size and gradient accumulation.\"\n        )\n\n    effective_minibatch_size: int = dataloader_batch_size * dp_size\n\n    if effective_minibatch_size &gt; train_batch_size:\n        raise ValueError(\n            f\"Effective minibatch size, {effective_minibatch_size}, is greater than train_batch_size, {train_batch_size}\"\n        )\n    if train_batch_size % effective_minibatch_size != 0:\n        raise ValueError(\n            f\"train_batch_size, {train_batch_size}, must be divisible by effective minibatch size, {effective_minibatch_size}\"\n        )\n\n    gradient_accumulation_steps = train_batch_size // effective_minibatch_size\n\n    # _num_train_batches is the number of dataloader batches per epoch\n    if train_steps_per_epoch is not None:\n        num_train_steps *= gradient_accumulation_steps\n\n    return train_batch_size, num_train_steps, gradient_accumulation_steps\n</code></pre>"},{"location":"api/utilities/data/#get_val_dataloader_steps","title":"get_val_dataloader_steps","text":"<p>Calculate the number of validation steps:</p>"},{"location":"api/utilities/data/#dream_trainer.utils.dataloader.get_val_dataloader_steps","title":"get_val_dataloader_steps","text":"<pre><code>get_val_dataloader_steps(dataloader: Iterable, val_steps_per_epoch: int | None, num_sanity_val_steps: int = 0, dp_size: int = 1) -&gt; tuple[int, int]\n</code></pre> <p>Calculate validation dataloader steps and sanity validation steps, accounting for data parallelism.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>dataloader</code> <p>The validation dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Iterable</code> </p> <code>val_steps_per_epoch</code> <p>Number of validation steps per epoch. If None, uses the length of the dataloader.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int | None</code> </p> <code>num_sanity_val_steps</code> <p>Number of sanity validation steps to run before training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dp_size</code> <p>Data parallel size (number of processes/devices).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>(num_val_batches, num_sanity_val_steps) - num_val_batches (int): Number of validation batches per epoch (divided by dp_size). - num_sanity_val_steps (int): Number of sanity validation steps (divided by dp_size).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>tuple[int, int]</code> </p> Source code in <code>src/dream_trainer/utils/dataloader.py</code> <pre><code>def get_val_dataloader_steps(\n    dataloader: Iterable,\n    val_steps_per_epoch: int | None,\n    num_sanity_val_steps: int = 0,\n    dp_size: int = 1,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Calculate validation dataloader steps and sanity validation steps, accounting for data parallelism.\n\n    Args:\n        dataloader (Iterable): The validation dataloader.\n        val_steps_per_epoch (int | None): Number of validation steps per epoch. If None, uses the length of the dataloader.\n        num_sanity_val_steps (int): Number of sanity validation steps to run before training.\n        dp_size (int): Data parallel size (number of processes/devices).\n\n    Returns:\n        tuple: (num_val_batches, num_sanity_val_steps)\n            - num_val_batches (int): Number of validation batches per epoch (divided by dp_size).\n            - num_sanity_val_steps (int): Number of sanity validation steps (divided by dp_size).\n    \"\"\"\n    _num_val_batches: int = get_epoch_length(dataloader, val_steps_per_epoch)\n    if val_steps_per_epoch is not None and val_steps_per_epoch &gt; _num_val_batches:\n        logger.warning(\n            f\"val_batches_per_epoch, {val_steps_per_epoch}, \"\n            f\"is greater than the number of batches in the dataloader, {_num_val_batches}. \"\n        )\n\n    return _num_val_batches // dp_size, num_sanity_val_steps // dp_size\n</code></pre>"},{"location":"api/utilities/data/#batch-type","title":"Batch Type","text":"<p>Dream Trainer uses a standard batch format:</p> <pre><code>from dream_trainer.utils.dataloader import Batch\n\n# Batch is a dict[str, Any]\nbatch: Batch = {\n    \"input\": input_tensor,\n    \"target\": target_tensor,\n    \"mask\": attention_mask,\n    # ... any other fields\n}\n</code></pre>"},{"location":"api/utilities/data/#dataloader-best-practices","title":"DataLoader Best Practices","text":""},{"location":"api/utilities/data/#1-distributed-sampling","title":"1. Distributed Sampling","text":"<pre><code>from torch.utils.data import DataLoader, DistributedSampler\n\ndef configure_dataloaders(self):\n    # Create distributed sampler for multi-GPU\n    train_sampler = None\n    if self.world.size &gt; 1:\n        train_sampler = DistributedSampler(\n            train_dataset,\n            num_replicas=self.world.size,\n            rank=self.world.rank,\n            shuffle=True,\n            seed=self.seed\n        )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        shuffle=(train_sampler is None),\n        num_workers=4,\n        pin_memory=True,\n        persistent_workers=True\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/utilities/data/#2-efficient-data-loading","title":"2. Efficient Data Loading","text":"<pre><code># Use multiple workers\nnum_workers = min(8, os.cpu_count() // self.world.local_size)\n\n# Pin memory for GPU transfer\npin_memory = self.world.device.type == \"cuda\"\n\n# Keep workers alive\npersistent_workers = num_workers &gt; 0\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    pin_memory=pin_memory,\n    persistent_workers=persistent_workers,\n    prefetch_factor=2 if num_workers &gt; 0 else None\n)\n</code></pre>"},{"location":"api/utilities/data/#3-stateful-dataloaders","title":"3. Stateful DataLoaders","text":"<p>For resumable training with custom samplers:</p> <pre><code>class StatefulDataLoader(DataLoader):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.current_epoch = 0\n\n    def state_dict(self):\n        return {\n            \"current_epoch\": self.current_epoch,\n            \"sampler_state\": getattr(self.sampler, \"state_dict\", lambda: {})()\n        }\n\n    def load_state_dict(self, state_dict):\n        self.current_epoch = state_dict[\"current_epoch\"]\n        if hasattr(self.sampler, \"load_state_dict\"):\n            self.sampler.load_state_dict(state_dict[\"sampler_state\"])\n</code></pre>"},{"location":"api/utilities/data/#4-memory-efficient-loading","title":"4. Memory-Efficient Loading","text":"<pre><code># For large datasets\nclass StreamingDataset(IterableDataset):\n    def __init__(self, urls: list[str], buffer_size: int = 1000):\n        self.urls = urls\n        self.buffer_size = buffer_size\n\n    def __iter__(self):\n        # Stream data from URLs\n        for url in self.urls:\n            yield from self.stream_from_url(url)\n\n# Use with DataLoader\ndataloader = DataLoader(\n    StreamingDataset(urls),\n    batch_size=None,  # Dataset yields batches\n    num_workers=1     # Streaming doesn't parallelize well\n)\n</code></pre>"},{"location":"api/utilities/data/#integration-with-trainer","title":"Integration with Trainer","text":"<p>DataLoaders are typically configured in the trainer:</p> <pre><code>class MyTrainer(DataLoaderSetupMixin):\n    def configure_dataloaders(self):\n        # Get steps for progress tracking\n        train_steps = get_train_dataloader_steps(\n            self.config.training_parameters,\n            train_loader\n        )\n\n        val_steps = get_val_dataloader_steps(\n            self.config.training_parameters,\n            val_loader\n        )\n\n        self._num_train_steps = train_steps\n        self._num_val_steps = val_steps\n\n        return train_loader, val_loader\n</code></pre>"},{"location":"api/utilities/data/#custom-batch-processing","title":"Custom Batch Processing","text":""},{"location":"api/utilities/data/#collate-functions","title":"Collate Functions","text":"<pre><code>def custom_collate_fn(samples: list[dict]) -&gt; Batch:\n    # Process list of samples into batch\n    inputs = torch.stack([s[\"input\"] for s in samples])\n    targets = torch.stack([s[\"target\"] for s in samples])\n\n    # Dynamic padding\n    max_len = max(len(s[\"sequence\"]) for s in samples)\n    padded_sequences = torch.zeros(len(samples), max_len)\n\n    for i, s in enumerate(samples):\n        seq_len = len(s[\"sequence\"])\n        padded_sequences[i, :seq_len] = s[\"sequence\"]\n\n    return {\n        \"input\": inputs,\n        \"target\": targets,\n        \"sequences\": padded_sequences,\n        \"lengths\": torch.tensor([len(s[\"sequence\"]) for s in samples])\n    }\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    collate_fn=custom_collate_fn\n)\n</code></pre>"},{"location":"api/utilities/data/#batch-transformations","title":"Batch Transformations","text":"<pre><code>class BatchTransform:\n    def __init__(self, tokenizer, max_length: int = 512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __call__(self, batch: list[dict]) -&gt; Batch:\n        texts = [item[\"text\"] for item in batch]\n        labels = [item[\"label\"] for item in batch]\n\n        # Tokenize batch\n        encoding = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"],\n            \"attention_mask\": encoding[\"attention_mask\"],\n            \"labels\": torch.tensor(labels)\n        }\n</code></pre>"},{"location":"api/utilities/data/#performance-tips","title":"Performance Tips","text":""},{"location":"api/utilities/data/#1-prefetching","title":"1. Prefetching","text":"<pre><code># Increase prefetch factor for CPU-bound loading\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=8,\n    prefetch_factor=4  # Prefetch 4 batches per worker\n)\n</code></pre>"},{"location":"api/utilities/data/#2-avoid-bottlenecks","title":"2. Avoid Bottlenecks","text":"<pre><code># Profile data loading\nimport time\n\ndef profile_dataloader(dataloader, num_batches: int = 100):\n    start = time.time()\n\n    for i, batch in enumerate(dataloader):\n        if i &gt;= num_batches:\n            break\n\n        # Ensure GPU sync\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n    elapsed = time.time() - start\n    print(f\"Average batch time: {elapsed / num_batches:.3f}s\")\n    print(f\"Throughput: {num_batches / elapsed:.1f} batches/s\")\n</code></pre>"},{"location":"api/utilities/data/#3-data-pipeline-optimization","title":"3. Data Pipeline Optimization","text":"<pre><code># Chain transforms efficiently\ntransform_pipeline = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n])\n\n# Use torch.compile for transforms\ncompiled_transform = torch.compile(transform_pipeline)\n</code></pre>"},{"location":"api/utilities/data/#see-also","title":"See Also","text":"<ul> <li>DataLoaderSetupMixin - DataLoader configuration</li> <li>Training Configuration - Batch size settings</li> <li>Distributed Training - Multi-GPU data loading </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"},{"location":"api/utilities/world/","title":"World Management","text":"<p>The world management utilities handle distributed training context, device meshes, and fault tolerance. The <code>DistributedWorld</code> class is the core abstraction for managing distributed training environments.</p>"},{"location":"api/utilities/world/#distributedworld","title":"DistributedWorld","text":"<p>The main class for managing distributed training context:</p>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld","title":"DistributedWorld","text":"<pre><code>DistributedWorld(config: DeviceParameters)\n</code></pre> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>def __init__(self, config: DeviceParameters):\n    config.validate()\n    self.config = config\n\n    self.world_mesh: DeviceMesh | None = None\n    self.world_size = int(os.environ.get(\"WORLD_SIZE\", torch.cuda.device_count()))\n\n    # NOTE: `device_module.set_device` has to be set before creating TorchFT manager.\n    self.device_type, self.device_module = get_device_info()\n    self.device = torch.device(f\"{self.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n    self.device_module.set_device(self.device)\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld-attributes","title":"Attributes","text":""},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.is_global_zero","title":"is_global_zero  <code>property</code>","text":"<pre><code>is_global_zero\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld-functions","title":"Functions","text":""},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.get_mesh","title":"get_mesh","text":"<pre><code>get_mesh(mesh_dim_name: str) -&gt; DeviceMesh | None\n</code></pre> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>def get_mesh(self, mesh_dim_name: str) -&gt; DeviceMesh | None:\n    if self.world_mesh is None:\n        raise RuntimeError(\"World mesh not yet initialized. Call `launch` first.\")\n\n    if not self.world_mesh.mesh_dim_names:\n        return None\n\n    try:\n        return self.world_mesh[mesh_dim_name]\n    except KeyError:\n        return None\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.all_reduce","title":"all_reduce","text":"<pre><code>all_reduce(tensor: Tensor, op: RedOpType | str = ReduceOp.SUM, group: ProcessGroup | None = None, async_op: bool = False)\n</code></pre> <p>All-reduce the given tensor across all processes in the specified process group.</p> <p>This method performs an all-reduce operation on the input tensor, summing its values across all processes.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>def all_reduce(\n    self,\n    tensor: torch.Tensor,\n    op: ReduceOp.RedOpType | str = ReduceOp.SUM,\n    group: ProcessGroup | None = None,\n    async_op: bool = False,\n):\n    \"\"\"\n    All-reduce the given tensor across all processes in the specified process group.\n\n    This method performs an all-reduce operation on the input tensor, summing its values across all processes.\n    \"\"\"\n    if self.world_mesh is None:\n        raise RuntimeError(\"World mesh not yet initialized. Call `launch` first.\")\n\n    dist.all_reduce(\n        tensor, op=cast(dist.ReduceOp.RedOpType, op), group=group, async_op=async_op\n    )\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.barrier","title":"barrier","text":"<pre><code>barrier(process_group: ProcessGroup | None = None)\n</code></pre> <p>Synchronizes all processes in the specified process group.</p> <p>This method blocks until all processes in the group reach this barrier. If no process group is specified, the world mesh group is used by default.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>process_group</code> <p>The process group to synchronize. Defaults to None, in which case the world mesh group is used.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>ProcessGroup | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>def barrier(self, process_group: ProcessGroup | None = None):\n    \"\"\"\n    Synchronizes all processes in the specified process group.\n\n    This method blocks until all processes in the group reach this barrier.\n    If no process group is specified, the world mesh group is used by default.\n\n    Args:\n        process_group (ProcessGroup | None, optional): The process group to synchronize.\n            Defaults to None, in which case the world mesh group is used.\n    \"\"\"\n    if self.world_mesh is None:\n        raise RuntimeError(\"World mesh not yet initialized. Call `launch` first.\")\n\n    dist.barrier(group=process_group)\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.get_total_norm","title":"get_total_norm","text":"<pre><code>get_total_norm(parameters: Tensor | Iterable[Tensor], norm_type: float = 2.0, error_if_nonfinite: bool = False, foreach: bool | None = None, async_op: bool = False) -&gt; torch.Tensor\n</code></pre> <p>Get the total gradient norm of an iterable of parameters.</p> <p>Gradient norm clipping requires computing the gradient norm over the entire model. <code>torch.nn.utils.clip_grad_norm_</code> only computes gradient norm along DP/FSDP/TP dimensions. We need to manually reduce the gradient norm across PP stages.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> PARAMETER DESCRIPTION <code>parameters</code> <p>an iterable of Tensors or a single Tensor that will have gradients normalized</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>Tensor | Iterable[Tensor]</code> </p> <code>max_norm</code> <p>max norm of the gradients</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>float</code> </p> <code>norm_type</code> <p>type of the used p-norm. Can be <code>'inf'</code> for infinity norm.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>error_if_nonfinite</code> <p>if True, an error is thrown if the total norm of the gradients from :attr:<code>parameters</code> is <code>nan</code>, <code>inf</code>, or <code>-inf</code>. Default: False (will switch to True in the future)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>foreach</code> <p>use the faster foreach-based implementation. If <code>None</code>, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types. Default: <code>None</code></p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Total norm of the parameter gradients (viewed as a single vector).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>@torch.no_grad()\ndef get_total_norm(\n    self,\n    parameters: torch.Tensor | Iterable[torch.Tensor],\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False,\n    foreach: bool | None = None,\n    async_op: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Get the total gradient norm of an iterable of parameters.\n\n    Gradient norm clipping requires computing the gradient norm over the entire model.\n    `torch.nn.utils.clip_grad_norm_` only computes gradient norm along DP/FSDP/TP dimensions.\n    We need to manually reduce the gradient norm across PP stages.\n\n    Args:\n        parameters: an iterable of Tensors or a single Tensor that will have gradients normalized\n        max_norm (float): max norm of the gradients\n        norm_type (float): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n        error_if_nonfinite (bool): if True, an error is thrown if the total\n            norm of the gradients from :attr:`parameters` is ``nan``,\n            ``inf``, or ``-inf``. Default: False (will switch to True in the future)\n        foreach (bool): use the faster foreach-based implementation.\n            If ``None``, use the foreach implementation for CUDA and CPU native tensors and silently\n            fall back to the slow implementation for other device types.\n            Default: ``None``\n\n    Returns:\n        Total norm of the parameter gradients (viewed as a single vector).\n\n    \"\"\"\n    grads = [p.grad for p in parameters if p.grad is not None]\n    total_norm = torch.nn.utils.get_total_norm(\n        grads, norm_type, error_if_nonfinite, foreach\n    )\n\n    # If total_norm is a DTensor, the placements must be `torch.distributed._tensor.ops.math_ops._NormPartial`.\n    # We can simply reduce the DTensor to get the total norm in this tensor's process group\n    # and then convert it to a local tensor.\n    # NOTE: It has two purposes:\n    #       1. to make sure the total norm is computed correctly when PP is used (see below)\n    #       2. to return a reduced total_norm tensor whose .item() would return the correct value\n    if isinstance(total_norm, DTensor):\n        # Will reach here if any non-PP parallelism is used.\n        # If only using PP, total_norm will be a local tensor.\n        total_norm = total_norm.full_tensor()\n\n    if (pp_mesh := self.get_mesh(\"pp\")) is not None:\n        if math.isinf(norm_type):\n            dist.all_reduce(\n                total_norm,\n                op=dist.ReduceOp.MAX,\n                group=pp_mesh.get_group(),\n                async_op=async_op,\n            )\n        else:\n            total_norm **= norm_type\n            dist.all_reduce(\n                total_norm,\n                op=dist.ReduceOp.SUM,\n                group=pp_mesh.get_group(),\n                async_op=async_op,\n            )\n            total_norm **= 1.0 / norm_type\n\n    return total_norm\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.train_context","title":"train_context","text":"<pre><code>train_context()\n</code></pre> <p>Returns a context manager for training that sets up optional distributed context features.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>This context manager enables</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <ul> <li>Compiled autograd if configured (<code>self.config.enable_compiled_autograd</code>)</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> RETURNS DESCRIPTION <p>contextlib._GeneratorContextManager: A context manager that sets up the training context.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>@contextlib.contextmanager\ndef train_context(self):\n    \"\"\"\n    Returns a context manager for training that sets up optional distributed context features.\n\n    This context manager enables:\n        - Compiled autograd if configured (`self.config.enable_compiled_autograd`)\n\n    Returns:\n        contextlib._GeneratorContextManager: A context manager that sets up the training context.\n    \"\"\"\n\n    contexts = []\n    if self.config.enable_compiled_autograd:\n        contexts.append(torch._dynamo.utils.maybe_enable_compiled_autograd(True))\n\n    with stacked_context(contexts):\n        yield\n</code></pre>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.DistributedWorld.loss_parallel","title":"loss_parallel","text":"<pre><code>loss_parallel()\n</code></pre> <p>Context manager for loss parallelism.</p> <p>This context manager enables loss parallelism by setting up the appropriate distributed environment, but does nothing if tensor parallelism (tp) is not enabled.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> YIELDS DESCRIPTION <p>contextlib._GeneratorContextManager: A context manager that sets up the loss parallelism.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code> <pre><code>@contextlib.contextmanager\ndef loss_parallel(self):\n    \"\"\"\n    Context manager for loss parallelism.\n\n    This context manager enables loss parallelism by setting up the appropriate distributed environment,\n    but does nothing if tensor parallelism (tp) is not enabled.\n\n    Yields:\n        contextlib._GeneratorContextManager: A context manager that sets up the loss parallelism.\n    \"\"\"\n    with (\n        torch.distributed.tensor.parallel.loss_parallel()\n        if self.loss_parallel_enabled\n        else contextlib.nullcontext()\n    ):\n        yield\n</code></pre>"},{"location":"api/utilities/world/#properties","title":"Properties","text":"<ul> <li><code>world_size</code> - Total number of processes</li> <li><code>rank</code> - Current process rank</li> <li><code>local_rank</code> - Rank within the node</li> <li><code>device</code> - Current device (cuda:X or cpu)</li> <li><code>dp_size</code> - Data parallel world size</li> <li><code>dp_rank</code> - Data parallel rank</li> <li><code>tp_enabled</code> - Whether tensor parallelism is enabled</li> <li><code>pp_enabled</code> - Whether pipeline parallelism is enabled</li> <li><code>cp_enabled</code> - Whether context parallelism is enabled</li> </ul>"},{"location":"api/utilities/world/#usage-examples","title":"Usage Examples","text":""},{"location":"api/utilities/world/#basic-usage","title":"Basic Usage","text":"<pre><code>from dream_trainer.trainer.world import DistributedWorld\nfrom dream_trainer.configs import DeviceParameters\n\n# Create world for FSDP training\ndevice_params = DeviceParameters.FSDP()\nworld = DistributedWorld(device_params)\n\n# Setup distributed environment\nworld.setup()\n\n# Access world properties\nprint(f\"World size: {world.world_size}\")\nprint(f\"Current rank: {world.rank}\")\nprint(f\"Device: {world.device}\")\n</code></pre>"},{"location":"api/utilities/world/#collective-operations","title":"Collective Operations","text":"<pre><code># All-reduce a tensor across all processes\ntensor = torch.tensor([world.rank], device=world.device)\nreduced = world.all_reduce(tensor, op=\"sum\")\n# reduced will be sum of all ranks\n\n# All-gather tensors from all processes\ngathered = world.all_gather(tensor)\n# gathered will contain tensors from all ranks\n\n# Synchronize all processes\nworld.barrier()\n</code></pre>"},{"location":"api/utilities/world/#device-meshes","title":"Device Meshes","text":"<p>The world manages device meshes for different parallelism strategies:</p> <pre><code># Get specific mesh\ntp_mesh = world.get_mesh(\"tp\")  # Tensor parallel mesh\ndp_mesh = world.get_mesh(\"dp_shard\")  # Data parallel shard mesh\n\n# Check if parallelism is enabled\nif world.tp_enabled:\n    print(\"Using tensor parallelism\")\n\nif world.dp_shard_enabled:\n    print(\"Using FSDP sharding\")\n</code></pre>"},{"location":"api/utilities/world/#training-context","title":"Training Context","text":"<pre><code># Use the training context for mixed precision\nwith world.train_context():\n    # Training code here - automatically handles autocast\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n# Loss parallelism for TP\nwith world.loss_parallel():\n    # Loss computation with proper TP handling\n    loss = parallel_cross_entropy(logits, labels)\n</code></pre>"},{"location":"api/utilities/world/#faulttolerantworld","title":"FaultTolerantWorld","text":"<p>Extended world for fault-tolerant training with torchft:</p>"},{"location":"api/utilities/world/#dream_trainer.trainer.world.FaultTolerantWorld","title":"FaultTolerantWorld","text":"<pre><code>FaultTolerantWorld(config: DeviceParameters, ft_config: FaultToleranceParameters)\n</code></pre> Source code in <code>src/dream_trainer/trainer/world/fault_tolerant_world.py</code> <pre><code>def __init__(self, config: DeviceParameters, ft_config: FaultToleranceParameters):\n    super().__init__(config)\n\n    self.ft_config = ft_config\n    self.group_rank = dist_util.core.get_dist_local_rank()\n    self.group_size = dist_util.core.get_dist_local_world_size()\n\n    self.replica_id = f\"{self.ft_config.replica_prefix or 'ft'}_{self.group_rank}\"\n</code></pre>"},{"location":"api/utilities/world/#usage","title":"Usage","text":"<pre><code>from dream_trainer.trainer.world import FaultTolerantWorld\nfrom dream_trainer.configs import FaultToleranceParameters\n\nft_params = FaultToleranceParameters(\n    enable=True,\n    min_replica_size=2,\n    max_consecutive_failures=3\n)\n\nworld = FaultTolerantWorld(device_params, ft_params)\nworld.setup()\n\n# Access fault tolerance manager\nft_manager = world.ft_manager\n</code></pre>"},{"location":"api/utilities/world/#fsdp-configuration","title":"FSDP Configuration","text":"<p>The world provides FSDP configuration based on device parameters:</p> <pre><code># Get FSDP configuration\nfsdp_config = world.get_fsdp_config()\n\n# Configuration includes:\n# - Sharding strategy\n# - CPU offloading\n# - Auto-wrap policy\n# - Backward prefetch\n# - Forward prefetch\n\n# Get mixed precision policy\nmp_policy = world.get_fsdp_mp_policy()\n# Configures param_dtype and reduce_dtype\n</code></pre>"},{"location":"api/utilities/world/#gradient-operations","title":"Gradient Operations","text":"<pre><code># Compute total gradient norm across distributed parameters\nparameters = model.parameters()\ntotal_norm = world.get_total_norm(\n    parameters=parameters,\n    norm_type=2.0,\n    error_if_nonfinite=True\n)\n\nprint(f\"Gradient norm: {total_norm}\")\n</code></pre>"},{"location":"api/utilities/world/#advanced-features","title":"Advanced Features","text":""},{"location":"api/utilities/world/#custom-mesh-construction","title":"Custom Mesh Construction","text":"<pre><code># The world automatically constructs meshes based on config\n# Mesh dimensions: [pp, cp, dp_replicate, dp_shard, tp]\n\n# Example 4D mesh for HSDP + TP\n# pp=1, cp=1, dp_replicate=2, dp_shard=4, tp=8\n# Total world size = 1 * 1 * 2 * 4 * 8 = 64\n</code></pre>"},{"location":"api/utilities/world/#context-parallelism","title":"Context Parallelism","text":"<pre><code>if world.cp_enabled:\n    cp_mesh = world.get_mesh(\"cp\")\n    # Use for sequence parallelism\n    cp_rank = cp_mesh.get_local_rank()\n    cp_size = cp_mesh.size()\n</code></pre>"},{"location":"api/utilities/world/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<pre><code>if world.pp_enabled:\n    pp_mesh = world.get_mesh(\"pp\")\n    pp_rank = pp_mesh.get_local_rank()\n\n    # Check if this rank has first/last stage\n    has_first_stage = (pp_rank == 0)\n    has_last_stage = (pp_rank == pp_mesh.size() - 1)\n</code></pre>"},{"location":"api/utilities/world/#integration-with-trainer","title":"Integration with Trainer","text":"<p>The world is automatically created and managed by trainers:</p> <pre><code>class MyTrainer(BaseTrainer):\n    def __init__(self, config):\n        super().__init__(config)\n        # self.world is automatically created\n\n    def training_step(self, batch, batch_idx):\n        # Use world for distributed operations\n        if self.world.is_global_zero:\n            print(\"Running on main process\")\n\n        # Collective operations\n        loss = self.compute_loss(batch)\n        avg_loss = self.world.all_reduce(loss, op=\"mean\")\n\n        return {\"loss\": avg_loss}\n</code></pre>"},{"location":"api/utilities/world/#best-practices","title":"Best Practices","text":""},{"location":"api/utilities/world/#1-check-rank-for-io","title":"1. Check Rank for I/O","text":"<pre><code># Only perform I/O on rank 0\nif world.is_global_zero:\n    save_checkpoint(model)\n    write_logs(metrics)\n</code></pre>"},{"location":"api/utilities/world/#2-use-appropriate-contexts","title":"2. Use Appropriate Contexts","text":"<pre><code># Training with autocast\nwith world.train_context():\n    loss = training_step(batch)\n\n# Loss computation with TP\nif world.loss_parallel_enabled:\n    with world.loss_parallel():\n        loss = compute_loss(logits, labels)\n</code></pre>"},{"location":"api/utilities/world/#3-handle-device-placement","title":"3. Handle Device Placement","text":"<pre><code># Always use world.device for tensors\ntensor = torch.randn(10, 10, device=world.device)\n\n# Move model to device\nmodel = model.to(world.device)\n</code></pre>"},{"location":"api/utilities/world/#4-synchronize-when-needed","title":"4. Synchronize When Needed","text":"<pre><code># Ensure all processes are ready\nworld.barrier()\n\n# Synchronize metrics\nmetric = compute_metric()\nsynced_metric = world.all_reduce(metric, op=\"mean\")\n</code></pre>"},{"location":"api/utilities/world/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/utilities/world/#common-issues","title":"Common Issues","text":"<ol> <li>Timeout errors: Increase <code>comm.init_timeout_seconds</code></li> <li>NCCL errors: Check network configuration</li> <li>Device mismatch: Ensure all tensors use <code>world.device</code></li> <li>Rank mismatch: Verify WORLD_SIZE and RANK env vars</li> </ol>"},{"location":"api/utilities/world/#debugging","title":"Debugging","text":"<pre><code># Print debug info\nlogger.info(f\"Rank {world.rank}/{world.world_size}\")\nlogger.info(f\"Device: {world.device}\")\nlogger.info(f\"DP size: {world.dp_size}, TP size: {world.get_mesh('tp').size()}\")\n\n# Check mesh configuration\nlogger.info(f\"Mesh dims: {world.world_mesh.mesh_dim_names}\")\n</code></pre>"},{"location":"api/utilities/world/#see-also","title":"See Also","text":"<ul> <li>DeviceParameters - Parallelism configuration</li> <li>AbstractTrainer - Trainer integration</li> <li>Parallelism Guide - Parallelism strategies </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p>"}]}