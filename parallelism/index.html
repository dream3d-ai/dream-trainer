<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Composable distributed training framework built around PyTorch DTensor abstractions"><link href=https://dream3d.ai/trainer/parallelism/ rel=canonical><link href=../callbacks/ rel=prev><link href=../api/ rel=next><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.15"><title>Parallelism - dream-trainer</title><link rel=stylesheet href=../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../assets/_mkdocstrings.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#parallelism-guide class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=dream-trainer class="md-header__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> dream-trainer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Parallelism </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../installation/ class=md-tabs__link> Getting Started </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../configuration/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../tutorials/first-trainer.md class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../examples/vision.md class=md-tabs__link> Examples </a> </li> <li class=md-tabs__item> <a href=../api/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../contributing.md class=md-tabs__link> Community </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=dream-trainer class="md-nav__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../assets/logo.png alt=logo> </a> dream-trainer </label> <div class=md-nav__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../installation/ class=md-nav__link> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> User Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../configuration-mastery/ class=md-nav__link> <span class=md-ellipsis> Configuration Mastery </span> </a> </li> <li class=md-nav__item> <a href=../trainer-guide/ class=md-nav__link> <span class=md-ellipsis> Trainer Guide </span> </a> </li> <li class=md-nav__item> <a href=../callbacks/ class=md-nav__link> <span class=md-ellipsis> Callbacks </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Parallelism </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Parallelism </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#table-of-contents class=md-nav__link> <span class=md-ellipsis> Table of Contents </span> </a> </li> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#understanding-memory-and-batch-size-constraints class=md-nav__link> <span class=md-ellipsis> Understanding Memory and Batch Size Constraints </span> </a> <nav class=md-nav aria-label="Understanding Memory and Batch Size Constraints"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#memory-breakdown-during-training class=md-nav__link> <span class=md-ellipsis> Memory Breakdown During Training </span> </a> </li> <li class=md-nav__item> <a href=#the-science-of-batch-size-scaling class=md-nav__link> <span class=md-ellipsis> The Science of Batch Size Scaling </span> </a> </li> <li class=md-nav__item> <a href=#memory-reduction-techniques class=md-nav__link> <span class=md-ellipsis> Memory Reduction Techniques </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#parallelism-strategies class=md-nav__link> <span class=md-ellipsis> Parallelism Strategies </span> </a> <nav class=md-nav aria-label="Parallelism Strategies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#understanding-the-trade-offs class=md-nav__link> <span class=md-ellipsis> Understanding the Trade-offs </span> </a> </li> <li class=md-nav__item> <a href=#data-parallelism-the-foundation class=md-nav__link> <span class=md-ellipsis> Data Parallelism: The Foundation </span> </a> </li> <li class=md-nav__item> <a href=#tensor-parallelism-splitting-layers class=md-nav__link> <span class=md-ellipsis> Tensor Parallelism: Splitting Layers </span> </a> <nav class=md-nav aria-label="Tensor Parallelism: Splitting Layers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#column-partitioning class=md-nav__link> <span class=md-ellipsis> Column Partitioning </span> </a> </li> <li class=md-nav__item> <a href=#row-partitioning class=md-nav__link> <span class=md-ellipsis> Row Partitioning </span> </a> </li> <li class=md-nav__item> <a href=#optimizing-communication class=md-nav__link> <span class=md-ellipsis> Optimizing Communication </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#pipeline-parallelism-splitting-stages class=md-nav__link> <span class=md-ellipsis> Pipeline Parallelism: Splitting Stages </span> </a> </li> <li class=md-nav__item> <a href=#context-parallelism-splitting-sequences class=md-nav__link> <span class=md-ellipsis> Context Parallelism: Splitting Sequences </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#network-topology-considerations class=md-nav__link> <span class=md-ellipsis> Network Topology Considerations </span> </a> <nav class=md-nav aria-label="Network Topology Considerations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimal-parallelism-placement class=md-nav__link> <span class=md-ellipsis> Optimal Parallelism Placement </span> </a> </li> <li class=md-nav__item> <a href=#example-llama-31-405b-configuration class=md-nav__link> <span class=md-ellipsis> Example: Llama 3.1 405B Configuration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#basic-configuration class=md-nav__link> <span class=md-ellipsis> Basic Configuration </span> </a> </li> <li class=md-nav__item> <a href=#self-parallelizing-models-with-fsdp2_utils class=md-nav__link> <span class=md-ellipsis> Self-Parallelizing Models with fsdp2_utils </span> </a> <nav class=md-nav aria-label="Self-Parallelizing Models with fsdp2_utils"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-pattern-models-that-parallelize-themselves class=md-nav__link> <span class=md-ellipsis> The Pattern: Models That Parallelize Themselves </span> </a> </li> <li class=md-nav__item> <a href=#implementing-parallelism-in-submodules class=md-nav__link> <span class=md-ellipsis> Implementing Parallelism in Submodules </span> </a> </li> <li class=md-nav__item> <a href=#simplified-trainer-implementation class=md-nav__link> <span class=md-ellipsis> Simplified Trainer Implementation </span> </a> </li> <li class=md-nav__item> <a href=#advanced-heterogeneous-parallelism-for-multi-modal-models class=md-nav__link> <span class=md-ellipsis> Advanced: Heterogeneous Parallelism for Multi-Modal Models </span> </a> </li> <li class=md-nav__item> <a href=#benefits-of-self-parallelizing-models class=md-nav__link> <span class=md-ellipsis> Benefits of Self-Parallelizing Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#manual-parallelism-implementation class=md-nav__link> <span class=md-ellipsis> Manual Parallelism Implementation </span> </a> <nav class=md-nav aria-label="Manual Parallelism Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#manual-tensor-parallelism class=md-nav__link> <span class=md-ellipsis> Manual Tensor Parallelism </span> </a> </li> <li class=md-nav__item> <a href=#manual-fsdp2 class=md-nav__link> <span class=md-ellipsis> Manual FSDP2 </span> </a> </li> <li class=md-nav__item> <a href=#manual-pipeline-parallelism class=md-nav__link> <span class=md-ellipsis> Manual Pipeline Parallelism </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#combining-parallelism-strategies class=md-nav__link> <span class=md-ellipsis> Combining Parallelism Strategies </span> </a> <nav class=md-nav aria-label="Combining Parallelism Strategies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#real-world-examples class=md-nav__link> <span class=md-ellipsis> Real-World Examples </span> </a> <nav class=md-nav aria-label="Real-World Examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#small-model-1b-parameters class=md-nav__link> <span class=md-ellipsis> Small Model (&lt; 1B parameters) </span> </a> </li> <li class=md-nav__item> <a href=#medium-model-1b-70b-parameters class=md-nav__link> <span class=md-ellipsis> Medium Model (1B - 70B parameters) </span> </a> </li> <li class=md-nav__item> <a href=#large-model-70b-500b-parameters class=md-nav__link> <span class=md-ellipsis> Large Model (70B - 500B parameters) </span> </a> </li> <li class=md-nav__item> <a href=#extreme-scale-500b-parameters class=md-nav__link> <span class=md-ellipsis> Extreme Scale (500B+ parameters) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3d-parallelism-dp-tp-pp class=md-nav__link> <span class=md-ellipsis> 3D Parallelism (DP + TP + PP) </span> </a> </li> <li class=md-nav__item> <a href=#hsdp-hybrid-sharded-data-parallel class=md-nav__link> <span class=md-ellipsis> HSDP (Hybrid Sharded Data Parallel) </span> </a> </li> <li class=md-nav__item> <a href=#context-parallel-tensor-parallel class=md-nav__link> <span class=md-ellipsis> Context Parallel + Tensor Parallel </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#performance-optimization class=md-nav__link> <span class=md-ellipsis> Performance Optimization </span> </a> <nav class=md-nav aria-label="Performance Optimization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimizing-for-your-hardware class=md-nav__link> <span class=md-ellipsis> Optimizing for Your Hardware </span> </a> </li> <li class=md-nav__item> <a href=#1-fsdp-prefetching class=md-nav__link> <span class=md-ellipsis> 1. FSDP Prefetching </span> </a> </li> <li class=md-nav__item> <a href=#2-async-tensor-parallelism class=md-nav__link> <span class=md-ellipsis> 2. Async Tensor Parallelism </span> </a> </li> <li class=md-nav__item> <a href=#3-compiled-autograd class=md-nav__link> <span class=md-ellipsis> 3. Compiled Autograd </span> </a> </li> <li class=md-nav__item> <a href=#4-communication-optimization class=md-nav__link> <span class=md-ellipsis> 4. Communication Optimization </span> </a> </li> <li class=md-nav__item> <a href=#5-batch-size-optimization class=md-nav__link> <span class=md-ellipsis> 5. Batch Size Optimization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#debugging-tips class=md-nav__link> <span class=md-ellipsis> Debugging Tips </span> </a> <nav class=md-nav aria-label="Debugging Tips"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-verify-sharding class=md-nav__link> <span class=md-ellipsis> 1. Verify Sharding </span> </a> </li> <li class=md-nav__item> <a href=#2-monitor-memory-usage class=md-nav__link> <span class=md-ellipsis> 2. Monitor Memory Usage </span> </a> </li> <li class=md-nav__item> <a href=#3-profile-communication class=md-nav__link> <span class=md-ellipsis> 3. Profile Communication </span> </a> </li> <li class=md-nav__item> <a href=#4-test-incrementally class=md-nav__link> <span class=md-ellipsis> 4. Test Incrementally </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#best-practices class=md-nav__link> <span class=md-ellipsis> Best Practices </span> </a> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../debugging.md class=md-nav__link> <span class=md-ellipsis> Debugging </span> </a> </li> <li class=md-nav__item> <a href=../performance.md class=md-nav__link> <span class=md-ellipsis> Performance Tuning </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../tutorials/first-trainer.md class=md-nav__link> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../examples/vision.md class=md-nav__link> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../api/ class=md-nav__link> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../contributing.md class=md-nav__link> <span class=md-ellipsis> Community </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dream3d/dream-trainer/edit/main/dream-trainer/pages/docs/parallelism.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dream3d/dream-trainer/raw/main/dream-trainer/pages/docs/parallelism.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=parallelism-guide>Parallelism Guide<a class=headerlink href=#parallelism-guide title="Permanent link">&para;</a></h1> <p>Dream Trainer provides first-class support for all modern parallelism strategies in PyTorch. This guide covers everything from basic data parallelism to advanced self-parallelizing models using <code>fsdp2_utils</code>.</p> <h2 id=table-of-contents>Table of Contents<a class=headerlink href=#table-of-contents title="Permanent link">&para;</a></h2> <ul> <li><a href=#overview>Overview</a></li> <li><a href=#understanding-memory-and-batch-size-constraints>Understanding Memory and Batch Size Constraints</a></li> <li><a href=#parallelism-strategies>Parallelism Strategies</a></li> <li><a href=#network-topology-considerations>Network Topology Considerations</a></li> <li><a href=#basic-configuration>Basic Configuration</a></li> <li><a href=#self-parallelizing-models-with-fsdp2_utils>Self-Parallelizing Models with fsdp2_utils</a></li> <li><a href=#manual-parallelism-implementation>Manual Parallelism Implementation</a></li> <li><a href=#combining-parallelism-strategies>Combining Parallelism Strategies</a></li> <li><a href=#performance-optimization>Performance Optimization</a></li> <li><a href=#debugging-tips>Debugging Tips</a></li> </ul> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">&para;</a></h2> <p>Dream Trainer supports five types of parallelism, all built on PyTorch's DTensor infrastructure:</p> <ol> <li><strong>Data Parallelism (DP)</strong>: Split batches across devices</li> <li><code>dp_replicate</code>: Traditional DDP replication</li> <li> <p><code>dp_shard</code>: FSDP2 sharding</p> </li> <li> <p><strong>Tensor Parallelism (TP)</strong>: Split model layers across devices</p> </li> <li> <p><strong>Pipeline Parallelism (PP)</strong>: Split model stages across devices</p> </li> <li> <p><strong>Context Parallelism (CP)</strong>: Split sequence dimension for long contexts</p> </li> <li> <p><strong>Hybrid Strategies</strong>: Combine multiple types (e.g., HSDP = DDP + FSDP)</p> </li> </ol> <h2 id=understanding-memory-and-batch-size-constraints>Understanding Memory and Batch Size Constraints<a class=headerlink href=#understanding-memory-and-batch-size-constraints title="Permanent link">&para;</a></h2> <p>Before diving into parallelism strategies, it's crucial to understand what consumes memory during training and how batch size affects training dynamics.</p> <h3 id=memory-breakdown-during-training>Memory Breakdown During Training<a class=headerlink href=#memory-breakdown-during-training title="Permanent link">&para;</a></h3> <p>According to <a href=https://www.jeremyjordan.me/distributed-training/ >Jeremy Jordan's analysis</a>, training a model requires keeping several components in memory:</p> <ol> <li><strong>Model Parameters</strong>: The learnable weights (e.g., 405B parameters = ~810GB in FP16)</li> <li><strong>Optimizer States</strong>: </li> <li>SGD: Just parameters (1x memory)</li> <li>Adam/AdamW: Parameters + first moment + second moment (3x memory)</li> <li><strong>Model Activations</strong>: Intermediate values needed for backpropagation</li> <li>Scales with batch size and model architecture</li> <li>Can be the dominant memory consumer for large batches</li> <li><strong>Gradients</strong>: Same size as model parameters</li> <li><strong>Input Data</strong>: The actual batch being processed</li> </ol> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># Example memory calculation for a 7B parameter model</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>model_params</span> <span class=o>=</span> <span class=mf>7e9</span> <span class=o>*</span> <span class=mi>2</span>  <span class=c1># 14GB in FP16</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>optimizer_states</span> <span class=o>=</span> <span class=n>model_params</span> <span class=o>*</span> <span class=mi>3</span>  <span class=c1># 42GB for AdamW</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>gradients</span> <span class=o>=</span> <span class=n>model_params</span>  <span class=c1># 14GB</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=c1># Total: 70GB + activations + data</span>
</span></code></pre></div> <h3 id=the-science-of-batch-size-scaling>The Science of Batch Size Scaling<a class=headerlink href=#the-science-of-batch-size-scaling title="Permanent link">&para;</a></h3> <p>Not all batch sizes are created equal. As explained in <a href=https://www.jeremyjordan.me/distributed-training/ >"An Empirical Model of Large-Batch Training"</a>, there are two distinct regimes:</p> <ol> <li><strong>Perfect Scaling Regime</strong>: When batch size is small, you can double the batch size and double the learning rate to train in half the steps</li> <li><strong>Ineffective Scaling Regime</strong>: Beyond a critical batch size, increasing batch size provides diminishing returns</li> </ol> <p>The transition point (called the <strong>gradient noise scale</strong>) depends on your data and model. Importantly, this transition point <em>increases</em> during training, which is why models like Llama 3.1 progressively increase batch size:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># Llama 3.1 training schedule</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>initial_batch</span> <span class=o>=</span> <span class=mi>4</span><span class=n>M</span> <span class=n>tokens</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>after</span> <span class=mi>252</span><span class=n>M</span> <span class=n>tokens</span><span class=p>:</span> <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>8</span><span class=n>M</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=n>after</span> <span class=mf>2.87</span><span class=n>T</span> <span class=n>tokens</span><span class=p>:</span> <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>16</span><span class=n>M</span>
</span></code></pre></div> <h3 id=memory-reduction-techniques>Memory Reduction Techniques<a class=headerlink href=#memory-reduction-techniques title="Permanent link">&para;</a></h3> <p>When you hit memory limits on a single GPU, you have several options before resorting to parallelism:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># 1. Gradient Accumulation (trade compute for memory)</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>TrainingParameters</span><span class=p>(</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>    <span class=n>train_batch_size</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># Effective batch = 32</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=p>)</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=c1># 2. Activation Checkpointing (recompute vs store)</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>    <span class=n>checkpoint_activations</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># ~30% memory savings</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=p>)</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># 3. Mixed Precision (reduce precision)</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a>    <span class=n>param_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>  <span class=c1># 2x memory savings vs FP32</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a><span class=p>)</span>
</span></code></pre></div> <h2 id=parallelism-strategies>Parallelism Strategies<a class=headerlink href=#parallelism-strategies title="Permanent link">&para;</a></h2> <h3 id=understanding-the-trade-offs>Understanding the Trade-offs<a class=headerlink href=#understanding-the-trade-offs title="Permanent link">&para;</a></h3> <p>Each parallelism strategy makes different trade-offs between memory savings, communication overhead, and implementation complexity:</p> <table> <thead> <tr> <th>Strategy</th> <th>Memory Savings</th> <th>Communication</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td>DDP</td> <td>None</td> <td>All-reduce gradients (once per step)</td> <td>Small models, high bandwidth</td> </tr> <tr> <td>FSDP2</td> <td>High</td> <td>All-gather params + reduce-scatter grads</td> <td>Large models</td> </tr> <tr> <td>TP</td> <td>Medium</td> <td>All-reduce activations (multiple per layer)</td> <td>Wide models (large hidden dims)</td> </tr> <tr> <td>PP</td> <td>High</td> <td>Point-to-point activations</td> <td>Deep models (many layers)</td> </tr> <tr> <td>CP</td> <td>Medium</td> <td>All-to-all for attention</td> <td>Long sequences</td> </tr> </tbody> </table> <h3 id=data-parallelism-the-foundation>Data Parallelism: The Foundation<a class=headerlink href=#data-parallelism-the-foundation title="Permanent link">&para;</a></h3> <p>Data parallelism is the simplest and most common form of distributed training. Each GPU maintains a complete copy of the model and processes different batches:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Traditional DDP - each GPU has full model</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_replicate</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>  <span class=c1># 8 GPUs</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=c1># FSDP2 - shards model across GPUs</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_shard</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>  <span class=c1># 8 GPUs, ~8x memory reduction</span>
</span></code></pre></div> <p><strong>When to use DDP:</strong> - Model fits on single GPU - High-bandwidth interconnect available - Want simplest implementation</p> <p><strong>When to use FSDP2:</strong> - Model too large for single GPU - Need to train larger models with same hardware - Can tolerate some communication overhead</p> <h3 id=tensor-parallelism-splitting-layers>Tensor Parallelism: Splitting Layers<a class=headerlink href=#tensor-parallelism-splitting-layers title="Permanent link">&para;</a></h3> <p>Tensor parallelism splits individual layers across GPUs. As <a href=https://www.jeremyjordan.me/distributed-training/ >explained by Jeremy Jordan</a>, there are two main approaches:</p> <h4 id=column-partitioning>Column Partitioning<a class=headerlink href=#column-partitioning title="Permanent link">&para;</a></h4> <p>Splits weight matrix along output dimension:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Weight W: [input_dim, output_dim]</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=c1># Each GPU gets W_i: [input_dim, output_dim/n_gpus]</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=c1># Forward: X @ W_i → Y_i (then all-gather)</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=c1># Backward: Gradient flows naturally</span>
</span></code></pre></div> <h4 id=row-partitioning>Row Partitioning<a class=headerlink href=#row-partitioning title="Permanent link">&para;</a></h4> <p>Splits weight matrix along input dimension:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Weight W: [input_dim, output_dim]</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=c1># Each GPU gets W_i: [input_dim/n_gpus, output_dim]</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=c1># Forward: X_i @ W_i → partial Y (then all-reduce)</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=c1># Backward: Need all-gather for input gradients</span>
</span></code></pre></div> <h4 id=optimizing-communication>Optimizing Communication<a class=headerlink href=#optimizing-communication title="Permanent link">&para;</a></h4> <p>The Megatron-LM paper showed how clever partitioning reduces communication:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=k>class</span><span class=w> </span><span class=nc>OptimizedMLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>ParallelPlan</span><span class=p>):</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>    <span class=k>def</span><span class=w> </span><span class=nf>parallelize_plan</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>            <span class=c1># First layer: column partition (output split)</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>            <span class=s2>&quot;fc1&quot;</span><span class=p>:</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>),</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>            <span class=c1># Activation: computed locally on each GPU</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>            <span class=c1># Second layer: row partition (input split)</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>            <span class=c1># Takes split input directly - no communication!</span>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>            <span class=s2>&quot;fc2&quot;</span><span class=p>:</span> <span class=n>rowwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>),</span>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>            <span class=c1># Only one all-reduce at the end</span>
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a>        <span class=p>}</span>
</span></code></pre></div> <p>This pattern reduces communication by 50% compared to naive partitioning!</p> <h3 id=pipeline-parallelism-splitting-stages>Pipeline Parallelism: Splitting Stages<a class=headerlink href=#pipeline-parallelism-splitting-stages title="Permanent link">&para;</a></h3> <p>Pipeline parallelism splits the model into sequential stages, with each GPU responsible for a subset of layers:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Model split across 4 GPUs</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>GPU</span> <span class=mi>0</span><span class=p>:</span> <span class=n>Embedding</span> <span class=o>+</span> <span class=n>Layers</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>8</span><span class=p>]</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=n>GPU</span> <span class=mi>1</span><span class=p>:</span> <span class=n>Layers</span><span class=p>[</span><span class=mi>8</span><span class=p>:</span><span class=mi>16</span><span class=p>]</span>  
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=n>GPU</span> <span class=mi>2</span><span class=p>:</span> <span class=n>Layers</span><span class=p>[</span><span class=mi>16</span><span class=p>:</span><span class=mi>24</span><span class=p>]</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=n>GPU</span> <span class=mi>3</span><span class=p>:</span> <span class=n>Layers</span><span class=p>[</span><span class=mi>24</span><span class=p>:</span><span class=mi>32</span><span class=p>]</span> <span class=o>+</span> <span class=n>Output</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class=c1># Microbatching keeps GPUs busy</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=n>microbatch_size</span> <span class=o>=</span> <span class=n>batch_size</span> <span class=o>//</span> <span class=n>num_microbatches</span>
</span></code></pre></div> <p><strong>Advantages:</strong> - Each GPU only stores its layers (high memory savings) - Only forward/backward activations communicated - Works well across nodes with slower interconnect</p> <p><strong>Disadvantages:</strong> - Pipeline bubbles reduce efficiency - Requires careful load balancing - More complex implementation</p> <h3 id=context-parallelism-splitting-sequences>Context Parallelism: Splitting Sequences<a class=headerlink href=#context-parallelism-splitting-sequences title="Permanent link">&para;</a></h3> <p>For extremely long sequences, context parallelism splits the sequence dimension:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=c1># Sequence length 128K split across 4 GPUs</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=n>GPU</span> <span class=mi>0</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>32</span><span class=n>K</span><span class=p>]</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=n>GPU</span> <span class=mi>1</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=mi>32</span><span class=n>K</span><span class=p>:</span><span class=mi>64</span><span class=n>K</span><span class=p>]</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=n>GPU</span> <span class=mi>2</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=mi>64</span><span class=n>K</span><span class=p>:</span><span class=mi>96</span><span class=n>K</span><span class=p>]</span>  
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=n>GPU</span> <span class=mi>3</span><span class=p>:</span> <span class=n>tokens</span><span class=p>[</span><span class=mi>96</span><span class=n>K</span><span class=p>:</span><span class=mi>128</span><span class=n>K</span><span class=p>]</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=c1># Attention requires all-to-all communication</span>
</span></code></pre></div> <h2 id=network-topology-considerations>Network Topology Considerations<a class=headerlink href=#network-topology-considerations title="Permanent link">&para;</a></h2> <p>Modern GPU clusters have hierarchical network structures that significantly impact parallelism choices. As noted in the <a href=https://www.jeremyjordan.me/distributed-training/ >Llama 3.1 training details</a>:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>Within node: 8 GPUs connected via NVLink (900 GB/s)
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>Within rack: 2 nodes connected via network switch
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>Within pod: 192 racks (3,072 GPUs)
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>Full cluster: 8 pods (24,576 GPUs)
</span></code></pre></div> <h3 id=optimal-parallelism-placement>Optimal Parallelism Placement<a class=headerlink href=#optimal-parallelism-placement title="Permanent link">&para;</a></h3> <p>Based on network topology, place parallelism strategies hierarchically:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=c1># Optimal configuration for hierarchical networks</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>    <span class=c1># Within node (NVLink - highest bandwidth)</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>  <span class=c1># Requires frequent communication</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>    <span class=c1># Across nodes within rack (InfiniBand - medium bandwidth)  </span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>    <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>  <span class=c1># Only activations communicated</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a>    <span class=c1># Across racks (Ethernet - lowest bandwidth)</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>  <span class=c1># Only gradient sync required</span>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a><span class=p>)</span>
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a>
</span><span id=__span-10-13><a id=__codelineno-10-13 name=__codelineno-10-13 href=#__codelineno-10-13></a><span class=c1># Total: 8 * 2 * 16 = 256 GPUs</span>
</span></code></pre></div> <h3 id=example-llama-31-405b-configuration>Example: Llama 3.1 405B Configuration<a class=headerlink href=#example-llama-31-405b-configuration title="Permanent link">&para;</a></h3> <p>The Llama 3.1 training used a carefully optimized setup:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=c1># Llama 3.1 405B parallelism</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>tensor_parallel</span> <span class=o>=</span> <span class=mi>8</span>      <span class=c1># Within node (NVLink)</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=n>pipeline_parallel</span> <span class=o>=</span> <span class=mi>16</span>   <span class=c1># Across nodes</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=n>data_parallel</span> <span class=o>=</span> <span class=mi>128</span>      <span class=c1># Across everything</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=c1># Total: 16,384 GPUs</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=c1># Memory per GPU: ~50GB (405B params / 8 TP / 16 PP)</span>
</span></code></pre></div> <h2 id=basic-configuration>Basic Configuration<a class=headerlink href=#basic-configuration title="Permanent link">&para;</a></h2> <p>Configure parallelism through <code>DeviceParameters</code>:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.configs</span><span class=w> </span><span class=kn>import</span> <span class=n>DeviceParameters</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>    <span class=c1># Data parallelism</span>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    <span class=n>dp_replicate</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>      <span class=c1># 2-way DDP</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>          <span class=c1># 4-way FSDP within each DDP group</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>    <span class=c1># Model parallelism</span>
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>   <span class=c1># 2-way TP</span>
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>    <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=c1># 4 pipeline stages</span>
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>    <span class=n>context_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>  <span class=c1># 2-way sequence splitting</span>
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>
</span><span id=__span-12-13><a id=__codelineno-12-13 name=__codelineno-12-13 href=#__codelineno-12-13></a>    <span class=c1># Optimizations</span>
</span><span id=__span-12-14><a id=__codelineno-12-14 name=__codelineno-12-14 href=#__codelineno-12-14></a>    <span class=n>compile_model</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-12-15><a id=__codelineno-12-15 name=__codelineno-12-15 href=#__codelineno-12-15></a>    <span class=n>loss_parallel</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># Parallel loss with TP</span>
</span><span id=__span-12-16><a id=__codelineno-12-16 name=__codelineno-12-16 href=#__codelineno-12-16></a>    <span class=n>async_tensor_parallel</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># Overlap TP communication</span>
</span><span id=__span-12-17><a id=__codelineno-12-17 name=__codelineno-12-17 href=#__codelineno-12-17></a><span class=p>)</span>
</span></code></pre></div> <h2 id=self-parallelizing-models-with-fsdp2_utils>Self-Parallelizing Models with fsdp2_utils<a class=headerlink href=#self-parallelizing-models-with-fsdp2_utils title="Permanent link">&para;</a></h2> <p>The most elegant approach to parallelism in Dream Trainer is using <code>fsdp2_utils</code> to create models that know how to parallelize themselves. This encapsulates complex sharding logic within the model definition.</p> <h3 id=the-pattern-models-that-parallelize-themselves>The Pattern: Models That Parallelize Themselves<a class=headerlink href=#the-pattern-models-that-parallelize-themselves title="Permanent link">&para;</a></h3> <p>Instead of implementing parallelism in the trainer, models inherit from <code>FullyShard</code> and <code>ParallelPlan</code>:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>Any</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=kn>from</span><span class=w> </span><span class=nn>fsdp2_utils</span><span class=w> </span><span class=kn>import</span> <span class=n>FullyShard</span><span class=p>,</span> <span class=n>ParallelPlan</span><span class=p>,</span> <span class=n>apply_tensor_parallel</span><span class=p>,</span> <span class=n>apply_fully_shard</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=kn>from</span><span class=w> </span><span class=nn>fsdp2_utils.tensor_parallel.plan</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>    <span class=n>colwise_parallel</span><span class=p>,</span> 
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a>    <span class=n>rowwise_parallel</span><span class=p>,</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a>    <span class=n>sequence_parallel</span><span class=p>,</span>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a>    <span class=n>prepare_module_input</span>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a><span class=p>)</span>
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.fsdp</span><span class=w> </span><span class=kn>import</span> <span class=n>fully_shard</span>
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.tensor.placement_types</span><span class=w> </span><span class=kn>import</span> <span class=n>Replicate</span><span class=p>,</span> <span class=n>Shard</span>
</span><span id=__span-13-12><a id=__codelineno-13-12 name=__codelineno-13-12 href=#__codelineno-13-12></a>
</span><span id=__span-13-13><a id=__codelineno-13-13 name=__codelineno-13-13 href=#__codelineno-13-13></a><span class=k>class</span><span class=w> </span><span class=nc>TransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>FullyShard</span><span class=p>,</span> <span class=n>ParallelPlan</span><span class=p>):</span>
</span><span id=__span-13-14><a id=__codelineno-13-14 name=__codelineno-13-14 href=#__codelineno-13-14></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;A transformer that knows how to parallelize itself&quot;&quot;&quot;</span>
</span><span id=__span-13-15><a id=__codelineno-13-15 name=__codelineno-13-15 href=#__codelineno-13-15></a>
</span><span id=__span-13-16><a id=__codelineno-13-16 name=__codelineno-13-16 href=#__codelineno-13-16></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span><span id=__span-13-17><a id=__codelineno-13-17 name=__codelineno-13-17 href=#__codelineno-13-17></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-13-18><a id=__codelineno-13-18 name=__codelineno-13-18 href=#__codelineno-13-18></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span><span id=__span-13-19><a id=__codelineno-13-19 name=__codelineno-13-19 href=#__codelineno-13-19></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span><span id=__span-13-20><a id=__codelineno-13-20 name=__codelineno-13-20 href=#__codelineno-13-20></a>            <span class=n>TransformerLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>num_layers</span><span class=p>)</span>
</span><span id=__span-13-21><a id=__codelineno-13-21 name=__codelineno-13-21 href=#__codelineno-13-21></a>        <span class=p>])</span>
</span><span id=__span-13-22><a id=__codelineno-13-22 name=__codelineno-13-22 href=#__codelineno-13-22></a>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>RMSNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span><span id=__span-13-23><a id=__codelineno-13-23 name=__codelineno-13-23 href=#__codelineno-13-23></a>        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-13-24><a id=__codelineno-13-24 name=__codelineno-13-24 href=#__codelineno-13-24></a>
</span><span id=__span-13-25><a id=__codelineno-13-25 name=__codelineno-13-25 href=#__codelineno-13-25></a>    <span class=k>def</span><span class=w> </span><span class=nf>fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]):</span>
</span><span id=__span-13-26><a id=__codelineno-13-26 name=__codelineno-13-26 href=#__codelineno-13-26></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Define FSDP sharding strategy&quot;&quot;&quot;</span>
</span><span id=__span-13-27><a id=__codelineno-13-27 name=__codelineno-13-27 href=#__codelineno-13-27></a>        <span class=c1># Shard each transformer layer independently</span>
</span><span id=__span-13-28><a id=__codelineno-13-28 name=__codelineno-13-28 href=#__codelineno-13-28></a>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span><span id=__span-13-29><a id=__codelineno-13-29 name=__codelineno-13-29 href=#__codelineno-13-29></a>            <span class=n>fully_shard</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-13-30><a id=__codelineno-13-30 name=__codelineno-13-30 href=#__codelineno-13-30></a>
</span><span id=__span-13-31><a id=__codelineno-13-31 name=__codelineno-13-31 href=#__codelineno-13-31></a>        <span class=c1># Shard embeddings and output separately</span>
</span><span id=__span-13-32><a id=__codelineno-13-32 name=__codelineno-13-32 href=#__codelineno-13-32></a>        <span class=n>fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-13-33><a id=__codelineno-13-33 name=__codelineno-13-33 href=#__codelineno-13-33></a>        <span class=n>fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-13-34><a id=__codelineno-13-34 name=__codelineno-13-34 href=#__codelineno-13-34></a>
</span><span id=__span-13-35><a id=__codelineno-13-35 name=__codelineno-13-35 href=#__codelineno-13-35></a>    <span class=k>def</span><span class=w> </span><span class=nf>parallelize_plan</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-13-36><a id=__codelineno-13-36 name=__codelineno-13-36 href=#__codelineno-13-36></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Define tensor parallel strategy&quot;&quot;&quot;</span>
</span><span id=__span-13-37><a id=__codelineno-13-37 name=__codelineno-13-37 href=#__codelineno-13-37></a>        <span class=n>plan</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-13-38><a id=__codelineno-13-38 name=__codelineno-13-38 href=#__codelineno-13-38></a>            <span class=c1># Embeddings: row-wise parallel (vocab dimension)</span>
</span><span id=__span-13-39><a id=__codelineno-13-39 name=__codelineno-13-39 href=#__codelineno-13-39></a>            <span class=s2>&quot;embeddings&quot;</span><span class=p>:</span> <span class=n>rowwise_parallel</span><span class=p>(</span>
</span><span id=__span-13-40><a id=__codelineno-13-40 name=__codelineno-13-40 href=#__codelineno-13-40></a>                <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>,</span>
</span><span id=__span-13-41><a id=__codelineno-13-41 name=__codelineno-13-41 href=#__codelineno-13-41></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Replicate</span><span class=p>(),</span>  <span class=c1># Input tokens are replicated</span>
</span><span id=__span-13-42><a id=__codelineno-13-42 name=__codelineno-13-42 href=#__codelineno-13-42></a>                <span class=n>output_layouts</span><span class=o>=</span><span class=n>Shard</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>    <span class=c1># Output is sharded on sequence dim</span>
</span><span id=__span-13-43><a id=__codelineno-13-43 name=__codelineno-13-43 href=#__codelineno-13-43></a>            <span class=p>),</span>
</span><span id=__span-13-44><a id=__codelineno-13-44 name=__codelineno-13-44 href=#__codelineno-13-44></a>
</span><span id=__span-13-45><a id=__codelineno-13-45 name=__codelineno-13-45 href=#__codelineno-13-45></a>            <span class=c1># Final layer norm: sequence parallel</span>
</span><span id=__span-13-46><a id=__codelineno-13-46 name=__codelineno-13-46 href=#__codelineno-13-46></a>            <span class=s2>&quot;norm&quot;</span><span class=p>:</span> <span class=n>sequence_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>),</span>
</span><span id=__span-13-47><a id=__codelineno-13-47 name=__codelineno-13-47 href=#__codelineno-13-47></a>
</span><span id=__span-13-48><a id=__codelineno-13-48 name=__codelineno-13-48 href=#__codelineno-13-48></a>            <span class=c1># LM head: column-wise parallel (vocab dimension)</span>
</span><span id=__span-13-49><a id=__codelineno-13-49 name=__codelineno-13-49 href=#__codelineno-13-49></a>            <span class=s2>&quot;lm_head&quot;</span><span class=p>:</span> <span class=n>colwise_parallel</span><span class=p>(</span>
</span><span id=__span-13-50><a id=__codelineno-13-50 name=__codelineno-13-50 href=#__codelineno-13-50></a>                <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>,</span>
</span><span id=__span-13-51><a id=__codelineno-13-51 name=__codelineno-13-51 href=#__codelineno-13-51></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Shard</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>     <span class=c1># Input is sharded</span>
</span><span id=__span-13-52><a id=__codelineno-13-52 name=__codelineno-13-52 href=#__codelineno-13-52></a>                <span class=n>output_layouts</span><span class=o>=</span><span class=n>Replicate</span><span class=p>(),</span> <span class=c1># Output is replicated</span>
</span><span id=__span-13-53><a id=__codelineno-13-53 name=__codelineno-13-53 href=#__codelineno-13-53></a>                <span class=n>use_local_output</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-13-54><a id=__codelineno-13-54 name=__codelineno-13-54 href=#__codelineno-13-54></a>            <span class=p>),</span>
</span><span id=__span-13-55><a id=__codelineno-13-55 name=__codelineno-13-55 href=#__codelineno-13-55></a>        <span class=p>}</span>
</span><span id=__span-13-56><a id=__codelineno-13-56 name=__codelineno-13-56 href=#__codelineno-13-56></a>
</span><span id=__span-13-57><a id=__codelineno-13-57 name=__codelineno-13-57 href=#__codelineno-13-57></a>        <span class=c1># Add plans for each transformer layer</span>
</span><span id=__span-13-58><a id=__codelineno-13-58 name=__codelineno-13-58 href=#__codelineno-13-58></a>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
</span><span id=__span-13-59><a id=__codelineno-13-59 name=__codelineno-13-59 href=#__codelineno-13-59></a>            <span class=n>plan</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>layer</span><span class=o>.</span><span class=n>parallelize_plan</span><span class=p>(</span><span class=n>prefix</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;layers.</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>))</span>
</span><span id=__span-13-60><a id=__codelineno-13-60 name=__codelineno-13-60 href=#__codelineno-13-60></a>
</span><span id=__span-13-61><a id=__codelineno-13-61 name=__codelineno-13-61 href=#__codelineno-13-61></a>        <span class=k>return</span> <span class=n>plan</span>
</span></code></pre></div> <h3 id=implementing-parallelism-in-submodules>Implementing Parallelism in Submodules<a class=headerlink href=#implementing-parallelism-in-submodules title="Permanent link">&para;</a></h3> <p>Each component defines its own parallelism strategy:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=k>class</span><span class=w> </span><span class=nc>TransformerLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>ParallelPlan</span><span class=p>):</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>RMSNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>RMSNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a>
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a>    <span class=k>def</span><span class=w> </span><span class=nf>parallelize_plan</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>prefix</span><span class=o>=</span><span class=s2>&quot;&quot;</span><span class=p>):</span>
</span><span id=__span-14-10><a id=__codelineno-14-10 name=__codelineno-14-10 href=#__codelineno-14-10></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Define TP plan for this layer&quot;&quot;&quot;</span>
</span><span id=__span-14-11><a id=__codelineno-14-11 name=__codelineno-14-11 href=#__codelineno-14-11></a>        <span class=k>def</span><span class=w> </span><span class=nf>add_prefix</span><span class=p>(</span><span class=n>name</span><span class=p>):</span>
</span><span id=__span-14-12><a id=__codelineno-14-12 name=__codelineno-14-12 href=#__codelineno-14-12></a>            <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>prefix</span><span class=si>}</span><span class=s2>.</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>&quot;</span> <span class=k>if</span> <span class=n>prefix</span> <span class=k>else</span> <span class=n>name</span>
</span><span id=__span-14-13><a id=__codelineno-14-13 name=__codelineno-14-13 href=#__codelineno-14-13></a>
</span><span id=__span-14-14><a id=__codelineno-14-14 name=__codelineno-14-14 href=#__codelineno-14-14></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-14-15><a id=__codelineno-14-15 name=__codelineno-14-15 href=#__codelineno-14-15></a>            <span class=c1># Attention: QKV column-wise, O row-wise</span>
</span><span id=__span-14-16><a id=__codelineno-14-16 name=__codelineno-14-16 href=#__codelineno-14-16></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;attention.q_proj&quot;</span><span class=p>):</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>q_proj</span><span class=p>),</span>
</span><span id=__span-14-17><a id=__codelineno-14-17 name=__codelineno-14-17 href=#__codelineno-14-17></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;attention.k_proj&quot;</span><span class=p>):</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>k_proj</span><span class=p>),</span>
</span><span id=__span-14-18><a id=__codelineno-14-18 name=__codelineno-14-18 href=#__codelineno-14-18></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;attention.v_proj&quot;</span><span class=p>):</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>v_proj</span><span class=p>),</span>
</span><span id=__span-14-19><a id=__codelineno-14-19 name=__codelineno-14-19 href=#__codelineno-14-19></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;attention.o_proj&quot;</span><span class=p>):</span> <span class=n>rowwise_parallel</span><span class=p>(</span>
</span><span id=__span-14-20><a id=__codelineno-14-20 name=__codelineno-14-20 href=#__codelineno-14-20></a>                <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>o_proj</span><span class=p>,</span>
</span><span id=__span-14-21><a id=__codelineno-14-21 name=__codelineno-14-21 href=#__codelineno-14-21></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Shard</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span>  <span class=c1># Sharded on head dimension</span>
</span><span id=__span-14-22><a id=__codelineno-14-22 name=__codelineno-14-22 href=#__codelineno-14-22></a>            <span class=p>),</span>
</span><span id=__span-14-23><a id=__codelineno-14-23 name=__codelineno-14-23 href=#__codelineno-14-23></a>
</span><span id=__span-14-24><a id=__codelineno-14-24 name=__codelineno-14-24 href=#__codelineno-14-24></a>            <span class=c1># MLP: standard Megatron-style parallelism</span>
</span><span id=__span-14-25><a id=__codelineno-14-25 name=__codelineno-14-25 href=#__codelineno-14-25></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;mlp.gate_proj&quot;</span><span class=p>):</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=o>.</span><span class=n>gate_proj</span><span class=p>),</span>
</span><span id=__span-14-26><a id=__codelineno-14-26 name=__codelineno-14-26 href=#__codelineno-14-26></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;mlp.up_proj&quot;</span><span class=p>):</span> <span class=n>colwise_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=o>.</span><span class=n>up_proj</span><span class=p>),</span>
</span><span id=__span-14-27><a id=__codelineno-14-27 name=__codelineno-14-27 href=#__codelineno-14-27></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;mlp.down_proj&quot;</span><span class=p>):</span> <span class=n>rowwise_parallel</span><span class=p>(</span>
</span><span id=__span-14-28><a id=__codelineno-14-28 name=__codelineno-14-28 href=#__codelineno-14-28></a>                <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=o>.</span><span class=n>down_proj</span><span class=p>,</span>
</span><span id=__span-14-29><a id=__codelineno-14-29 name=__codelineno-14-29 href=#__codelineno-14-29></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Shard</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span>  <span class=c1># Sharded on hidden dimension</span>
</span><span id=__span-14-30><a id=__codelineno-14-30 name=__codelineno-14-30 href=#__codelineno-14-30></a>            <span class=p>),</span>
</span><span id=__span-14-31><a id=__codelineno-14-31 name=__codelineno-14-31 href=#__codelineno-14-31></a>
</span><span id=__span-14-32><a id=__codelineno-14-32 name=__codelineno-14-32 href=#__codelineno-14-32></a>            <span class=c1># Layer norms use sequence parallelism</span>
</span><span id=__span-14-33><a id=__codelineno-14-33 name=__codelineno-14-33 href=#__codelineno-14-33></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;norm1&quot;</span><span class=p>):</span> <span class=n>sequence_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>),</span>
</span><span id=__span-14-34><a id=__codelineno-14-34 name=__codelineno-14-34 href=#__codelineno-14-34></a>            <span class=n>add_prefix</span><span class=p>(</span><span class=s2>&quot;norm2&quot;</span><span class=p>):</span> <span class=n>sequence_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>),</span>
</span><span id=__span-14-35><a id=__codelineno-14-35 name=__codelineno-14-35 href=#__codelineno-14-35></a>        <span class=p>}</span>
</span></code></pre></div> <h3 id=simplified-trainer-implementation>Simplified Trainer Implementation<a class=headerlink href=#simplified-trainer-implementation title="Permanent link">&para;</a></h3> <p>With self-parallelizing models, trainers become remarkably simple:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=kn>from</span><span class=w> </span><span class=nn>fsdp2_utils</span><span class=w> </span><span class=kn>import</span> <span class=n>apply_tensor_parallel</span><span class=p>,</span> <span class=n>apply_fully_shard</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>DreamTrainer</span><span class=p>):</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_models</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>        <span class=c1># Model knows how to parallelize itself!</span>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>TransformerModel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a>    <span class=k>def</span><span class=w> </span><span class=nf>apply_tensor_parallel</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tp_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span><span class=p>):</span>
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Just delegate to the model&#39;s built-in TP strategy&quot;&quot;&quot;</span>
</span><span id=__span-15-10><a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a>        <span class=n>apply_tensor_parallel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=n>tp_mesh</span><span class=p>)</span>
</span><span id=__span-15-11><a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a>
</span><span id=__span-15-12><a id=__codelineno-15-12 name=__codelineno-15-12 href=#__codelineno-15-12></a>    <span class=k>def</span><span class=w> </span><span class=nf>apply_fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]):</span>
</span><span id=__span-15-13><a id=__codelineno-15-13 name=__codelineno-15-13 href=#__codelineno-15-13></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Just delegate to the model&#39;s built-in FSDP strategy&quot;&quot;&quot;</span>
</span><span id=__span-15-14><a id=__codelineno-15-14 name=__codelineno-15-14 href=#__codelineno-15-14></a>        <span class=n>apply_fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=n>config</span><span class=p>)</span>
</span></code></pre></div> <h3 id=advanced-heterogeneous-parallelism-for-multi-modal-models>Advanced: Heterogeneous Parallelism for Multi-Modal Models<a class=headerlink href=#advanced-heterogeneous-parallelism-for-multi-modal-models title="Permanent link">&para;</a></h3> <p>Different model components can use different strategies:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=k>class</span><span class=w> </span><span class=nc>VisionLanguageModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>FullyShard</span><span class=p>,</span> <span class=n>ParallelPlan</span><span class=p>):</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Multi-modal model with component-specific parallelism&quot;&quot;&quot;</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>vision_encoder</span> <span class=o>=</span> <span class=n>VisionTransformer</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vision</span><span class=p>)</span>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>text_encoder</span> <span class=o>=</span> <span class=n>TextTransformer</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>cross_attention</span> <span class=o>=</span> <span class=n>CrossModalAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-16-9><a id=__codelineno-16-9 name=__codelineno-16-9 href=#__codelineno-16-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>projection</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>output_size</span><span class=p>)</span>
</span><span id=__span-16-10><a id=__codelineno-16-10 name=__codelineno-16-10 href=#__codelineno-16-10></a>
</span><span id=__span-16-11><a id=__codelineno-16-11 name=__codelineno-16-11 href=#__codelineno-16-11></a>    <span class=k>def</span><span class=w> </span><span class=nf>fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]):</span>
</span><span id=__span-16-12><a id=__codelineno-16-12 name=__codelineno-16-12 href=#__codelineno-16-12></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Different sharding for different components&quot;&quot;&quot;</span>
</span><span id=__span-16-13><a id=__codelineno-16-13 name=__codelineno-16-13 href=#__codelineno-16-13></a>        <span class=c1># Vision: larger sharding units (less communication)</span>
</span><span id=__span-16-14><a id=__codelineno-16-14 name=__codelineno-16-14 href=#__codelineno-16-14></a>        <span class=n>vision_config</span> <span class=o>=</span> <span class=p>{</span><span class=o>**</span><span class=n>config</span><span class=p>,</span> <span class=s2>&quot;min_num_params_per_shard&quot;</span><span class=p>:</span> <span class=mi>50_000_000</span><span class=p>}</span>
</span><span id=__span-16-15><a id=__codelineno-16-15 name=__codelineno-16-15 href=#__codelineno-16-15></a>        <span class=bp>self</span><span class=o>.</span><span class=n>vision_encoder</span><span class=o>.</span><span class=n>fully_shard</span><span class=p>(</span><span class=n>vision_config</span><span class=p>)</span>
</span><span id=__span-16-16><a id=__codelineno-16-16 name=__codelineno-16-16 href=#__codelineno-16-16></a>
</span><span id=__span-16-17><a id=__codelineno-16-17 name=__codelineno-16-17 href=#__codelineno-16-17></a>        <span class=c1># Text: standard sharding</span>
</span><span id=__span-16-18><a id=__codelineno-16-18 name=__codelineno-16-18 href=#__codelineno-16-18></a>        <span class=bp>self</span><span class=o>.</span><span class=n>text_encoder</span><span class=o>.</span><span class=n>fully_shard</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-16-19><a id=__codelineno-16-19 name=__codelineno-16-19 href=#__codelineno-16-19></a>
</span><span id=__span-16-20><a id=__codelineno-16-20 name=__codelineno-16-20 href=#__codelineno-16-20></a>        <span class=c1># Cross-attention: aggressive sharding (memory intensive)</span>
</span><span id=__span-16-21><a id=__codelineno-16-21 name=__codelineno-16-21 href=#__codelineno-16-21></a>        <span class=n>cross_config</span> <span class=o>=</span> <span class=p>{</span><span class=o>**</span><span class=n>config</span><span class=p>,</span> <span class=s2>&quot;min_num_params_per_shard&quot;</span><span class=p>:</span> <span class=mi>10_000_000</span><span class=p>}</span>
</span><span id=__span-16-22><a id=__codelineno-16-22 name=__codelineno-16-22 href=#__codelineno-16-22></a>        <span class=n>fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cross_attention</span><span class=p>,</span> <span class=o>**</span><span class=n>cross_config</span><span class=p>)</span>
</span><span id=__span-16-23><a id=__codelineno-16-23 name=__codelineno-16-23 href=#__codelineno-16-23></a>
</span><span id=__span-16-24><a id=__codelineno-16-24 name=__codelineno-16-24 href=#__codelineno-16-24></a>    <span class=k>def</span><span class=w> </span><span class=nf>parallelize_plan</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-16-25><a id=__codelineno-16-25 name=__codelineno-16-25 href=#__codelineno-16-25></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Different TP strategies for different modalities&quot;&quot;&quot;</span>
</span><span id=__span-16-26><a id=__codelineno-16-26 name=__codelineno-16-26 href=#__codelineno-16-26></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-16-27><a id=__codelineno-16-27 name=__codelineno-16-27 href=#__codelineno-16-27></a>            <span class=c1># Vision doesn&#39;t use TP (compute bound, not memory bound)</span>
</span><span id=__span-16-28><a id=__codelineno-16-28 name=__codelineno-16-28 href=#__codelineno-16-28></a>            <span class=s2>&quot;vision_encoder&quot;</span><span class=p>:</span> <span class=n>prepare_module_input</span><span class=p>(</span>
</span><span id=__span-16-29><a id=__codelineno-16-29 name=__codelineno-16-29 href=#__codelineno-16-29></a>                <span class=bp>self</span><span class=o>.</span><span class=n>vision_encoder</span><span class=p>,</span>
</span><span id=__span-16-30><a id=__codelineno-16-30 name=__codelineno-16-30 href=#__codelineno-16-30></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Replicate</span><span class=p>(),</span>
</span><span id=__span-16-31><a id=__codelineno-16-31 name=__codelineno-16-31 href=#__codelineno-16-31></a>            <span class=p>),</span>
</span><span id=__span-16-32><a id=__codelineno-16-32 name=__codelineno-16-32 href=#__codelineno-16-32></a>
</span><span id=__span-16-33><a id=__codelineno-16-33 name=__codelineno-16-33 href=#__codelineno-16-33></a>            <span class=c1># Text uses full TP</span>
</span><span id=__span-16-34><a id=__codelineno-16-34 name=__codelineno-16-34 href=#__codelineno-16-34></a>            <span class=o>**</span><span class=p>{</span><span class=sa>f</span><span class=s2>&quot;text_encoder.</span><span class=si>{</span><span class=n>k</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>:</span> <span class=n>v</span> 
</span><span id=__span-16-35><a id=__codelineno-16-35 name=__codelineno-16-35 href=#__codelineno-16-35></a>               <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>text_encoder</span><span class=o>.</span><span class=n>parallelize_plan</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()},</span>
</span><span id=__span-16-36><a id=__codelineno-16-36 name=__codelineno-16-36 href=#__codelineno-16-36></a>
</span><span id=__span-16-37><a id=__codelineno-16-37 name=__codelineno-16-37 href=#__codelineno-16-37></a>            <span class=c1># Cross-attention uses custom strategy</span>
</span><span id=__span-16-38><a id=__codelineno-16-38 name=__codelineno-16-38 href=#__codelineno-16-38></a>            <span class=s2>&quot;cross_attention.q_proj&quot;</span><span class=p>:</span> <span class=n>colwise_parallel</span><span class=p>(</span>
</span><span id=__span-16-39><a id=__codelineno-16-39 name=__codelineno-16-39 href=#__codelineno-16-39></a>                <span class=bp>self</span><span class=o>.</span><span class=n>cross_attention</span><span class=o>.</span><span class=n>q_proj</span><span class=p>,</span>
</span><span id=__span-16-40><a id=__codelineno-16-40 name=__codelineno-16-40 href=#__codelineno-16-40></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Replicate</span><span class=p>(),</span>  <span class=c1># From vision</span>
</span><span id=__span-16-41><a id=__codelineno-16-41 name=__codelineno-16-41 href=#__codelineno-16-41></a>            <span class=p>),</span>
</span><span id=__span-16-42><a id=__codelineno-16-42 name=__codelineno-16-42 href=#__codelineno-16-42></a>            <span class=s2>&quot;cross_attention.kv_proj&quot;</span><span class=p>:</span> <span class=n>colwise_parallel</span><span class=p>(</span>
</span><span id=__span-16-43><a id=__codelineno-16-43 name=__codelineno-16-43 href=#__codelineno-16-43></a>                <span class=bp>self</span><span class=o>.</span><span class=n>cross_attention</span><span class=o>.</span><span class=n>kv_proj</span><span class=p>,</span>
</span><span id=__span-16-44><a id=__codelineno-16-44 name=__codelineno-16-44 href=#__codelineno-16-44></a>                <span class=n>input_layouts</span><span class=o>=</span><span class=n>Shard</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>     <span class=c1># From text</span>
</span><span id=__span-16-45><a id=__codelineno-16-45 name=__codelineno-16-45 href=#__codelineno-16-45></a>            <span class=p>),</span>
</span><span id=__span-16-46><a id=__codelineno-16-46 name=__codelineno-16-46 href=#__codelineno-16-46></a>        <span class=p>}</span>
</span></code></pre></div> <h3 id=benefits-of-self-parallelizing-models>Benefits of Self-Parallelizing Models<a class=headerlink href=#benefits-of-self-parallelizing-models title="Permanent link">&para;</a></h3> <ol> <li><strong>Encapsulation</strong>: Parallelism logic lives with model definition</li> <li><strong>Reusability</strong>: Same model works in any trainer</li> <li><strong>Clarity</strong>: Structure and strategy are co-located</li> <li><strong>Composability</strong>: Complex models built from simple parallel components</li> <li><strong>Type Safety</strong>: IDE understands the parallelism interface</li> </ol> <h2 id=manual-parallelism-implementation>Manual Parallelism Implementation<a class=headerlink href=#manual-parallelism-implementation title="Permanent link">&para;</a></h2> <p>For cases where you need fine-grained control:</p> <h3 id=manual-tensor-parallelism>Manual Tensor Parallelism<a class=headerlink href=#manual-tensor-parallelism title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.tensor.parallel</span><span class=w> </span><span class=kn>import</span> <span class=n>parallelize_module</span><span class=p>,</span> <span class=n>ColwiseParallel</span><span class=p>,</span> <span class=n>RowwiseParallel</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>DreamTrainer</span><span class=p>):</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>apply_tensor_parallel</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tp_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span><span class=p>):</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>        <span class=c1># Manual TP for specific layers</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a>            <span class=n>plan</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a>                <span class=s2>&quot;attention.wq&quot;</span><span class=p>:</span> <span class=n>ColwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-9><a id=__codelineno-17-9 name=__codelineno-17-9 href=#__codelineno-17-9></a>                <span class=s2>&quot;attention.wk&quot;</span><span class=p>:</span> <span class=n>ColwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-10><a id=__codelineno-17-10 name=__codelineno-17-10 href=#__codelineno-17-10></a>                <span class=s2>&quot;attention.wv&quot;</span><span class=p>:</span> <span class=n>ColwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-11><a id=__codelineno-17-11 name=__codelineno-17-11 href=#__codelineno-17-11></a>                <span class=s2>&quot;attention.wo&quot;</span><span class=p>:</span> <span class=n>RowwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-12><a id=__codelineno-17-12 name=__codelineno-17-12 href=#__codelineno-17-12></a>                <span class=s2>&quot;mlp.w1&quot;</span><span class=p>:</span> <span class=n>ColwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-13><a id=__codelineno-17-13 name=__codelineno-17-13 href=#__codelineno-17-13></a>                <span class=s2>&quot;mlp.w2&quot;</span><span class=p>:</span> <span class=n>RowwiseParallel</span><span class=p>(),</span>
</span><span id=__span-17-14><a id=__codelineno-17-14 name=__codelineno-17-14 href=#__codelineno-17-14></a>            <span class=p>}</span>
</span><span id=__span-17-15><a id=__codelineno-17-15 name=__codelineno-17-15 href=#__codelineno-17-15></a>            <span class=n>parallelize_module</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>tp_mesh</span><span class=p>,</span> <span class=n>plan</span><span class=p>)</span>
</span></code></pre></div> <h3 id=manual-fsdp2>Manual FSDP2<a class=headerlink href=#manual-fsdp2 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed._composable.fsdp</span><span class=w> </span><span class=kn>import</span> <span class=n>fully_shard</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>DreamTrainer</span><span class=p>):</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>apply_fully_shard</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]):</span>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a>        <span class=c1># Custom sharding per layer</span>
</span><span id=__span-18-6><a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
</span><span id=__span-18-7><a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a>            <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>10</span><span class=p>:</span>  <span class=c1># First 10 layers</span>
</span><span id=__span-18-8><a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a>                <span class=n>fully_shard</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-18-9><a id=__codelineno-18-9 name=__codelineno-18-9 href=#__codelineno-18-9></a>            <span class=k>else</span><span class=p>:</span>  <span class=c1># Later layers use different config</span>
</span><span id=__span-18-10><a id=__codelineno-18-10 name=__codelineno-18-10 href=#__codelineno-18-10></a>                <span class=n>custom_config</span> <span class=o>=</span> <span class=p>{</span><span class=o>**</span><span class=n>config</span><span class=p>,</span> <span class=s2>&quot;reshard_after_forward&quot;</span><span class=p>:</span> <span class=kc>False</span><span class=p>}</span>
</span><span id=__span-18-11><a id=__codelineno-18-11 name=__codelineno-18-11 href=#__codelineno-18-11></a>                <span class=n>fully_shard</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=o>**</span><span class=n>custom_config</span><span class=p>)</span>
</span></code></pre></div> <h3 id=manual-pipeline-parallelism>Manual Pipeline Parallelism<a class=headerlink href=#manual-pipeline-parallelism title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.pipelining</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span><span class=p>,</span> <span class=n>SplitPoint</span><span class=p>,</span> <span class=n>PipelineStage</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>DreamTrainer</span><span class=p>):</span>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>apply_pipeline_parallel</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>pp_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span><span class=p>):</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a>        <span class=c1># Define split points</span>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a>        <span class=n>mb</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>embeddings</span><span class=p>,</span> <span class=o>*</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>layers</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>output</span><span class=p>]</span>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a>        <span class=c1># Create pipeline stages</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a>        <span class=n>stages</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a>            <span class=n>PipelineStage</span><span class=p>(</span><span class=n>mb</span><span class=p>[:</span><span class=mi>8</span><span class=p>],</span> <span class=mi>0</span><span class=p>),</span>   <span class=c1># First 8 layers</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a>            <span class=n>PipelineStage</span><span class=p>(</span><span class=n>mb</span><span class=p>[</span><span class=mi>8</span><span class=p>:</span><span class=mi>16</span><span class=p>],</span> <span class=mi>1</span><span class=p>),</span>  <span class=c1># Next 8 layers</span>
</span><span id=__span-19-12><a id=__codelineno-19-12 name=__codelineno-19-12 href=#__codelineno-19-12></a>            <span class=n>PipelineStage</span><span class=p>(</span><span class=n>mb</span><span class=p>[</span><span class=mi>16</span><span class=p>:</span><span class=mi>24</span><span class=p>],</span> <span class=mi>2</span><span class=p>),</span> <span class=c1># Next 8 layers  </span>
</span><span id=__span-19-13><a id=__codelineno-19-13 name=__codelineno-19-13 href=#__codelineno-19-13></a>            <span class=n>PipelineStage</span><span class=p>(</span><span class=n>mb</span><span class=p>[</span><span class=mi>24</span><span class=p>:],</span> <span class=mi>3</span><span class=p>),</span>   <span class=c1># Rest</span>
</span><span id=__span-19-14><a id=__codelineno-19-14 name=__codelineno-19-14 href=#__codelineno-19-14></a>        <span class=p>]</span>
</span><span id=__span-19-15><a id=__codelineno-19-15 name=__codelineno-19-15 href=#__codelineno-19-15></a>
</span><span id=__span-19-16><a id=__codelineno-19-16 name=__codelineno-19-16 href=#__codelineno-19-16></a>        <span class=c1># Create schedule</span>
</span><span id=__span-19-17><a id=__codelineno-19-17 name=__codelineno-19-17 href=#__codelineno-19-17></a>        <span class=n>schedule</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span>
</span><span id=__span-19-18><a id=__codelineno-19-18 name=__codelineno-19-18 href=#__codelineno-19-18></a>            <span class=n>stages</span><span class=p>,</span>
</span><span id=__span-19-19><a id=__codelineno-19-19 name=__codelineno-19-19 href=#__codelineno-19-19></a>            <span class=n>pp_mesh</span><span class=p>,</span>
</span><span id=__span-19-20><a id=__codelineno-19-20 name=__codelineno-19-20 href=#__codelineno-19-20></a>            <span class=n>mb_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pipeline_parallel_microbatch_size</span><span class=p>,</span>
</span><span id=__span-19-21><a id=__codelineno-19-21 name=__codelineno-19-21 href=#__codelineno-19-21></a>        <span class=p>)</span>
</span><span id=__span-19-22><a id=__codelineno-19-22 name=__codelineno-19-22 href=#__codelineno-19-22></a>
</span><span id=__span-19-23><a id=__codelineno-19-23 name=__codelineno-19-23 href=#__codelineno-19-23></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;model&quot;</span><span class=p>:</span> <span class=p>(</span><span class=n>schedule</span><span class=p>,</span> <span class=n>stages</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>)}</span>
</span></code></pre></div> <h2 id=combining-parallelism-strategies>Combining Parallelism Strategies<a class=headerlink href=#combining-parallelism-strategies title="Permanent link">&para;</a></h2> <h3 id=real-world-examples>Real-World Examples<a class=headerlink href=#real-world-examples title="Permanent link">&para;</a></h3> <p>Based on model size and available hardware, here are recommended configurations:</p> <h4 id=small-model-1b-parameters>Small Model (&lt; 1B parameters)<a class=headerlink href=#small-model-1b-parameters title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># Fits on single GPU - use pure data parallelism</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>    <span class=n>dp_replicate</span><span class=o>=</span><span class=n>num_gpus</span><span class=p>,</span>  <span class=c1># Simple DDP</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a>    <span class=n>compile_model</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>     <span class=c1># Optimize single GPU perf</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a><span class=p>)</span>
</span></code></pre></div> <h4 id=medium-model-1b-70b-parameters>Medium Model (1B - 70B parameters)<a class=headerlink href=#medium-model-1b-70b-parameters title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=c1># Needs FSDP for memory, optional TP for speed</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>          <span class=c1># FSDP for memory efficiency</span>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>   <span class=c1># Optional: faster but more GPUs</span>
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a><span class=p>)</span>
</span></code></pre></div> <h4 id=large-model-70b-500b-parameters>Large Model (70B - 500B parameters)<a class=headerlink href=#large-model-70b-500b-parameters title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=c1># Requires both memory and compute parallelism</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>    <span class=c1># Split within nodes</span>
</span><span id=__span-22-4><a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a>    <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># Split across nodes</span>
</span><span id=__span-22-5><a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>         <span class=c1># FSDP across everything</span>
</span><span id=__span-22-6><a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a><span class=p>)</span>
</span></code></pre></div> <h4 id=extreme-scale-500b-parameters>Extreme Scale (500B+ parameters)<a class=headerlink href=#extreme-scale-500b-parameters title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a><span class=c1># Everything at maximum scale</span>
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>     <span class=c1># Max TP within node</span>
</span><span id=__span-23-4><a id=__codelineno-23-4 name=__codelineno-23-4 href=#__codelineno-23-4></a>    <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>  <span class=c1># Many pipeline stages</span>
</span><span id=__span-23-5><a id=__codelineno-23-5 name=__codelineno-23-5 href=#__codelineno-23-5></a>    <span class=n>dp_replicate</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>       <span class=c1># HSDP: DDP across pods</span>
</span><span id=__span-23-6><a id=__codelineno-23-6 name=__codelineno-23-6 href=#__codelineno-23-6></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span>          <span class=c1># FSDP within pods</span>
</span><span id=__span-23-7><a id=__codelineno-23-7 name=__codelineno-23-7 href=#__codelineno-23-7></a>    <span class=n>context_parallel</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>    <span class=c1># For long sequences</span>
</span><span id=__span-23-8><a id=__codelineno-23-8 name=__codelineno-23-8 href=#__codelineno-23-8></a><span class=p>)</span>
</span></code></pre></div> <h3 id=3d-parallelism-dp-tp-pp>3D Parallelism (DP + TP + PP)<a class=headerlink href=#3d-parallelism-dp-tp-pp title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>          <span class=c1># 4-way FSDP</span>
</span><span id=__span-24-3><a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>   <span class=c1># 2-way TP  </span>
</span><span id=__span-24-4><a id=__codelineno-24-4 name=__codelineno-24-4 href=#__codelineno-24-4></a>    <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=c1># 2 pipeline stages</span>
</span><span id=__span-24-5><a id=__codelineno-24-5 name=__codelineno-24-5 href=#__codelineno-24-5></a>    <span class=c1># Total: 4 * 2 * 2 = 16 GPUs</span>
</span><span id=__span-24-6><a id=__codelineno-24-6 name=__codelineno-24-6 href=#__codelineno-24-6></a><span class=p>)</span>
</span></code></pre></div> <h3 id=hsdp-hybrid-sharded-data-parallel>HSDP (Hybrid Sharded Data Parallel)<a class=headerlink href=#hsdp-hybrid-sharded-data-parallel title="Permanent link">&para;</a></h3> <p>HSDP combines the benefits of DDP and FSDP by creating a hierarchy:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>    <span class=n>dp_replicate</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>  <span class=c1># 2 DDP groups (across pods)</span>
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a>    <span class=n>dp_shard</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>      <span class=c1># 4-way FSDP within each group</span>
</span><span id=__span-25-4><a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a><span class=p>)</span>
</span><span id=__span-25-5><a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a>
</span><span id=__span-25-6><a id=__codelineno-25-6 name=__codelineno-25-6 href=#__codelineno-25-6></a><span class=c1># Communication pattern:</span>
</span><span id=__span-25-7><a id=__codelineno-25-7 name=__codelineno-25-7 href=#__codelineno-25-7></a><span class=c1># - Gradient reduce-scatter within FSDP groups (high bandwidth)</span>
</span><span id=__span-25-8><a id=__codelineno-25-8 name=__codelineno-25-8 href=#__codelineno-25-8></a><span class=c1># - Gradient all-reduce across DDP groups (low bandwidth)</span>
</span><span id=__span-25-9><a id=__codelineno-25-9 name=__codelineno-25-9 href=#__codelineno-25-9></a><span class=c1># Result: Less cross-pod communication</span>
</span></code></pre></div> <h3 id=context-parallel-tensor-parallel>Context Parallel + Tensor Parallel<a class=headerlink href=#context-parallel-tensor-parallel title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a>    <span class=n>context_parallel</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># 4-way sequence splitting</span>
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>   <span class=c1># 2-way tensor parallel</span>
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a>    <span class=c1># Enables 4x longer sequences with 2x model parallel</span>
</span><span id=__span-26-5><a id=__codelineno-26-5 name=__codelineno-26-5 href=#__codelineno-26-5></a><span class=p>)</span>
</span></code></pre></div> <h2 id=performance-optimization>Performance Optimization<a class=headerlink href=#performance-optimization title="Permanent link">&para;</a></h2> <h3 id=optimizing-for-your-hardware>Optimizing for Your Hardware<a class=headerlink href=#optimizing-for-your-hardware title="Permanent link">&para;</a></h3> <ol> <li> <p><strong>Measure Your Baseline</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=c1># Profile single GPU performance first</span>
</span><span id=__span-27-2><a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>profile</span><span class=p>()</span> <span class=k>as</span> <span class=n>prof</span><span class=p>:</span>
</span><span id=__span-27-3><a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a>    <span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span><span id=__span-27-4><a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a>
</span><span id=__span-27-5><a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a><span class=c1># Key metrics:</span>
</span><span id=__span-27-6><a id=__codelineno-27-6 name=__codelineno-27-6 href=#__codelineno-27-6></a><span class=c1># - GPU utilization (target: &gt;90%)</span>
</span><span id=__span-27-7><a id=__codelineno-27-7 name=__codelineno-27-7 href=#__codelineno-27-7></a><span class=c1># - Memory bandwidth utilization</span>
</span><span id=__span-27-8><a id=__codelineno-27-8 name=__codelineno-27-8 href=#__codelineno-27-8></a><span class=c1># - Time spent in communication vs compute</span>
</span></code></pre></div></p> </li> <li> <p><strong>Scale Gradually</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=c1># Start simple, add parallelism incrementally</span>
</span><span id=__span-28-2><a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a><span class=n>configs</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-28-3><a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a>    <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_replicate</span><span class=o>=</span><span class=mi>8</span><span class=p>),</span>  <span class=c1># Pure DDP</span>
</span><span id=__span-28-4><a id=__codelineno-28-4 name=__codelineno-28-4 href=#__codelineno-28-4></a>    <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_shard</span><span class=o>=</span><span class=mi>8</span><span class=p>),</span>      <span class=c1># Pure FSDP</span>
</span><span id=__span-28-5><a id=__codelineno-28-5 name=__codelineno-28-5 href=#__codelineno-28-5></a>    <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_shard</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>  <span class=c1># FSDP+TP</span>
</span><span id=__span-28-6><a id=__codelineno-28-6 name=__codelineno-28-6 href=#__codelineno-28-6></a>    <span class=n>DeviceParameters</span><span class=p>(</span><span class=n>dp_shard</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>pipeline_parallel</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>  <span class=c1># 3D</span>
</span><span id=__span-28-7><a id=__codelineno-28-7 name=__codelineno-28-7 href=#__codelineno-28-7></a><span class=p>]</span>
</span></code></pre></div></p> </li> <li> <p><strong>Monitor Scaling Efficiency</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=c1># Perfect scaling: 2x GPUs = 2x throughput</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a><span class=n>scaling_efficiency</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a>    <span class=n>throughput_ngpus</span> <span class=o>/</span> <span class=n>throughput_1gpu</span>
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a><span class=p>)</span> <span class=o>/</span> <span class=n>n_gpus</span>
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a><span class=c1># Good: &gt;0.8</span>
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a><span class=c1># Okay: 0.6-0.8  </span>
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a><span class=c1># Poor: &lt;0.6 (reconsider strategy)</span>
</span></code></pre></div></p> </li> </ol> <h3 id=1-fsdp-prefetching>1. FSDP Prefetching<a class=headerlink href=#1-fsdp-prefetching title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>OptimizeFSDP</span>
</span><span id=__span-30-2><a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a>
</span><span id=__span-30-3><a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a><span class=n>config</span><span class=o>.</span><span class=n>callbacks</span> <span class=o>=</span> <span class=n>CallbackCollection</span><span class=p>([</span>
</span><span id=__span-30-4><a id=__codelineno-30-4 name=__codelineno-30-4 href=#__codelineno-30-4></a>    <span class=n>OptimizeFSDP</span><span class=p>(</span><span class=n>prefetch</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>  <span class=c1># Prefetch next 2 modules</span>
</span><span id=__span-30-5><a id=__codelineno-30-5 name=__codelineno-30-5 href=#__codelineno-30-5></a><span class=p>])</span>
</span></code></pre></div> <h3 id=2-async-tensor-parallelism>2. Async Tensor Parallelism<a class=headerlink href=#2-async-tensor-parallelism title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-31-2><a id=__codelineno-31-2 name=__codelineno-31-2 href=#__codelineno-31-2></a>    <span class=n>tensor_parallel</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-31-3><a id=__codelineno-31-3 name=__codelineno-31-3 href=#__codelineno-31-3></a>    <span class=n>async_tensor_parallel</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># Overlap TP comms</span>
</span><span id=__span-31-4><a id=__codelineno-31-4 name=__codelineno-31-4 href=#__codelineno-31-4></a><span class=p>)</span>
</span></code></pre></div> <h3 id=3-compiled-autograd>3. Compiled Autograd<a class=headerlink href=#3-compiled-autograd title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=n>config</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a>    <span class=n>compile_model</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a>    <span class=n>enable_compiled_autograd</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># Compile backward too</span>
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a><span class=p>)</span>
</span></code></pre></div> <h3 id=4-communication-optimization>4. Communication Optimization<a class=headerlink href=#4-communication-optimization title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a><span class=c1># Set NCCL environment variables</span>
</span><span id=__span-33-2><a id=__codelineno-33-2 name=__codelineno-33-2 href=#__codelineno-33-2></a><span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-33-3><a id=__codelineno-33-3 name=__codelineno-33-3 href=#__codelineno-33-3></a><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&quot;NCCL_ALGO&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;Tree&quot;</span>  <span class=c1># Better for small messages</span>
</span><span id=__span-33-4><a id=__codelineno-33-4 name=__codelineno-33-4 href=#__codelineno-33-4></a><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&quot;NCCL_PROTO&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;Simple&quot;</span>  <span class=c1># Lower latency</span>
</span><span id=__span-33-5><a id=__codelineno-33-5 name=__codelineno-33-5 href=#__codelineno-33-5></a><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&quot;NCCL_NSOCKS_PERTHREAD&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;4&quot;</span>
</span><span id=__span-33-6><a id=__codelineno-33-6 name=__codelineno-33-6 href=#__codelineno-33-6></a><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&quot;NCCL_SOCKET_NTHREADS&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;8&quot;</span>
</span></code></pre></div> <h3 id=5-batch-size-optimization>5. Batch Size Optimization<a class=headerlink href=#5-batch-size-optimization title="Permanent link">&para;</a></h3> <p>Based on gradient noise scale theory:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=c1># Start with smaller batch size</span>
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2 href=#__codelineno-34-2></a><span class=n>initial_batch_size</span> <span class=o>=</span> <span class=mi>256</span>
</span><span id=__span-34-3><a id=__codelineno-34-3 name=__codelineno-34-3 href=#__codelineno-34-3></a>
</span><span id=__span-34-4><a id=__codelineno-34-4 name=__codelineno-34-4 href=#__codelineno-34-4></a><span class=c1># Measure gradient noise scale</span>
</span><span id=__span-34-5><a id=__codelineno-34-5 name=__codelineno-34-5 href=#__codelineno-34-5></a><span class=c1># (See &quot;An Empirical Model of Large-Batch Training&quot;)</span>
</span><span id=__span-34-6><a id=__codelineno-34-6 name=__codelineno-34-6 href=#__codelineno-34-6></a>
</span><span id=__span-34-7><a id=__codelineno-34-7 name=__codelineno-34-7 href=#__codelineno-34-7></a><span class=c1># Progressively increase batch size during training</span>
</span><span id=__span-34-8><a id=__codelineno-34-8 name=__codelineno-34-8 href=#__codelineno-34-8></a><span class=n>schedule</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-34-9><a id=__codelineno-34-9 name=__codelineno-34-9 href=#__codelineno-34-9></a>    <span class=mi>0</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span>        <span class=c1># Initial: high gradient noise</span>
</span><span id=__span-34-10><a id=__codelineno-34-10 name=__codelineno-34-10 href=#__codelineno-34-10></a>    <span class=mi>100_000</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span>  <span class=c1># Model improving, can use larger batches</span>
</span><span id=__span-34-11><a id=__codelineno-34-11 name=__codelineno-34-11 href=#__codelineno-34-11></a>    <span class=mi>500_000</span><span class=p>:</span> <span class=mi>1024</span><span class=p>,</span> <span class=c1># Near convergence, maximize efficiency</span>
</span><span id=__span-34-12><a id=__codelineno-34-12 name=__codelineno-34-12 href=#__codelineno-34-12></a><span class=p>}</span>
</span></code></pre></div> <h2 id=debugging-tips>Debugging Tips<a class=headerlink href=#debugging-tips title="Permanent link">&para;</a></h2> <h3 id=1-verify-sharding>1. Verify Sharding<a class=headerlink href=#1-verify-sharding title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a>    <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a>        <span class=c1># Check parameter sharding</span>
</span><span id=__span-35-4><a id=__codelineno-35-4 name=__codelineno-35-4 href=#__codelineno-35-4></a>        <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span><span id=__span-35-5><a id=__codelineno-35-5 name=__codelineno-35-5 href=#__codelineno-35-5></a>            <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>param</span><span class=p>,</span> <span class=s1>&#39;placements&#39;</span><span class=p>):</span>
</span><span id=__span-35-6><a id=__codelineno-35-6 name=__codelineno-35-6 href=#__codelineno-35-6></a>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>param</span><span class=o>.</span><span class=n>placements</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=2-monitor-memory-usage>2. Monitor Memory Usage<a class=headerlink href=#2-monitor-memory-usage title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.utils</span><span class=w> </span><span class=kn>import</span> <span class=n>log_memory_usage</span>
</span><span id=__span-36-2><a id=__codelineno-36-2 name=__codelineno-36-2 href=#__codelineno-36-2></a>
</span><span id=__span-36-3><a id=__codelineno-36-3 name=__codelineno-36-3 href=#__codelineno-36-3></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-36-4><a id=__codelineno-36-4 name=__codelineno-36-4 href=#__codelineno-36-4></a>    <span class=n>log_memory_usage</span><span class=p>(</span><span class=s2>&quot;before_forward&quot;</span><span class=p>)</span>
</span><span id=__span-36-5><a id=__codelineno-36-5 name=__codelineno-36-5 href=#__codelineno-36-5></a>    <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-36-6><a id=__codelineno-36-6 name=__codelineno-36-6 href=#__codelineno-36-6></a>    <span class=n>log_memory_usage</span><span class=p>(</span><span class=s2>&quot;after_forward&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=3-profile-communication>3. Profile Communication<a class=headerlink href=#3-profile-communication title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.distributed</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>dist</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a>
</span><span id=__span-37-3><a id=__codelineno-37-3 name=__codelineno-37-3 href=#__codelineno-37-3></a><span class=c1># Enable NCCL debug logging</span>
</span><span id=__span-37-4><a id=__codelineno-37-4 name=__codelineno-37-4 href=#__codelineno-37-4></a><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&quot;NCCL_DEBUG&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;INFO&quot;</span>
</span><span id=__span-37-5><a id=__codelineno-37-5 name=__codelineno-37-5 href=#__codelineno-37-5></a>
</span><span id=__span-37-6><a id=__codelineno-37-6 name=__codelineno-37-6 href=#__codelineno-37-6></a><span class=c1># Profile with PyTorch profiler</span>
</span><span id=__span-37-7><a id=__codelineno-37-7 name=__codelineno-37-7 href=#__codelineno-37-7></a><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>profile</span><span class=p>(</span>
</span><span id=__span-37-8><a id=__codelineno-37-8 name=__codelineno-37-8 href=#__codelineno-37-8></a>    <span class=n>activities</span><span class=o>=</span><span class=p>[</span><span class=n>ProfilerActivity</span><span class=o>.</span><span class=n>CPU</span><span class=p>,</span> <span class=n>ProfilerActivity</span><span class=o>.</span><span class=n>CUDA</span><span class=p>],</span>
</span><span id=__span-37-9><a id=__codelineno-37-9 name=__codelineno-37-9 href=#__codelineno-37-9></a>    <span class=n>record_shapes</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-37-10><a id=__codelineno-37-10 name=__codelineno-37-10 href=#__codelineno-37-10></a>    <span class=n>profile_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-37-11><a id=__codelineno-37-11 name=__codelineno-37-11 href=#__codelineno-37-11></a>    <span class=n>with_stack</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-37-12><a id=__codelineno-37-12 name=__codelineno-37-12 href=#__codelineno-37-12></a><span class=p>)</span> <span class=k>as</span> <span class=n>prof</span><span class=p>:</span>
</span><span id=__span-37-13><a id=__codelineno-37-13 name=__codelineno-37-13 href=#__codelineno-37-13></a>    <span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span></code></pre></div> <h3 id=4-test-incrementally>4. Test Incrementally<a class=headerlink href=#4-test-incrementally title="Permanent link">&para;</a></h3> <ol> <li>Start with single GPU</li> <li>Add DP (either DDP or FSDP)</li> <li>Add TP</li> <li>Add PP</li> <li>Combine strategies</li> </ol> <h2 id=best-practices>Best Practices<a class=headerlink href=#best-practices title="Permanent link">&para;</a></h2> <ol> <li><strong>Understand Your Bottleneck</strong></li> <li>Memory bound? → Use FSDP or PP</li> <li>Compute bound? → Use TP for large matmuls</li> <li> <p>Communication bound? → Optimize placement based on topology</p> </li> <li> <p><strong>Profile Before Optimizing</strong></p> </li> <li>Measure baseline single-GPU performance</li> <li>Identify whether you're compute or memory bound</li> <li> <p>Profile communication patterns</p> </li> <li> <p><strong>Consider Total System Design</strong></p> </li> <li>Network topology (NVLink vs InfiniBand vs Ethernet)</li> <li>Model architecture (depth vs width)</li> <li> <p>Batch size requirements (gradient noise scale)</p> </li> <li> <p><strong>Use Self-Parallelizing Models</strong></p> </li> <li>Encapsulate complexity in model definitions</li> <li>Makes experimentation easier</li> <li> <p>Improves code reusability</p> </li> <li> <p><strong>Test Scaling Incrementally</strong></p> </li> <li>Start with single GPU</li> <li>Add data parallelism</li> <li>Add model parallelism only if needed</li> <li>Verify near-linear scaling at each step</li> </ol> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <ul> <li>Read the <a href=../configuration/ >Configuration Guide</a> for detailed parameter options</li> <li>Check <a href=examples/advanced.md>Examples</a> for real-world parallelism usage</li> <li>See <a href=performance.md>Performance Guide</a> for optimization tips</li> <li>Review <a href=https://www.jeremyjordan.me/distributed-training/ >Jeremy Jordan's distributed training guide</a> for more theory</li> </ul> <hr> <p><strong>Remember</strong>: The best parallelism strategy depends on your model architecture, hardware topology, and training requirements. Dream Trainer gives you the flexibility to experiment and find what works best! 🚀</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../callbacks/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Callbacks"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Callbacks </div> </div> </a> <a href=../api/ class="md-footer__link md-footer__link--next" aria-label="Next: Overview"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Overview </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dream3d/dream-trainer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://discord.gg/dream-trainer target=_blank rel=noopener title=discord.gg class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> <a href=https://twitter.com/dream3d_ai target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.prune", "navigation.indexes", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tooltips", "toc.follow", "toc.integrate"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script> <script src=../assets/javascripts/bundle.56ea9cef.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>