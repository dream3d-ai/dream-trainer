<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Composable distributed training framework built around PyTorch DTensor abstractions"><link href=https://dream3d.ai/trainer/core-concepts/ rel=canonical><link href=../getting-started/ rel=prev><link href=../configuration/ rel=next><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.15"><title>Core Concepts - dream-trainer</title><link rel=stylesheet href=../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../assets/_mkdocstrings.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#core-concepts class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=dream-trainer class="md-header__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> dream-trainer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Core Concepts </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../installation/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../configuration/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../tutorials/first-trainer.md class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../examples/vision.md class=md-tabs__link> Examples </a> </li> <li class=md-tabs__item> <a href=../api/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../contributing.md class=md-tabs__link> Community </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=dream-trainer class="md-nav__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../assets/logo.png alt=logo> </a> dream-trainer </label> <div class=md-nav__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../getting-started/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Core Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Core Concepts </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#understanding-dtensor class=md-nav__link> <span class=md-ellipsis> Understanding DTensor </span> </a> <nav class=md-nav aria-label="Understanding DTensor"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-dtensor class=md-nav__link> <span class=md-ellipsis> What is DTensor? </span> </a> </li> <li class=md-nav__item> <a href=#dtensor-vs-traditional-distributed-training class=md-nav__link> <span class=md-ellipsis> DTensor vs Traditional Distributed Training </span> </a> </li> <li class=md-nav__item> <a href=#placement-and-sharding class=md-nav__link> <span class=md-ellipsis> Placement and Sharding </span> </a> </li> <li class=md-nav__item> <a href=#devicemesh-concept class=md-nav__link> <span class=md-ellipsis> DeviceMesh Concept </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#the-mixin-architecture class=md-nav__link> <span class=md-ellipsis> The Mixin Architecture </span> </a> <nav class=md-nav aria-label="The Mixin Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-mixins class=md-nav__link> <span class=md-ellipsis> Why Mixins? </span> </a> </li> <li class=md-nav__item> <a href=#how-mixins-work class=md-nav__link> <span class=md-ellipsis> How Mixins Work </span> </a> </li> <li class=md-nav__item> <a href=#available-mixins class=md-nav__link> <span class=md-ellipsis> Available Mixins </span> </a> <nav class=md-nav aria-label="Available Mixins"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#setupmixin-family class=md-nav__link> <span class=md-ellipsis> SetupMixin Family </span> </a> </li> <li class=md-nav__item> <a href=#evalmetricmixin class=md-nav__link> <span class=md-ellipsis> EvalMetricMixin </span> </a> </li> <li class=md-nav__item> <a href=#loggermixin-variants class=md-nav__link> <span class=md-ellipsis> LoggerMixin Variants </span> </a> </li> <li class=md-nav__item> <a href=#quantizemixin class=md-nav__link> <span class=md-ellipsis> QuantizeMixin </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-a-custom-trainer class=md-nav__link> <span class=md-ellipsis> Building a Custom Trainer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#training-loop-lifecycle class=md-nav__link> <span class=md-ellipsis> Training Loop Lifecycle </span> </a> <nav class=md-nav aria-label="Training Loop Lifecycle"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#initialization-phase class=md-nav__link> <span class=md-ellipsis> Initialization Phase </span> </a> </li> <li class=md-nav__item> <a href=#training-phase class=md-nav__link> <span class=md-ellipsis> Training Phase </span> </a> </li> <li class=md-nav__item> <a href=#gradient-accumulation-flow class=md-nav__link> <span class=md-ellipsis> Gradient Accumulation Flow </span> </a> </li> <li class=md-nav__item> <a href=#validation-phase class=md-nav__link> <span class=md-ellipsis> Validation Phase </span> </a> </li> <li class=md-nav__item> <a href=#callback-integration class=md-nav__link> <span class=md-ellipsis> Callback Integration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#state-management class=md-nav__link> <span class=md-ellipsis> State Management </span> </a> <nav class=md-nav aria-label="State Management"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#trainer-state class=md-nav__link> <span class=md-ellipsis> Trainer State </span> </a> </li> <li class=md-nav__item> <a href=#checkpointing class=md-nav__link> <span class=md-ellipsis> Checkpointing </span> </a> </li> <li class=md-nav__item> <a href=#distributed-state class=md-nav__link> <span class=md-ellipsis> Distributed State </span> </a> </li> <li class=md-nav__item> <a href=#fault-tolerance class=md-nav__link> <span class=md-ellipsis> Fault Tolerance </span> </a> </li> <li class=md-nav__item> <a href=#memory-management class=md-nav__link> <span class=md-ellipsis> Memory Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../configuration/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../tutorials/first-trainer.md class=md-nav__link> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../examples/vision.md class=md-nav__link> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../api/ class=md-nav__link> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../contributing.md class=md-nav__link> <span class=md-ellipsis> Community </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dream3d/dream-trainer/edit/main/dream-trainer/pages/docs/core-concepts.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dream3d/dream-trainer/raw/main/dream-trainer/pages/docs/core-concepts.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=core-concepts>Core Concepts<a class=headerlink href=#core-concepts title="Permanent link">&para;</a></h1> <p>Welcome to the Dream Trainer Core Concepts guide. This document explains the fundamental ideas and design patterns that power Dream Trainer's flexible, distributed training framework.</p> <h2 id=understanding-dtensor>Understanding DTensor<a class=headerlink href=#understanding-dtensor title="Permanent link">&para;</a></h2> <h3 id=what-is-dtensor>What is DTensor?<a class=headerlink href=#what-is-dtensor title="Permanent link">&para;</a></h3> <p>DTensor (Distributed Tensor) is PyTorch's next-generation distributed computing primitive that provides a unified abstraction for tensor parallelism. Unlike traditional distributed approaches that treat each device's data separately, DTensor represents a global logical tensor that can be sharded across multiple devices while maintaining a single, coherent view.</p> <p><strong>Key Benefits:</strong> - <strong>Unified API</strong>: Write code once, run with any parallelism strategy - <strong>Automatic gradient synchronization</strong>: DTensor handles communication patterns - <strong>Composability</strong>: Easily combine different parallelism strategies - <strong>Device mesh awareness</strong>: Understands multi-dimensional device topologies</p> <h3 id=dtensor-vs-traditional-distributed-training>DTensor vs Traditional Distributed Training<a class=headerlink href=#dtensor-vs-traditional-distributed-training title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># Traditional DDP approach - each rank has its own tensor</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>rank</span><span class=p>)</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>DDP</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=n>rank</span><span class=p>])</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># Each rank processes different data</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=c1># DTensor approach - single logical tensor across all ranks</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.tensor</span><span class=w> </span><span class=kn>import</span> <span class=n>DTensor</span><span class=p>,</span> <span class=n>Shard</span><span class=p>,</span> <span class=n>Replicate</span><span class=p>,</span> <span class=n>DeviceMesh</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=c1># Create a 2D device mesh for hybrid parallelism</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=n>device_mesh</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]])</span>  <span class=c1># 2x2 mesh</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=c1># Create a DTensor with specific sharding</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=n>tensor</span> <span class=o>=</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>from_local</span><span class=p>(</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>    <span class=n>local_tensor</span><span class=p>,</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>    <span class=n>device_mesh</span><span class=p>,</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>    <span class=n>placements</span><span class=o>=</span><span class=p>[</span><span class=n>Shard</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>Replicate</span><span class=p>()]</span>  <span class=c1># Shard on dim 0, replicate on dim 1</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=p>)</span>
</span></code></pre></div> <h3 id=placement-and-sharding>Placement and Sharding<a class=headerlink href=#placement-and-sharding title="Permanent link">&para;</a></h3> <p>DTensor uses <strong>placements</strong> to describe how data is distributed:</p> <ul> <li><strong><code>Replicate()</code></strong>: Each device has a full copy of the tensor</li> <li><strong><code>Shard(dim)</code></strong>: Tensor is split along the specified dimension</li> <li><strong><code>Partial()</code></strong>: Each device has a partial value (used during reduction)</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># Example: Creating different DTensor distributions</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.distributed.tensor</span><span class=w> </span><span class=kn>import</span> <span class=n>DTensor</span><span class=p>,</span> <span class=n>DeviceMesh</span><span class=p>,</span> <span class=n>Shard</span><span class=p>,</span> <span class=n>Replicate</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=c1># Initialize a simple device mesh</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=n>device_mesh</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=c1># Full replication - all devices have complete tensor</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=n>weight_replicated</span> <span class=o>=</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>from_local</span><span class=p>(</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>    <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=mi>1024</span><span class=p>),</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>    <span class=n>device_mesh</span><span class=p>,</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>    <span class=p>[</span><span class=n>Replicate</span><span class=p>()]</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a><span class=p>)</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a><span class=c1># Sharding along dimension 0 - each device gets 256 rows</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a><span class=n>weight_sharded</span> <span class=o>=</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>from_local</span><span class=p>(</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a>    <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>1024</span><span class=p>),</span>  <span class=c1># Local shard size</span>
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18 href=#__codelineno-1-18></a>    <span class=n>device_mesh</span><span class=p>,</span>
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19 href=#__codelineno-1-19></a>    <span class=p>[</span><span class=n>Shard</span><span class=p>(</span><span class=mi>0</span><span class=p>)]</span>
</span><span id=__span-1-20><a id=__codelineno-1-20 name=__codelineno-1-20 href=#__codelineno-1-20></a><span class=p>)</span>
</span><span id=__span-1-21><a id=__codelineno-1-21 name=__codelineno-1-21 href=#__codelineno-1-21></a>
</span><span id=__span-1-22><a id=__codelineno-1-22 name=__codelineno-1-22 href=#__codelineno-1-22></a><span class=c1># 2D mesh with hybrid sharding</span>
</span><span id=__span-1-23><a id=__codelineno-1-23 name=__codelineno-1-23 href=#__codelineno-1-23></a><span class=n>mesh_2d</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]])</span>
</span><span id=__span-1-24><a id=__codelineno-1-24 name=__codelineno-1-24 href=#__codelineno-1-24></a><span class=n>weight_2d</span> <span class=o>=</span> <span class=n>DTensor</span><span class=o>.</span><span class=n>from_local</span><span class=p>(</span>
</span><span id=__span-1-25><a id=__codelineno-1-25 name=__codelineno-1-25 href=#__codelineno-1-25></a>    <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
</span><span id=__span-1-26><a id=__codelineno-1-26 name=__codelineno-1-26 href=#__codelineno-1-26></a>    <span class=n>mesh_2d</span><span class=p>,</span>
</span><span id=__span-1-27><a id=__codelineno-1-27 name=__codelineno-1-27 href=#__codelineno-1-27></a>    <span class=p>[</span><span class=n>Shard</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>Shard</span><span class=p>(</span><span class=mi>1</span><span class=p>)]</span>  <span class=c1># Shard rows on first mesh dim, cols on second</span>
</span><span id=__span-1-28><a id=__codelineno-1-28 name=__codelineno-1-28 href=#__codelineno-1-28></a><span class=p>)</span>
</span></code></pre></div> <h3 id=devicemesh-concept>DeviceMesh Concept<a class=headerlink href=#devicemesh-concept title="Permanent link">&para;</a></h3> <p>DeviceMesh represents the topology of devices participating in distributed computation:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># 1D mesh - simple data parallel</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>dp_mesh</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=c1># 2D mesh - hybrid data + tensor parallel</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># First dimension: data parallel groups</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=c1># Second dimension: tensor parallel groups</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=n>mesh_2d</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>  <span class=c1># TP group 0</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>  <span class=c1># TP group 1</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=p>])</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># 3D mesh - data + tensor + pipeline parallel</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=n>mesh_3d</span> <span class=o>=</span> <span class=n>DeviceMesh</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>,</span> <span class=p>[</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a>    <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]],</span>    <span class=c1># PP stage 0</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a>    <span class=p>[[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>]],</span>    <span class=c1># PP stage 1</span>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a><span class=p>])</span>
</span></code></pre></div> <h2 id=the-mixin-architecture>The Mixin Architecture<a class=headerlink href=#the-mixin-architecture title="Permanent link">&para;</a></h2> <h3 id=why-mixins>Why Mixins?<a class=headerlink href=#why-mixins title="Permanent link">&para;</a></h3> <p>Dream Trainer uses a mixin-based architecture to achieve maximum flexibility and code reuse. Instead of a monolithic trainer class with every feature built-in, we compose trainers from smaller, focused components.</p> <p><strong>Traditional Inheritance Problems:</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Rigid hierarchy - hard to mix features</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=k>class</span><span class=w> </span><span class=nc>Trainer</span><span class=p>:</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=k>def</span><span class=w> </span><span class=nf>train</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=o>...</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=k>class</span><span class=w> </span><span class=nc>DistributedTrainer</span><span class=p>(</span><span class=n>Trainer</span><span class=p>):</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup_distributed</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=o>...</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=k>class</span><span class=w> </span><span class=nc>LoggingTrainer</span><span class=p>(</span><span class=n>DistributedTrainer</span><span class=p>):</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    <span class=k>def</span><span class=w> </span><span class=nf>log_metrics</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=o>...</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=c1># What if I want logging without distributed? ðŸ¤”</span>
</span></code></pre></div></p> <p><strong>Mixin Solution:</strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Flexible composition - take what you need</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>SetupMixin</span><span class=p>,</span> <span class=n>LoggerMixin</span><span class=p>):</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>    <span class=c1># Get distributed setup from SetupMixin</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>    <span class=c1># Get logging from LoggerMixin</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>    <span class=c1># Skip what you don&#39;t need!</span>
</span></code></pre></div></p> <h3 id=how-mixins-work>How Mixins Work<a class=headerlink href=#how-mixins-work title="Permanent link">&para;</a></h3> <p>Mixins leverage Python's Method Resolution Order (MRO) to combine functionality:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Understanding MRO with mixins</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=k>class</span><span class=w> </span><span class=nc>BaseTrainer</span><span class=p>:</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;BaseTrainer.setup()&quot;</span><span class=p>)</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=k>class</span><span class=w> </span><span class=nc>ModelMixin</span><span class=p>:</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>  <span class=c1># Calls next in MRO</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;ModelMixin.setup()&quot;</span><span class=p>)</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>        <span class=bp>self</span><span class=o>.</span><span class=n>setup_models</span><span class=p>()</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=k>class</span><span class=w> </span><span class=nc>OptimizerMixin</span><span class=p>:</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>  <span class=c1># Calls next in MRO</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;OptimizerMixin.setup()&quot;</span><span class=p>)</span>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a>        <span class=bp>self</span><span class=o>.</span><span class=n>setup_optimizers</span><span class=p>()</span>
</span><span id=__span-5-17><a id=__codelineno-5-17 name=__codelineno-5-17 href=#__codelineno-5-17></a>
</span><span id=__span-5-18><a id=__codelineno-5-18 name=__codelineno-5-18 href=#__codelineno-5-18></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>ModelMixin</span><span class=p>,</span> <span class=n>OptimizerMixin</span><span class=p>):</span>
</span><span id=__span-5-19><a id=__codelineno-5-19 name=__codelineno-5-19 href=#__codelineno-5-19></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-20><a id=__codelineno-5-20 name=__codelineno-5-20 href=#__codelineno-5-20></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>  <span class=c1># Triggers the chain</span>
</span><span id=__span-5-21><a id=__codelineno-5-21 name=__codelineno-5-21 href=#__codelineno-5-21></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;MyTrainer.setup()&quot;</span><span class=p>)</span>
</span><span id=__span-5-22><a id=__codelineno-5-22 name=__codelineno-5-22 href=#__codelineno-5-22></a>
</span><span id=__span-5-23><a id=__codelineno-5-23 name=__codelineno-5-23 href=#__codelineno-5-23></a><span class=c1># MRO: MyTrainer -&gt; OptimizerMixin -&gt; ModelMixin -&gt; BaseTrainer</span>
</span><span id=__span-5-24><a id=__codelineno-5-24 name=__codelineno-5-24 href=#__codelineno-5-24></a><span class=c1># Output:</span>
</span><span id=__span-5-25><a id=__codelineno-5-25 name=__codelineno-5-25 href=#__codelineno-5-25></a><span class=c1># BaseTrainer.setup()</span>
</span><span id=__span-5-26><a id=__codelineno-5-26 name=__codelineno-5-26 href=#__codelineno-5-26></a><span class=c1># ModelMixin.setup()</span>
</span><span id=__span-5-27><a id=__codelineno-5-27 name=__codelineno-5-27 href=#__codelineno-5-27></a><span class=c1># OptimizerMixin.setup()</span>
</span><span id=__span-5-28><a id=__codelineno-5-28 name=__codelineno-5-28 href=#__codelineno-5-28></a><span class=c1># MyTrainer.setup()</span>
</span></code></pre></div> <h3 id=available-mixins>Available Mixins<a class=headerlink href=#available-mixins title="Permanent link">&para;</a></h3> <p>Dream Trainer provides several mixins for different aspects of training:</p> <h4 id=setupmixin-family>SetupMixin Family<a class=headerlink href=#setupmixin-family title="Permanent link">&para;</a></h4> <p>Handles model initialization, parallelism, and optimization:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.mixins</span><span class=w> </span><span class=kn>import</span> <span class=n>SetupMixin</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>SetupMixin</span><span class=p>):</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_models</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>        <span class=c1># Create models on meta device for efficiency</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>TransformerModel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>    <span class=k>def</span><span class=w> </span><span class=nf>init_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>        <span class=c1># Initialize weights after device placement</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_init_weights</span><span class=p>)</span>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_optimizers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>        <span class=c1># Create optimizers after model is on device</span>
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a>        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span>
</span><span id=__span-6-15><a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a>            <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span><span id=__span-6-16><a id=__codelineno-6-16 name=__codelineno-6-16 href=#__codelineno-6-16></a>            <span class=n>lr</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>learning_rate</span>
</span><span id=__span-6-17><a id=__codelineno-6-17 name=__codelineno-6-17 href=#__codelineno-6-17></a>        <span class=p>)</span>
</span><span id=__span-6-18><a id=__codelineno-6-18 name=__codelineno-6-18 href=#__codelineno-6-18></a>
</span><span id=__span-6-19><a id=__codelineno-6-19 name=__codelineno-6-19 href=#__codelineno-6-19></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_dataloaders</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-20><a id=__codelineno-6-20 name=__codelineno-6-20 href=#__codelineno-6-20></a>        <span class=c1># Return train and validation dataloaders</span>
</span><span id=__span-6-21><a id=__codelineno-6-21 name=__codelineno-6-21 href=#__codelineno-6-21></a>        <span class=k>return</span> <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>val_dataloader</span>
</span></code></pre></div> <p>The SetupMixin is actually composed of several sub-mixins: - <strong>ModelSetupMixin</strong>: Model creation, parallelism application, compilation - <strong>OptimizerAndSchedulerSetupMixin</strong>: Optimizer and LR scheduler management<br> - <strong>DataLoaderSetupMixin</strong>: DataLoader configuration and setup</p> <h4 id=evalmetricmixin>EvalMetricMixin<a class=headerlink href=#evalmetricmixin title="Permanent link">&para;</a></h4> <p>Integrates torchmetrics for standardized evaluation:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.mixins</span><span class=w> </span><span class=kn>import</span> <span class=n>EvalMetricMixin</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>SetupMixin</span><span class=p>,</span> <span class=n>EvalMetricMixin</span><span class=p>):</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_metrics</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>        <span class=c1># Metrics are automatically moved to correct devices</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>accuracy</span> <span class=o>=</span> <span class=n>torchmetrics</span><span class=o>.</span><span class=n>Accuracy</span><span class=p>(</span><span class=n>task</span><span class=o>=</span><span class=s2>&quot;multiclass&quot;</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>f1_score</span> <span class=o>=</span> <span class=n>torchmetrics</span><span class=o>.</span><span class=n>F1Score</span><span class=p>(</span><span class=n>task</span><span class=o>=</span><span class=s2>&quot;multiclass&quot;</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a>    <span class=k>def</span><span class=w> </span><span class=nf>validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a>        <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>batch</span>
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-7-12><a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a>
</span><span id=__span-7-13><a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a>        <span class=c1># Update metrics - handles distributed sync automatically</span>
</span><span id=__span-7-14><a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a>        <span class=bp>self</span><span class=o>.</span><span class=n>accuracy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span><span id=__span-7-15><a id=__codelineno-7-15 name=__codelineno-7-15 href=#__codelineno-7-15></a>        <span class=bp>self</span><span class=o>.</span><span class=n>f1_score</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span><span id=__span-7-16><a id=__codelineno-7-16 name=__codelineno-7-16 href=#__codelineno-7-16></a>
</span><span id=__span-7-17><a id=__codelineno-7-17 name=__codelineno-7-17 href=#__codelineno-7-17></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;val_loss&quot;</span><span class=p>:</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)}</span>
</span></code></pre></div> <h4 id=loggermixin-variants>LoggerMixin Variants<a class=headerlink href=#loggermixin-variants title="Permanent link">&para;</a></h4> <p>Different logging backends with consistent interfaces:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.mixins</span><span class=w> </span><span class=kn>import</span> <span class=n>WandBLoggerMixin</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>SetupMixin</span><span class=p>,</span> <span class=n>WandBLoggerMixin</span><span class=p>):</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a>        <span class=c1># Automatic logging to Weights &amp; Biases</span>
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s2>&quot;train/loss&quot;</span><span class=p>,</span> <span class=n>loss</span><span class=p>)</span>
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a>
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a>        <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-8-11><a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a>            <span class=bp>self</span><span class=o>.</span><span class=n>log_image</span><span class=p>(</span><span class=s2>&quot;train/samples&quot;</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;image&quot;</span><span class=p>][:</span><span class=mi>8</span><span class=p>])</span>
</span><span id=__span-8-12><a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a>
</span><span id=__span-8-13><a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span></code></pre></div> <h4 id=quantizemixin>QuantizeMixin<a class=headerlink href=#quantizemixin title="Permanent link">&para;</a></h4> <p>For model quantization (FP8, INT8):</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.mixins</span><span class=w> </span><span class=kn>import</span> <span class=n>QuantizeMixin</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>SetupMixin</span><span class=p>,</span> <span class=n>QuantizeMixin</span><span class=p>):</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>        <span class=c1># Quantize after model setup</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>apply_quantization</span><span class=p>()</span>
</span></code></pre></div> <h3 id=building-a-custom-trainer>Building a Custom Trainer<a class=headerlink href=#building-a-custom-trainer title="Permanent link">&para;</a></h3> <p>Here's a complete example showing how to compose a custom trainer:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dataclasses</span><span class=w> </span><span class=kn>import</span> <span class=n>dataclass</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer</span><span class=w> </span><span class=kn>import</span> <span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>BaseTrainerConfig</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.mixins</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>    <span class=n>SetupMixin</span><span class=p>,</span> <span class=n>SetupConfigMixin</span><span class=p>,</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>    <span class=n>EvalMetricMixin</span><span class=p>,</span> <span class=n>EvalMetricConfigMixin</span><span class=p>,</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>    <span class=n>WandBLoggerMixin</span><span class=p>,</span> <span class=n>WandBLoggerConfigMixin</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a><span class=p>)</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a><span class=c1># Step 1: Define configuration by mixing config classes</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a><span class=nd>@dataclass</span><span class=p>(</span><span class=n>kw_only</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainerConfig</span><span class=p>(</span>
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a>    <span class=n>BaseTrainerConfig</span><span class=p>,</span>
</span><span id=__span-10-13><a id=__codelineno-10-13 name=__codelineno-10-13 href=#__codelineno-10-13></a>    <span class=n>SetupConfigMixin</span><span class=p>,</span>
</span><span id=__span-10-14><a id=__codelineno-10-14 name=__codelineno-10-14 href=#__codelineno-10-14></a>    <span class=n>EvalMetricConfigMixin</span><span class=p>,</span>
</span><span id=__span-10-15><a id=__codelineno-10-15 name=__codelineno-10-15 href=#__codelineno-10-15></a>    <span class=n>WandBLoggerConfigMixin</span>
</span><span id=__span-10-16><a id=__codelineno-10-16 name=__codelineno-10-16 href=#__codelineno-10-16></a><span class=p>):</span>
</span><span id=__span-10-17><a id=__codelineno-10-17 name=__codelineno-10-17 href=#__codelineno-10-17></a>    <span class=c1># Add custom config fields</span>
</span><span id=__span-10-18><a id=__codelineno-10-18 name=__codelineno-10-18 href=#__codelineno-10-18></a>    <span class=n>model_dim</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>768</span>
</span><span id=__span-10-19><a id=__codelineno-10-19 name=__codelineno-10-19 href=#__codelineno-10-19></a>    <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>12</span>
</span><span id=__span-10-20><a id=__codelineno-10-20 name=__codelineno-10-20 href=#__codelineno-10-20></a>    <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>12</span>
</span><span id=__span-10-21><a id=__codelineno-10-21 name=__codelineno-10-21 href=#__codelineno-10-21></a>    <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>50257</span>
</span><span id=__span-10-22><a id=__codelineno-10-22 name=__codelineno-10-22 href=#__codelineno-10-22></a>
</span><span id=__span-10-23><a id=__codelineno-10-23 name=__codelineno-10-23 href=#__codelineno-10-23></a><span class=c1># Step 2: Define trainer by mixing trainer classes</span>
</span><span id=__span-10-24><a id=__codelineno-10-24 name=__codelineno-10-24 href=#__codelineno-10-24></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span>
</span><span id=__span-10-25><a id=__codelineno-10-25 name=__codelineno-10-25 href=#__codelineno-10-25></a>    <span class=n>BaseTrainer</span><span class=p>,</span>
</span><span id=__span-10-26><a id=__codelineno-10-26 name=__codelineno-10-26 href=#__codelineno-10-26></a>    <span class=n>SetupMixin</span><span class=p>,</span>
</span><span id=__span-10-27><a id=__codelineno-10-27 name=__codelineno-10-27 href=#__codelineno-10-27></a>    <span class=n>EvalMetricMixin</span><span class=p>,</span>
</span><span id=__span-10-28><a id=__codelineno-10-28 name=__codelineno-10-28 href=#__codelineno-10-28></a>    <span class=n>WandBLoggerMixin</span>
</span><span id=__span-10-29><a id=__codelineno-10-29 name=__codelineno-10-29 href=#__codelineno-10-29></a><span class=p>):</span>
</span><span id=__span-10-30><a id=__codelineno-10-30 name=__codelineno-10-30 href=#__codelineno-10-30></a>    <span class=n>config</span><span class=p>:</span> <span class=n>MyTrainerConfig</span>
</span><span id=__span-10-31><a id=__codelineno-10-31 name=__codelineno-10-31 href=#__codelineno-10-31></a>
</span><span id=__span-10-32><a id=__codelineno-10-32 name=__codelineno-10-32 href=#__codelineno-10-32></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_models</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-33><a id=__codelineno-10-33 name=__codelineno-10-33 href=#__codelineno-10-33></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>TransformerLM</span><span class=p>(</span>
</span><span id=__span-10-34><a id=__codelineno-10-34 name=__codelineno-10-34 href=#__codelineno-10-34></a>            <span class=n>dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model_dim</span><span class=p>,</span>
</span><span id=__span-10-35><a id=__codelineno-10-35 name=__codelineno-10-35 href=#__codelineno-10-35></a>            <span class=n>heads</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span>
</span><span id=__span-10-36><a id=__codelineno-10-36 name=__codelineno-10-36 href=#__codelineno-10-36></a>            <span class=n>layers</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_layers</span><span class=p>,</span>
</span><span id=__span-10-37><a id=__codelineno-10-37 name=__codelineno-10-37 href=#__codelineno-10-37></a>            <span class=n>vocab_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span>
</span><span id=__span-10-38><a id=__codelineno-10-38 name=__codelineno-10-38 href=#__codelineno-10-38></a>        <span class=p>)</span>
</span><span id=__span-10-39><a id=__codelineno-10-39 name=__codelineno-10-39 href=#__codelineno-10-39></a>
</span><span id=__span-10-40><a id=__codelineno-10-40 name=__codelineno-10-40 href=#__codelineno-10-40></a>    <span class=k>def</span><span class=w> </span><span class=nf>init_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-41><a id=__codelineno-10-41 name=__codelineno-10-41 href=#__codelineno-10-41></a>        <span class=c1># Custom weight initialization</span>
</span><span id=__span-10-42><a id=__codelineno-10-42 name=__codelineno-10-42 href=#__codelineno-10-42></a>        <span class=k>def</span><span class=w> </span><span class=nf>_init_weights</span><span class=p>(</span><span class=n>module</span><span class=p>):</span>
</span><span id=__span-10-43><a id=__codelineno-10-43 name=__codelineno-10-43 href=#__codelineno-10-43></a>            <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>
</span><span id=__span-10-44><a id=__codelineno-10-44 name=__codelineno-10-44 href=#__codelineno-10-44></a>                <span class=n>module</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mf>0.02</span><span class=p>)</span>
</span><span id=__span-10-45><a id=__codelineno-10-45 name=__codelineno-10-45 href=#__codelineno-10-45></a>                <span class=k>if</span> <span class=n>module</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-10-46><a id=__codelineno-10-46 name=__codelineno-10-46 href=#__codelineno-10-46></a>                    <span class=n>module</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span><span id=__span-10-47><a id=__codelineno-10-47 name=__codelineno-10-47 href=#__codelineno-10-47></a>
</span><span id=__span-10-48><a id=__codelineno-10-48 name=__codelineno-10-48 href=#__codelineno-10-48></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>_init_weights</span><span class=p>)</span>
</span><span id=__span-10-49><a id=__codelineno-10-49 name=__codelineno-10-49 href=#__codelineno-10-49></a>
</span><span id=__span-10-50><a id=__codelineno-10-50 name=__codelineno-10-50 href=#__codelineno-10-50></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_optimizers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-51><a id=__codelineno-10-51 name=__codelineno-10-51 href=#__codelineno-10-51></a>        <span class=c1># Separate weight decay for different param groups</span>
</span><span id=__span-10-52><a id=__codelineno-10-52 name=__codelineno-10-52 href=#__codelineno-10-52></a>        <span class=n>decay_params</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-10-53><a id=__codelineno-10-53 name=__codelineno-10-53 href=#__codelineno-10-53></a>        <span class=n>no_decay_params</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-10-54><a id=__codelineno-10-54 name=__codelineno-10-54 href=#__codelineno-10-54></a>
</span><span id=__span-10-55><a id=__codelineno-10-55 name=__codelineno-10-55 href=#__codelineno-10-55></a>        <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span><span id=__span-10-56><a id=__codelineno-10-56 name=__codelineno-10-56 href=#__codelineno-10-56></a>            <span class=k>if</span> <span class=s2>&quot;bias&quot;</span> <span class=ow>in</span> <span class=n>name</span> <span class=ow>or</span> <span class=s2>&quot;norm&quot;</span> <span class=ow>in</span> <span class=n>name</span><span class=p>:</span>
</span><span id=__span-10-57><a id=__codelineno-10-57 name=__codelineno-10-57 href=#__codelineno-10-57></a>                <span class=n>no_decay_params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>
</span><span id=__span-10-58><a id=__codelineno-10-58 name=__codelineno-10-58 href=#__codelineno-10-58></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-10-59><a id=__codelineno-10-59 name=__codelineno-10-59 href=#__codelineno-10-59></a>                <span class=n>decay_params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>
</span><span id=__span-10-60><a id=__codelineno-10-60 name=__codelineno-10-60 href=#__codelineno-10-60></a>
</span><span id=__span-10-61><a id=__codelineno-10-61 name=__codelineno-10-61 href=#__codelineno-10-61></a>        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>([</span>
</span><span id=__span-10-62><a id=__codelineno-10-62 name=__codelineno-10-62 href=#__codelineno-10-62></a>            <span class=p>{</span><span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=n>decay_params</span><span class=p>,</span> <span class=s2>&quot;weight_decay&quot;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>},</span>
</span><span id=__span-10-63><a id=__codelineno-10-63 name=__codelineno-10-63 href=#__codelineno-10-63></a>            <span class=p>{</span><span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=n>no_decay_params</span><span class=p>,</span> <span class=s2>&quot;weight_decay&quot;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>}</span>
</span><span id=__span-10-64><a id=__codelineno-10-64 name=__codelineno-10-64 href=#__codelineno-10-64></a>        <span class=p>],</span> <span class=n>lr</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>learning_rate</span><span class=p>)</span>
</span><span id=__span-10-65><a id=__codelineno-10-65 name=__codelineno-10-65 href=#__codelineno-10-65></a>
</span><span id=__span-10-66><a id=__codelineno-10-66 name=__codelineno-10-66 href=#__codelineno-10-66></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_schedulers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-67><a id=__codelineno-10-67 name=__codelineno-10-67 href=#__codelineno-10-67></a>        <span class=c1># Cosine annealing with warmup</span>
</span><span id=__span-10-68><a id=__codelineno-10-68 name=__codelineno-10-68 href=#__codelineno-10-68></a>        <span class=bp>self</span><span class=o>.</span><span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>CosineAnnealingLR</span><span class=p>(</span>
</span><span id=__span-10-69><a id=__codelineno-10-69 name=__codelineno-10-69 href=#__codelineno-10-69></a>            <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>,</span>
</span><span id=__span-10-70><a id=__codelineno-10-70 name=__codelineno-10-70 href=#__codelineno-10-70></a>            <span class=n>T_max</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>training_parameters</span><span class=o>.</span><span class=n>epochs</span>
</span><span id=__span-10-71><a id=__codelineno-10-71 name=__codelineno-10-71 href=#__codelineno-10-71></a>        <span class=p>)</span>
</span><span id=__span-10-72><a id=__codelineno-10-72 name=__codelineno-10-72 href=#__codelineno-10-72></a>
</span><span id=__span-10-73><a id=__codelineno-10-73 name=__codelineno-10-73 href=#__codelineno-10-73></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_dataloaders</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-74><a id=__codelineno-10-74 name=__codelineno-10-74 href=#__codelineno-10-74></a>        <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>MyDataset</span><span class=p>(</span><span class=s2>&quot;train&quot;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-10-75><a id=__codelineno-10-75 name=__codelineno-10-75 href=#__codelineno-10-75></a>        <span class=n>val_dataset</span> <span class=o>=</span> <span class=n>MyDataset</span><span class=p>(</span><span class=s2>&quot;validation&quot;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-10-76><a id=__codelineno-10-76 name=__codelineno-10-76 href=#__codelineno-10-76></a>
</span><span id=__span-10-77><a id=__codelineno-10-77 name=__codelineno-10-77 href=#__codelineno-10-77></a>        <span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-10-78><a id=__codelineno-10-78 name=__codelineno-10-78 href=#__codelineno-10-78></a>            <span class=n>train_dataset</span><span class=p>,</span>
</span><span id=__span-10-79><a id=__codelineno-10-79 name=__codelineno-10-79 href=#__codelineno-10-79></a>            <span class=n>batch_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
</span><span id=__span-10-80><a id=__codelineno-10-80 name=__codelineno-10-80 href=#__codelineno-10-80></a>            <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-10-81><a id=__codelineno-10-81 name=__codelineno-10-81 href=#__codelineno-10-81></a>            <span class=n>num_workers</span><span class=o>=</span><span class=mi>4</span>
</span><span id=__span-10-82><a id=__codelineno-10-82 name=__codelineno-10-82 href=#__codelineno-10-82></a>        <span class=p>)</span>
</span><span id=__span-10-83><a id=__codelineno-10-83 name=__codelineno-10-83 href=#__codelineno-10-83></a>
</span><span id=__span-10-84><a id=__codelineno-10-84 name=__codelineno-10-84 href=#__codelineno-10-84></a>        <span class=n>val_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-10-85><a id=__codelineno-10-85 name=__codelineno-10-85 href=#__codelineno-10-85></a>            <span class=n>val_dataset</span><span class=p>,</span>
</span><span id=__span-10-86><a id=__codelineno-10-86 name=__codelineno-10-86 href=#__codelineno-10-86></a>            <span class=n>batch_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
</span><span id=__span-10-87><a id=__codelineno-10-87 name=__codelineno-10-87 href=#__codelineno-10-87></a>            <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span><span id=__span-10-88><a id=__codelineno-10-88 name=__codelineno-10-88 href=#__codelineno-10-88></a>            <span class=n>num_workers</span><span class=o>=</span><span class=mi>4</span>
</span><span id=__span-10-89><a id=__codelineno-10-89 name=__codelineno-10-89 href=#__codelineno-10-89></a>        <span class=p>)</span>
</span><span id=__span-10-90><a id=__codelineno-10-90 name=__codelineno-10-90 href=#__codelineno-10-90></a>
</span><span id=__span-10-91><a id=__codelineno-10-91 name=__codelineno-10-91 href=#__codelineno-10-91></a>        <span class=k>return</span> <span class=n>train_loader</span><span class=p>,</span> <span class=n>val_loader</span>
</span><span id=__span-10-92><a id=__codelineno-10-92 name=__codelineno-10-92 href=#__codelineno-10-92></a>
</span><span id=__span-10-93><a id=__codelineno-10-93 name=__codelineno-10-93 href=#__codelineno-10-93></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure_metrics</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-10-94><a id=__codelineno-10-94 name=__codelineno-10-94 href=#__codelineno-10-94></a>        <span class=bp>self</span><span class=o>.</span><span class=n>perplexity</span> <span class=o>=</span> <span class=n>torchmetrics</span><span class=o>.</span><span class=n>Perplexity</span><span class=p>()</span>
</span><span id=__span-10-95><a id=__codelineno-10-95 name=__codelineno-10-95 href=#__codelineno-10-95></a>
</span><span id=__span-10-96><a id=__codelineno-10-96 name=__codelineno-10-96 href=#__codelineno-10-96></a>    <span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-10-97><a id=__codelineno-10-97 name=__codelineno-10-97 href=#__codelineno-10-97></a>        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span>
</span><span id=__span-10-98><a id=__codelineno-10-98 name=__codelineno-10-98 href=#__codelineno-10-98></a>        <span class=n>labels</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span>
</span><span id=__span-10-99><a id=__codelineno-10-99 name=__codelineno-10-99 href=#__codelineno-10-99></a>
</span><span id=__span-10-100><a id=__codelineno-10-100 name=__codelineno-10-100 href=#__codelineno-10-100></a>        <span class=c1># Forward pass</span>
</span><span id=__span-10-101><a id=__codelineno-10-101 name=__codelineno-10-101 href=#__codelineno-10-101></a>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span><span id=__span-10-102><a id=__codelineno-10-102 name=__codelineno-10-102 href=#__codelineno-10-102></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span>
</span><span id=__span-10-103><a id=__codelineno-10-103 name=__codelineno-10-103 href=#__codelineno-10-103></a>            <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span>
</span><span id=__span-10-104><a id=__codelineno-10-104 name=__codelineno-10-104 href=#__codelineno-10-104></a>            <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-10-105><a id=__codelineno-10-105 name=__codelineno-10-105 href=#__codelineno-10-105></a>        <span class=p>)</span>
</span><span id=__span-10-106><a id=__codelineno-10-106 name=__codelineno-10-106 href=#__codelineno-10-106></a>
</span><span id=__span-10-107><a id=__codelineno-10-107 name=__codelineno-10-107 href=#__codelineno-10-107></a>        <span class=c1># Backward pass with gradient accumulation</span>
</span><span id=__span-10-108><a id=__codelineno-10-108 name=__codelineno-10-108 href=#__codelineno-10-108></a>        <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-10-109><a id=__codelineno-10-109 name=__codelineno-10-109 href=#__codelineno-10-109></a>
</span><span id=__span-10-110><a id=__codelineno-10-110 name=__codelineno-10-110 href=#__codelineno-10-110></a>        <span class=c1># Step optimizer when not accumulating</span>
</span><span id=__span-10-111><a id=__codelineno-10-111 name=__codelineno-10-111 href=#__codelineno-10-111></a>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-10-112><a id=__codelineno-10-112 name=__codelineno-10-112 href=#__codelineno-10-112></a>            <span class=n>grad_norm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-10-113><a id=__codelineno-10-113 name=__codelineno-10-113 href=#__codelineno-10-113></a>            <span class=bp>self</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s2>&quot;train/grad_norm&quot;</span><span class=p>,</span> <span class=n>grad_norm</span><span class=p>)</span>
</span><span id=__span-10-114><a id=__codelineno-10-114 name=__codelineno-10-114 href=#__codelineno-10-114></a>
</span><span id=__span-10-115><a id=__codelineno-10-115 name=__codelineno-10-115 href=#__codelineno-10-115></a>        <span class=c1># Log metrics</span>
</span><span id=__span-10-116><a id=__codelineno-10-116 name=__codelineno-10-116 href=#__codelineno-10-116></a>        <span class=bp>self</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s2>&quot;train/loss&quot;</span><span class=p>,</span> <span class=n>loss</span><span class=p>)</span>
</span><span id=__span-10-117><a id=__codelineno-10-117 name=__codelineno-10-117 href=#__codelineno-10-117></a>        <span class=bp>self</span><span class=o>.</span><span class=n>log_scalar</span><span class=p>(</span><span class=s2>&quot;train/lr&quot;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s2>&quot;lr&quot;</span><span class=p>])</span>
</span><span id=__span-10-118><a id=__codelineno-10-118 name=__codelineno-10-118 href=#__codelineno-10-118></a>
</span><span id=__span-10-119><a id=__codelineno-10-119 name=__codelineno-10-119 href=#__codelineno-10-119></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span><span id=__span-10-120><a id=__codelineno-10-120 name=__codelineno-10-120 href=#__codelineno-10-120></a>
</span><span id=__span-10-121><a id=__codelineno-10-121 name=__codelineno-10-121 href=#__codelineno-10-121></a>    <span class=k>def</span><span class=w> </span><span class=nf>validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-10-122><a id=__codelineno-10-122 name=__codelineno-10-122 href=#__codelineno-10-122></a>        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input_ids&quot;</span><span class=p>]</span>
</span><span id=__span-10-123><a id=__codelineno-10-123 name=__codelineno-10-123 href=#__codelineno-10-123></a>        <span class=n>labels</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;labels&quot;</span><span class=p>]</span>
</span><span id=__span-10-124><a id=__codelineno-10-124 name=__codelineno-10-124 href=#__codelineno-10-124></a>
</span><span id=__span-10-125><a id=__codelineno-10-125 name=__codelineno-10-125 href=#__codelineno-10-125></a>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span><span id=__span-10-126><a id=__codelineno-10-126 name=__codelineno-10-126 href=#__codelineno-10-126></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span>
</span><span id=__span-10-127><a id=__codelineno-10-127 name=__codelineno-10-127 href=#__codelineno-10-127></a>            <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span>
</span><span id=__span-10-128><a id=__codelineno-10-128 name=__codelineno-10-128 href=#__codelineno-10-128></a>            <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-10-129><a id=__codelineno-10-129 name=__codelineno-10-129 href=#__codelineno-10-129></a>        <span class=p>)</span>
</span><span id=__span-10-130><a id=__codelineno-10-130 name=__codelineno-10-130 href=#__codelineno-10-130></a>
</span><span id=__span-10-131><a id=__codelineno-10-131 name=__codelineno-10-131 href=#__codelineno-10-131></a>        <span class=c1># Update metrics</span>
</span><span id=__span-10-132><a id=__codelineno-10-132 name=__codelineno-10-132 href=#__codelineno-10-132></a>        <span class=bp>self</span><span class=o>.</span><span class=n>perplexity</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span><span id=__span-10-133><a id=__codelineno-10-133 name=__codelineno-10-133 href=#__codelineno-10-133></a>
</span><span id=__span-10-134><a id=__codelineno-10-134 name=__codelineno-10-134 href=#__codelineno-10-134></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;val_loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span></code></pre></div> <h2 id=training-loop-lifecycle>Training Loop Lifecycle<a class=headerlink href=#training-loop-lifecycle title="Permanent link">&para;</a></h2> <p>The training loop in Dream Trainer follows a well-defined lifecycle with clear phases and hooks for customization:</p> <h3 id=initialization-phase>Initialization Phase<a class=headerlink href=#initialization-phase title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>MyTrainer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=c1># 1. __init__ is called</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=c1>#    - Initialize world/distributed setup</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=c1>#    - Create callback collection</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a><span class=c1>#    - Set initial state (epoch=0, global_step=0)</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a><span class=c1># 2. configure() is called</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a><span class=c1>#    - configure_models() on meta device</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a><span class=c1>#    - post_configure_models() hook</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a><span class=c1># 3. setup() is called</span>
</span><span id=__span-11-13><a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a><span class=c1>#    - Apply parallelism (TP, PP, FSDP)</span>
</span><span id=__span-11-14><a id=__codelineno-11-14 name=__codelineno-11-14 href=#__codelineno-11-14></a><span class=c1>#    - Initialize weights</span>
</span><span id=__span-11-15><a id=__codelineno-11-15 name=__codelineno-11-15 href=#__codelineno-11-15></a><span class=c1>#    - Setup optimizers &amp; schedulers</span>
</span><span id=__span-11-16><a id=__codelineno-11-16 name=__codelineno-11-16 href=#__codelineno-11-16></a><span class=c1>#    - Setup dataloaders</span>
</span><span id=__span-11-17><a id=__codelineno-11-17 name=__codelineno-11-17 href=#__codelineno-11-17></a><span class=c1>#    - Setup metrics</span>
</span></code></pre></div> <h3 id=training-phase>Training Phase<a class=headerlink href=#training-phase title="Permanent link">&para;</a></h3> <p>The training loop executes these steps for each epoch:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=k>def</span><span class=w> </span><span class=nf>perform_training_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>    <span class=bp>self</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># Set models to training mode</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>):</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>        <span class=c1># 1. Pre-batch callbacks</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_train_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a>        <span class=c1># 2. Training step with autocast</span>
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>        <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_context</span><span class=p>():</span>  <span class=c1># Includes autocast, profiling, etc.</span>
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>            <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_step</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>
</span><span id=__span-12-13><a id=__codelineno-12-13 name=__codelineno-12-13 href=#__codelineno-12-13></a>        <span class=c1># 3. Post-batch callbacks</span>
</span><span id=__span-12-14><a id=__codelineno-12-14 name=__codelineno-12-14 href=#__codelineno-12-14></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_train_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-12-15><a id=__codelineno-12-15 name=__codelineno-12-15 href=#__codelineno-12-15></a>
</span><span id=__span-12-16><a id=__codelineno-12-16 name=__codelineno-12-16 href=#__codelineno-12-16></a>        <span class=c1># 4. Increment counters</span>
</span><span id=__span-12-17><a id=__codelineno-12-17 name=__codelineno-12-17 href=#__codelineno-12-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>local_batches</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-12-18><a id=__codelineno-12-18 name=__codelineno-12-18 href=#__codelineno-12-18></a>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-12-19><a id=__codelineno-12-19 name=__codelineno-12-19 href=#__codelineno-12-19></a>            <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-12-20><a id=__codelineno-12-20 name=__codelineno-12-20 href=#__codelineno-12-20></a>
</span><span id=__span-12-21><a id=__codelineno-12-21 name=__codelineno-12-21 href=#__codelineno-12-21></a>        <span class=c1># 5. Validation check</span>
</span><span id=__span-12-22><a id=__codelineno-12-22 name=__codelineno-12-22 href=#__codelineno-12-22></a>        <span class=k>if</span> <span class=n>should_validate</span><span class=p>():</span>
</span><span id=__span-12-23><a id=__codelineno-12-23 name=__codelineno-12-23 href=#__codelineno-12-23></a>            <span class=bp>self</span><span class=o>.</span><span class=n>perform_validation_epoch</span><span class=p>()</span>
</span><span id=__span-12-24><a id=__codelineno-12-24 name=__codelineno-12-24 href=#__codelineno-12-24></a>
</span><span id=__span-12-25><a id=__codelineno-12-25 name=__codelineno-12-25 href=#__codelineno-12-25></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span></code></pre></div> <h3 id=gradient-accumulation-flow>Gradient Accumulation Flow<a class=headerlink href=#gradient-accumulation-flow title="Permanent link">&para;</a></h3> <p>Dream Trainer handles gradient accumulation automatically:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a>    <span class=c1># Your implementation</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a>    <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>    <span class=c1># backward() handles scaling by accumulation steps</span>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a>    <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>  <span class=c1># Internally: (loss / accumulation_steps).backward()</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a>    <span class=c1># Only step optimizer when gradients are ready</span>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a>    <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a>        <span class=c1># All gradients accumulated, time to step</span>
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a>        <span class=n>grad_norm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-13-12><a id=__codelineno-13-12 name=__codelineno-13-12 href=#__codelineno-13-12></a>
</span><span id=__span-13-13><a id=__codelineno-13-13 name=__codelineno-13-13 href=#__codelineno-13-13></a>    <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span></code></pre></div> <p>The <code>no_gradient_sync</code> context manager optimizes distributed training:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=c1># Automatically applied during accumulation steps</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_gradient_sync</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>  <span class=c1># No distributed sync until final accumulation</span>
</span></code></pre></div> <h3 id=validation-phase>Validation Phase<a class=headerlink href=#validation-phase title="Permanent link">&para;</a></h3> <p>Validation runs with gradients disabled and models in eval mode:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=nd>@torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>()</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=k>def</span><span class=w> </span><span class=nf>perform_validation_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>    <span class=bp>self</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>  <span class=c1># Set models to eval mode</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_val_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>val_dataloader</span><span class=p>):</span>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>        <span class=c1># 1. Pre-batch callbacks</span>
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_val_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a>
</span><span id=__span-15-10><a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a>        <span class=c1># 2. Validation step</span>
</span><span id=__span-15-11><a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>validation_step</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-15-12><a id=__codelineno-15-12 name=__codelineno-15-12 href=#__codelineno-15-12></a>
</span><span id=__span-15-13><a id=__codelineno-15-13 name=__codelineno-15-13 href=#__codelineno-15-13></a>        <span class=c1># 3. Post-batch callbacks</span>
</span><span id=__span-15-14><a id=__codelineno-15-14 name=__codelineno-15-14 href=#__codelineno-15-14></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_val_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-15-15><a id=__codelineno-15-15 name=__codelineno-15-15 href=#__codelineno-15-15></a>
</span><span id=__span-15-16><a id=__codelineno-15-16 name=__codelineno-15-16 href=#__codelineno-15-16></a>    <span class=c1># 4. Compute epoch metrics</span>
</span><span id=__span-15-17><a id=__codelineno-15-17 name=__codelineno-15-17 href=#__codelineno-15-17></a>    <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=s2>&quot;compute_metrics&quot;</span><span class=p>):</span>
</span><span id=__span-15-18><a id=__codelineno-15-18 name=__codelineno-15-18 href=#__codelineno-15-18></a>        <span class=n>metrics</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_metrics</span><span class=p>()</span>
</span><span id=__span-15-19><a id=__codelineno-15-19 name=__codelineno-15-19 href=#__codelineno-15-19></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>on_val_epoch_end</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>metrics</span><span class=p>)</span>
</span><span id=__span-15-20><a id=__codelineno-15-20 name=__codelineno-15-20 href=#__codelineno-15-20></a>
</span><span id=__span-15-21><a id=__codelineno-15-21 name=__codelineno-15-21 href=#__codelineno-15-21></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_val_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span></code></pre></div> <h3 id=callback-integration>Callback Integration<a class=headerlink href=#callback-integration title="Permanent link">&para;</a></h3> <p>Callbacks provide hooks at every stage of training:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>Callback</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a><span class=k>class</span><span class=w> </span><span class=nc>MyCallback</span><span class=p>(</span><span class=n>Callback</span><span class=p>):</span>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a>    <span class=k>def</span><span class=w> </span><span class=nf>on_train_start</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trainer</span><span class=p>):</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training started!&quot;</span><span class=p>)</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a>    <span class=k>def</span><span class=w> </span><span class=nf>on_train_batch_end</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trainer</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a>        <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-16-9><a id=__codelineno-16-9 name=__codelineno-16-9 href=#__codelineno-16-9></a>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Batch </span><span class=si>{</span><span class=n>batch_idx</span><span class=si>}</span><span class=s2>: loss = </span><span class=si>{</span><span class=n>output</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-16-10><a id=__codelineno-16-10 name=__codelineno-16-10 href=#__codelineno-16-10></a>
</span><span id=__span-16-11><a id=__codelineno-16-11 name=__codelineno-16-11 href=#__codelineno-16-11></a>    <span class=k>def</span><span class=w> </span><span class=nf>on_validation_end</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trainer</span><span class=p>,</span> <span class=n>metrics</span><span class=p>):</span>
</span><span id=__span-16-12><a id=__codelineno-16-12 name=__codelineno-16-12 href=#__codelineno-16-12></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Validation perplexity: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;perplexity&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=state-management>State Management<a class=headerlink href=#state-management title="Permanent link">&para;</a></h2> <p>Dream Trainer provides comprehensive state management for checkpointing and resumption:</p> <h3 id=trainer-state>Trainer State<a class=headerlink href=#trainer-state title="Permanent link">&para;</a></h3> <p>The trainer maintains several state variables:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=k>class</span><span class=w> </span><span class=nc>BaseTrainer</span><span class=p>:</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>    <span class=c1># Global state</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>    <span class=n>global_step</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span>        <span class=c1># Total optimizer steps across all epochs</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>    <span class=n>current_epoch</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span>      <span class=c1># Current epoch number</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>    <span class=n>local_batches</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span>      <span class=c1># Batches processed since start</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a>    <span class=c1># Training flags</span>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a>    <span class=n>training</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span>      <span class=c1># True during training, False during eval</span>
</span><span id=__span-17-9><a id=__codelineno-17-9 name=__codelineno-17-9 href=#__codelineno-17-9></a>
</span><span id=__span-17-10><a id=__codelineno-17-10 name=__codelineno-17-10 href=#__codelineno-17-10></a>    <span class=c1># Properties</span>
</span><span id=__span-17-11><a id=__codelineno-17-11 name=__codelineno-17-11 href=#__codelineno-17-11></a>    <span class=nd>@property</span>
</span><span id=__span-17-12><a id=__codelineno-17-12 name=__codelineno-17-12 href=#__codelineno-17-12></a>    <span class=k>def</span><span class=w> </span><span class=nf>is_accumulating_gradients</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span><span id=__span-17-13><a id=__codelineno-17-13 name=__codelineno-17-13 href=#__codelineno-17-13></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;True if currently accumulating gradients&quot;&quot;&quot;</span>
</span><span id=__span-17-14><a id=__codelineno-17-14 name=__codelineno-17-14 href=#__codelineno-17-14></a>        <span class=k>return</span> <span class=p>(</span>
</span><span id=__span-17-15><a id=__codelineno-17-15 name=__codelineno-17-15 href=#__codelineno-17-15></a>            <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_batches</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_gradient_accumulation_steps</span> <span class=o>!=</span> <span class=mi>0</span>
</span><span id=__span-17-16><a id=__codelineno-17-16 name=__codelineno-17-16 href=#__codelineno-17-16></a>        <span class=p>)</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_last_training_batch</span>
</span></code></pre></div> <h3 id=checkpointing>Checkpointing<a class=headerlink href=#checkpointing title="Permanent link">&para;</a></h3> <p>State dict includes all components needed for exact resumption:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=k>def</span><span class=w> </span><span class=nf>state_dict</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a>        <span class=s2>&quot;trainer&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a>            <span class=s2>&quot;global_step&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span><span class=p>,</span>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a>            <span class=s2>&quot;current_epoch&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_epoch</span><span class=p>,</span>
</span><span id=__span-18-6><a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a>            <span class=s2>&quot;callbacks&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span><span id=__span-18-7><a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a>        <span class=p>},</span>
</span><span id=__span-18-8><a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a>        <span class=s2>&quot;models&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-18-9><a id=__codelineno-18-9 name=__codelineno-18-9 href=#__codelineno-18-9></a>            <span class=n>name</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span> 
</span><span id=__span-18-10><a id=__codelineno-18-10 name=__codelineno-18-10 href=#__codelineno-18-10></a>            <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_models</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-18-11><a id=__codelineno-18-11 name=__codelineno-18-11 href=#__codelineno-18-11></a>        <span class=p>},</span>
</span><span id=__span-18-12><a id=__codelineno-18-12 name=__codelineno-18-12 href=#__codelineno-18-12></a>        <span class=s2>&quot;optimizers&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-18-13><a id=__codelineno-18-13 name=__codelineno-18-13 href=#__codelineno-18-13></a>            <span class=n>name</span><span class=p>:</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span><span id=__span-18-14><a id=__codelineno-18-14 name=__codelineno-18-14 href=#__codelineno-18-14></a>            <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>optimizer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_optimizers</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-18-15><a id=__codelineno-18-15 name=__codelineno-18-15 href=#__codelineno-18-15></a>        <span class=p>},</span>
</span><span id=__span-18-16><a id=__codelineno-18-16 name=__codelineno-18-16 href=#__codelineno-18-16></a>        <span class=s2>&quot;schedulers&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-18-17><a id=__codelineno-18-17 name=__codelineno-18-17 href=#__codelineno-18-17></a>            <span class=n>name</span><span class=p>:</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span><span id=__span-18-18><a id=__codelineno-18-18 name=__codelineno-18-18 href=#__codelineno-18-18></a>            <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>scheduler</span> <span class=ow>in</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>named_schedulers</span><span class=p>()</span> <span class=ow>or</span> <span class=p>{})</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-18-19><a id=__codelineno-18-19 name=__codelineno-18-19 href=#__codelineno-18-19></a>        <span class=p>},</span>
</span><span id=__span-18-20><a id=__codelineno-18-20 name=__codelineno-18-20 href=#__codelineno-18-20></a>        <span class=s2>&quot;dataloaders&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-18-21><a id=__codelineno-18-21 name=__codelineno-18-21 href=#__codelineno-18-21></a>            <span class=s2>&quot;train&quot;</span><span class=p>:</span> <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>,</span> <span class=s2>&quot;state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=p>{})(),</span>
</span><span id=__span-18-22><a id=__codelineno-18-22 name=__codelineno-18-22 href=#__codelineno-18-22></a>            <span class=s2>&quot;val&quot;</span><span class=p>:</span> <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>val_dataloader</span><span class=p>,</span> <span class=s2>&quot;state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=p>{})(),</span>
</span><span id=__span-18-23><a id=__codelineno-18-23 name=__codelineno-18-23 href=#__codelineno-18-23></a>        <span class=p>},</span>
</span><span id=__span-18-24><a id=__codelineno-18-24 name=__codelineno-18-24 href=#__codelineno-18-24></a>    <span class=p>}</span>
</span><span id=__span-18-25><a id=__codelineno-18-25 name=__codelineno-18-25 href=#__codelineno-18-25></a>
</span><span id=__span-18-26><a id=__codelineno-18-26 name=__codelineno-18-26 href=#__codelineno-18-26></a><span class=k>def</span><span class=w> </span><span class=nf>load_state_dict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state_dict</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>strict</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span><span id=__span-18-27><a id=__codelineno-18-27 name=__codelineno-18-27 href=#__codelineno-18-27></a>    <span class=c1># Restore all components</span>
</span><span id=__span-18-28><a id=__codelineno-18-28 name=__codelineno-18-28 href=#__codelineno-18-28></a>    <span class=n>trainer_state</span> <span class=o>=</span> <span class=n>state_dict</span><span class=p>[</span><span class=s2>&quot;trainer&quot;</span><span class=p>]</span>
</span><span id=__span-18-29><a id=__codelineno-18-29 name=__codelineno-18-29 href=#__codelineno-18-29></a>    <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span> <span class=o>=</span> <span class=n>trainer_state</span><span class=p>[</span><span class=s2>&quot;global_step&quot;</span><span class=p>]</span>
</span><span id=__span-18-30><a id=__codelineno-18-30 name=__codelineno-18-30 href=#__codelineno-18-30></a>    <span class=bp>self</span><span class=o>.</span><span class=n>current_epoch</span> <span class=o>=</span> <span class=n>trainer_state</span><span class=p>[</span><span class=s2>&quot;current_epoch&quot;</span><span class=p>]</span>
</span><span id=__span-18-31><a id=__codelineno-18-31 name=__codelineno-18-31 href=#__codelineno-18-31></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>trainer_state</span><span class=p>[</span><span class=s2>&quot;callbacks&quot;</span><span class=p>])</span>
</span><span id=__span-18-32><a id=__codelineno-18-32 name=__codelineno-18-32 href=#__codelineno-18-32></a>
</span><span id=__span-18-33><a id=__codelineno-18-33 name=__codelineno-18-33 href=#__codelineno-18-33></a>    <span class=c1># Load model, optimizer, scheduler states...</span>
</span></code></pre></div> <h3 id=distributed-state>Distributed State<a class=headerlink href=#distributed-state title="Permanent link">&para;</a></h3> <p>The <code>DistributedWorld</code> class manages distributed training state:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=k>class</span><span class=w> </span><span class=nc>DistributedWorld</span><span class=p>:</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>device_parameters</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>):</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>        <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>()</span>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>local_rank</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_local_rank</span><span class=p>()</span>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a>        <span class=c1># Device meshes for different parallelism types</span>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>dp_mesh</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_build_mesh</span><span class=p>(</span><span class=s2>&quot;dp&quot;</span><span class=p>,</span> <span class=n>device_parameters</span><span class=p>)</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>tp_mesh</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_build_mesh</span><span class=p>(</span><span class=s2>&quot;tp&quot;</span><span class=p>,</span> <span class=n>device_parameters</span><span class=p>)</span>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pp_mesh</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_build_mesh</span><span class=p>(</span><span class=s2>&quot;pp&quot;</span><span class=p>,</span> <span class=n>device_parameters</span><span class=p>)</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a>
</span><span id=__span-19-12><a id=__codelineno-19-12 name=__codelineno-19-12 href=#__codelineno-19-12></a>    <span class=nd>@contextlib</span><span class=o>.</span><span class=n>contextmanager</span>
</span><span id=__span-19-13><a id=__codelineno-19-13 name=__codelineno-19-13 href=#__codelineno-19-13></a>    <span class=k>def</span><span class=w> </span><span class=nf>train_context</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-19-14><a id=__codelineno-19-14 name=__codelineno-19-14 href=#__codelineno-19-14></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Context for training with autocast and other optimizations&quot;&quot;&quot;</span>
</span><span id=__span-19-15><a id=__codelineno-19-15 name=__codelineno-19-15 href=#__codelineno-19-15></a>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>amp</span><span class=o>.</span><span class=n>autocast</span><span class=p>(</span>
</span><span id=__span-19-16><a id=__codelineno-19-16 name=__codelineno-19-16 href=#__codelineno-19-16></a>            <span class=n>enabled</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>mixed_precision_enabled</span><span class=p>,</span>
</span><span id=__span-19-17><a id=__codelineno-19-17 name=__codelineno-19-17 href=#__codelineno-19-17></a>            <span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>mixed_precision_dtype</span>
</span><span id=__span-19-18><a id=__codelineno-19-18 name=__codelineno-19-18 href=#__codelineno-19-18></a>        <span class=p>):</span>
</span><span id=__span-19-19><a id=__codelineno-19-19 name=__codelineno-19-19 href=#__codelineno-19-19></a>            <span class=k>yield</span>
</span><span id=__span-19-20><a id=__codelineno-19-20 name=__codelineno-19-20 href=#__codelineno-19-20></a>
</span><span id=__span-19-21><a id=__codelineno-19-21 name=__codelineno-19-21 href=#__codelineno-19-21></a>    <span class=k>def</span><span class=w> </span><span class=nf>all_reduce</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-19-22><a id=__codelineno-19-22 name=__codelineno-19-22 href=#__codelineno-19-22></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;All-reduce across data parallel group&quot;&quot;&quot;</span>
</span><span id=__span-19-23><a id=__codelineno-19-23 name=__codelineno-19-23 href=#__codelineno-19-23></a>        <span class=k>return</span> <span class=n>dist_ops</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dp_process_group</span><span class=p>)</span>
</span><span id=__span-19-24><a id=__codelineno-19-24 name=__codelineno-19-24 href=#__codelineno-19-24></a>
</span><span id=__span-19-25><a id=__codelineno-19-25 name=__codelineno-19-25 href=#__codelineno-19-25></a>    <span class=k>def</span><span class=w> </span><span class=nf>all_gather</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-19-26><a id=__codelineno-19-26 name=__codelineno-19-26 href=#__codelineno-19-26></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;All-gather across data parallel group&quot;&quot;&quot;</span>
</span><span id=__span-19-27><a id=__codelineno-19-27 name=__codelineno-19-27 href=#__codelineno-19-27></a>        <span class=k>return</span> <span class=n>dist_ops</span><span class=o>.</span><span class=n>all_gather_tensor</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dp_process_group</span><span class=p>)</span>
</span></code></pre></div> <h3 id=fault-tolerance>Fault Tolerance<a class=headerlink href=#fault-tolerance title="Permanent link">&para;</a></h3> <p>Dream Trainer integrates with PyTorch's fault tolerance mechanisms:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># Automatic checkpoint saving</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=n>checkpoint_callback</span> <span class=o>=</span> <span class=n>CheckpointCallback</span><span class=p>(</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>    <span class=n>checkpoint_dir</span><span class=o>=</span><span class=s2>&quot;./checkpoints&quot;</span><span class=p>,</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a>    <span class=n>save_frequency</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>  <span class=c1># Save every 1000 steps</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a>    <span class=n>keep_last_n</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>        <span class=c1># Keep last 3 checkpoints</span>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a><span class=p>)</span>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a>
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=c1># Fault-tolerant training with torchft</span>
</span><span id=__span-20-9><a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>enable_fault_tolerance</span><span class=p>:</span>
</span><span id=__span-20-10><a id=__codelineno-20-10 name=__codelineno-20-10 href=#__codelineno-20-10></a>    <span class=kn>from</span><span class=w> </span><span class=nn>torchft</span><span class=w> </span><span class=kn>import</span> <span class=n>FaultTolerantWorld</span>
</span><span id=__span-20-11><a id=__codelineno-20-11 name=__codelineno-20-11 href=#__codelineno-20-11></a>    <span class=n>trainer</span><span class=o>.</span><span class=n>world</span> <span class=o>=</span> <span class=n>FaultTolerantWorld</span><span class=p>(</span><span class=n>trainer</span><span class=o>.</span><span class=n>world</span><span class=p>)</span>
</span></code></pre></div> <h3 id=memory-management>Memory Management<a class=headerlink href=#memory-management title="Permanent link">&para;</a></h3> <p>DTensor enables efficient memory usage through sharding:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=c1># Memory usage comparison</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=c1># Traditional: Each GPU stores full 405B parameter model = 810GB per GPU</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a><span class=c1># With FSDP sharding: 810GB / 8 GPUs = ~101GB per GPU</span>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a><span class=c1># Configure memory-efficient training</span>
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a><span class=n>config</span> <span class=o>=</span> <span class=n>DreamTrainerConfig</span><span class=p>(</span>
</span><span id=__span-21-7><a id=__codelineno-21-7 name=__codelineno-21-7 href=#__codelineno-21-7></a>    <span class=n>device_parameters</span><span class=o>=</span><span class=n>DeviceParameters</span><span class=p>(</span>
</span><span id=__span-21-8><a id=__codelineno-21-8 name=__codelineno-21-8 href=#__codelineno-21-8></a>        <span class=n>dp_degree</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>  <span class=c1># Shard across 8 GPUs</span>
</span><span id=__span-21-9><a id=__codelineno-21-9 name=__codelineno-21-9 href=#__codelineno-21-9></a>        <span class=n>enable_fsdp</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-21-10><a id=__codelineno-21-10 name=__codelineno-21-10 href=#__codelineno-21-10></a>        <span class=n>fsdp_limit_all_gathers</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># Rate limit to save memory</span>
</span><span id=__span-21-11><a id=__codelineno-21-11 name=__codelineno-21-11 href=#__codelineno-21-11></a>        <span class=n>fsdp_forward_prefetch</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>   <span class=c1># Overlap computation/communication</span>
</span><span id=__span-21-12><a id=__codelineno-21-12 name=__codelineno-21-12 href=#__codelineno-21-12></a>    <span class=p>)</span>
</span><span id=__span-21-13><a id=__codelineno-21-13 name=__codelineno-21-13 href=#__codelineno-21-13></a><span class=p>)</span>
</span></code></pre></div> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <p>Now that you understand the core concepts:</p> <ol> <li><strong><a href=../getting-started/ >Getting Started</a></strong>: Install Dream Trainer and run your first training</li> <li><strong><a href=../configuration/ >Configuration Guide</a></strong>: Deep dive into configuration options</li> <li><strong><a href=../trainer-guide/ >Trainer Guide</a></strong>: Build custom trainers for your use case</li> <li><strong><a href=../callbacks/ >Callbacks</a></strong>: Extend functionality with the callback system</li> <li><strong><a href=../api/ >API Reference</a></strong>: Detailed API documentation</li> </ol> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">&para;</a></h2> <ul> <li><a href=https://github.com/pytorch/pytorch/issues/88838>PyTorch DTensor RFC</a></li> <li><a href=https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>PyTorch FSDP Tutorial</a></li> <li><a href=https://arxiv.org/html/2410.06511v1>TorchTitan: Production-Ready LLM Training</a> </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../getting-started/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Quick Start"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Quick Start </div> </div> </a> <a href=../configuration/ class="md-footer__link md-footer__link--next" aria-label="Next: Configuration"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Configuration </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dream3d/dream-trainer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://discord.gg/dream-trainer target=_blank rel=noopener title=discord.gg class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> <a href=https://twitter.com/dream3d_ai target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.prune", "navigation.indexes", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tooltips", "toc.follow", "toc.integrate"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script> <script src=../assets/javascripts/bundle.56ea9cef.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>