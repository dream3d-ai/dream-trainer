<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Composable distributed training framework built around PyTorch DTensor abstractions"><link href=https://dream3d.ai/trainer/api/utilities/world/ rel=canonical><link href=../../configuration/training/ rel=prev><link href=../data/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.15"><title>World Management - dream-trainer</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../css/timeago.css><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#world-management class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=dream-trainer class="md-header__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> dream-trainer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> World Management </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../installation/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../../configuration/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../../../tutorials/first-trainer.md class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../../../examples/vision.md class=md-tabs__link> Examples </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../../contributing.md class=md-tabs__link> Community </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=dream-trainer class="md-nav__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> dream-trainer </label> <div class=md-nav__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../installation/ class=md-nav__link> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../configuration/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../tutorials/first-trainer.md class=md-nav__link> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../examples/vision.md class=md-nav__link> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> API Reference </span> </a> <label class="md-nav__link " for=__nav_6 id=__nav_6_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=true> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_2> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex> <span class=md-ellipsis> Trainers </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> Trainers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../trainers/abstract/ class=md-nav__link> <span class=md-ellipsis> AbstractTrainer </span> </a> </li> <li class=md-nav__item> <a href=../../trainers/base/ class=md-nav__link> <span class=md-ellipsis> BaseTrainer </span> </a> </li> <li class=md-nav__item> <a href=../../trainers/dream/ class=md-nav__link> <span class=md-ellipsis> DreamTrainer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_3> <label class=md-nav__link for=__nav_6_3 id=__nav_6_3_label tabindex> <span class=md-ellipsis> Mixins </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_3_label aria-expanded=false> <label class=md-nav__title for=__nav_6_3> <span class="md-nav__icon md-icon"></span> Mixins </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../mixins/setup/ class=md-nav__link> <span class=md-ellipsis> Setup Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/eval_metric/ class=md-nav__link> <span class=md-ellipsis> Evaluation Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/loggers/ class=md-nav__link> <span class=md-ellipsis> Logger Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/quantize/ class=md-nav__link> <span class=md-ellipsis> Quantization Mixins </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_4> <label class=md-nav__link for=__nav_6_4 id=__nav_6_4_label tabindex> <span class=md-ellipsis> Callbacks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_4_label aria-expanded=false> <label class=md-nav__title for=__nav_6_4> <span class="md-nav__icon md-icon"></span> Callbacks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../callbacks/base/ class=md-nav__link> <span class=md-ellipsis> Callback Base </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/checkpoint/ class=md-nav__link> <span class=md-ellipsis> Checkpoint Callbacks </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/monitoring/ class=md-nav__link> <span class=md-ellipsis> Monitoring Callbacks </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/performance/ class=md-nav__link> <span class=md-ellipsis> Performance Callbacks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_5> <label class=md-nav__link for=__nav_6_5 id=__nav_6_5_label tabindex> <span class=md-ellipsis> Configuration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_5_label aria-expanded=false> <label class=md-nav__title for=__nav_6_5> <span class="md-nav__icon md-icon"></span> Configuration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../configuration/parameters/ class=md-nav__link> <span class=md-ellipsis> Parameter Classes </span> </a> </li> <li class=md-nav__item> <a href=../../configuration/device/ class=md-nav__link> <span class=md-ellipsis> Device Config </span> </a> </li> <li class=md-nav__item> <a href=../../configuration/training/ class=md-nav__link> <span class=md-ellipsis> Training Config </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_6 checked> <label class=md-nav__link for=__nav_6_6 id=__nav_6_6_label tabindex> <span class=md-ellipsis> Utilities </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_6_label aria-expanded=true> <label class=md-nav__title for=__nav_6_6> <span class="md-nav__icon md-icon"></span> Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> World Management </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> World Management </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#distributedworld class=md-nav__link> <span class=md-ellipsis> DistributedWorld </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DistributedWorld </span> </a> <nav class=md-nav aria-label= DistributedWorld> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld-attributes class=md-nav__link> <span class=md-ellipsis> Attributes </span> </a> <nav class=md-nav aria-label=Attributes> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.is_global_zero class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;is_global_zero </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld-functions class=md-nav__link> <span class=md-ellipsis> Functions </span> </a> <nav class=md-nav aria-label=Functions> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.get_mesh class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_mesh </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.all_reduce class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;all_reduce </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.barrier class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;barrier </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.get_total_norm class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_total_norm </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.train_context class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train_context </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.DistributedWorld.loss_parallel class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;loss_parallel </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#properties class=md-nav__link> <span class=md-ellipsis> Properties </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#usage-examples class=md-nav__link> <span class=md-ellipsis> Usage Examples </span> </a> <nav class=md-nav aria-label="Usage Examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#basic-usage class=md-nav__link> <span class=md-ellipsis> Basic Usage </span> </a> </li> <li class=md-nav__item> <a href=#collective-operations class=md-nav__link> <span class=md-ellipsis> Collective Operations </span> </a> </li> <li class=md-nav__item> <a href=#device-meshes class=md-nav__link> <span class=md-ellipsis> Device Meshes </span> </a> </li> <li class=md-nav__item> <a href=#training-context class=md-nav__link> <span class=md-ellipsis> Training Context </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#faulttolerantworld class=md-nav__link> <span class=md-ellipsis> FaultTolerantWorld </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.world.FaultTolerantWorld class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FaultTolerantWorld </span> </a> <nav class=md-nav aria-label= FaultTolerantWorld> <ul class=md-nav__list> <li class=md-nav__item> <a href=#usage class=md-nav__link> <span class=md-ellipsis> Usage </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#fsdp-configuration class=md-nav__link> <span class=md-ellipsis> FSDP Configuration </span> </a> </li> <li class=md-nav__item> <a href=#gradient-operations class=md-nav__link> <span class=md-ellipsis> Gradient Operations </span> </a> </li> <li class=md-nav__item> <a href=#advanced-features class=md-nav__link> <span class=md-ellipsis> Advanced Features </span> </a> <nav class=md-nav aria-label="Advanced Features"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#custom-mesh-construction class=md-nav__link> <span class=md-ellipsis> Custom Mesh Construction </span> </a> </li> <li class=md-nav__item> <a href=#context-parallelism class=md-nav__link> <span class=md-ellipsis> Context Parallelism </span> </a> </li> <li class=md-nav__item> <a href=#pipeline-parallelism class=md-nav__link> <span class=md-ellipsis> Pipeline Parallelism </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#integration-with-trainer class=md-nav__link> <span class=md-ellipsis> Integration with Trainer </span> </a> </li> <li class=md-nav__item> <a href=#best-practices class=md-nav__link> <span class=md-ellipsis> Best Practices </span> </a> <nav class=md-nav aria-label="Best Practices"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-check-rank-for-io class=md-nav__link> <span class=md-ellipsis> 1. Check Rank for I/O </span> </a> </li> <li class=md-nav__item> <a href=#2-use-appropriate-contexts class=md-nav__link> <span class=md-ellipsis> 2. Use Appropriate Contexts </span> </a> </li> <li class=md-nav__item> <a href=#3-handle-device-placement class=md-nav__link> <span class=md-ellipsis> 3. Handle Device Placement </span> </a> </li> <li class=md-nav__item> <a href=#4-synchronize-when-needed class=md-nav__link> <span class=md-ellipsis> 4. Synchronize When Needed </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#troubleshooting class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> <nav class=md-nav aria-label=Troubleshooting> <ul class=md-nav__list> <li class=md-nav__item> <a href=#common-issues class=md-nav__link> <span class=md-ellipsis> Common Issues </span> </a> </li> <li class=md-nav__item> <a href=#debugging class=md-nav__link> <span class=md-ellipsis> Debugging </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#see-also class=md-nav__link> <span class=md-ellipsis> See Also </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../data/ class=md-nav__link> <span class=md-ellipsis> Data Utilities </span> </a> </li> <li class=md-nav__item> <a href=../common.md class=md-nav__link> <span class=md-ellipsis> Common Utilities </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../contributing.md class=md-nav__link> <span class=md-ellipsis> Community </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dream3d/dream-trainer/edit/main/dream-trainer/pages/docs/api/utilities/world.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dream3d/dream-trainer/raw/main/dream-trainer/pages/docs/api/utilities/world.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=world-management>World Management<a class=headerlink href=#world-management title="Permanent link">&para;</a></h1> <p>The world management utilities handle distributed training context, device meshes, and fault tolerance. The <code>DistributedWorld</code> class is the core abstraction for managing distributed training environments.</p> <h2 id=distributedworld>DistributedWorld<a class=headerlink href=#distributedworld title="Permanent link">&para;</a></h2> <p>The main class for managing distributed training context:</p> <div class="doc doc-object doc-class"> <h2 id=dream_trainer.trainer.world.DistributedWorld class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">DistributedWorld</span> <a href=#dream_trainer.trainer.world.DistributedWorld class=headerlink title="Permanent link">&para;</a></h2> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>DistributedWorld</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents first"> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-93> 93</a></span>
<span class=normal><a href=#__codelineno-0-94> 94</a></span>
<span class=normal><a href=#__codelineno-0-95> 95</a></span>
<span class=normal><a href=#__codelineno-0-96> 96</a></span>
<span class=normal><a href=#__codelineno-0-97> 97</a></span>
<span class=normal><a href=#__codelineno-0-98> 98</a></span>
<span class=normal><a href=#__codelineno-0-99> 99</a></span>
<span class=normal><a href=#__codelineno-0-100>100</a></span>
<span class=normal><a href=#__codelineno-0-101>101</a></span>
<span class=normal><a href=#__codelineno-0-102>102</a></span>
<span class=normal><a href=#__codelineno-0-103>103</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-93><a id=__codelineno-0-93 name=__codelineno-0-93></a><span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>):</span>
</span><span id=__span-0-94><a id=__codelineno-0-94 name=__codelineno-0-94></a>    <span class=n>config</span><span class=o>.</span><span class=n>validate</span><span class=p>()</span>
</span><span id=__span-0-95><a id=__codelineno-0-95 name=__codelineno-0-95></a>    <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span><span id=__span-0-96><a id=__codelineno-0-96 name=__codelineno-0-96></a>
</span><span id=__span-0-97><a id=__codelineno-0-97 name=__codelineno-0-97></a>    <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-98><a id=__codelineno-0-98 name=__codelineno-0-98></a>    <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;WORLD_SIZE&quot;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>device_count</span><span class=p>()))</span>
</span><span id=__span-0-99><a id=__codelineno-0-99 name=__codelineno-0-99></a>
</span><span id=__span-0-100><a id=__codelineno-0-100 name=__codelineno-0-100></a>    <span class=c1># NOTE: `device_module.set_device` has to be set before creating TorchFT manager.</span>
</span><span id=__span-0-101><a id=__codelineno-0-101 name=__codelineno-0-101></a>    <span class=bp>self</span><span class=o>.</span><span class=n>device_type</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_module</span> <span class=o>=</span> <span class=n>get_device_info</span><span class=p>()</span>
</span><span id=__span-0-102><a id=__codelineno-0-102 name=__codelineno-0-102></a>    <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>device_type</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;LOCAL_RANK&#39;</span><span class=p>])</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-103><a id=__codelineno-0-103 name=__codelineno-0-103></a>    <span class=bp>self</span><span class=o>.</span><span class=n>device_module</span><span class=o>.</span><span class=n>set_device</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <h3 id=dream_trainer.trainer.world.DistributedWorld-attributes>Attributes<a href=#dream_trainer.trainer.world.DistributedWorld-attributes class=headerlink title="Permanent link">&para;</a></h3> <div class="doc doc-object doc-attribute"> <h4 id=dream_trainer.trainer.world.DistributedWorld.is_global_zero class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code> <span class="doc doc-object-name doc-attribute-name">is_global_zero</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-property"><code>property</code></small> </span> <a href=#dream_trainer.trainer.world.DistributedWorld.is_global_zero class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>is_global_zero</span>
</span></code></pre></div> <div class="doc doc-contents "> </div> </div> <h3 id=dream_trainer.trainer.world.DistributedWorld-functions>Functions<a href=#dream_trainer.trainer.world.DistributedWorld-functions class=headerlink title="Permanent link">&para;</a></h3> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.get_mesh class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">get_mesh</span> <a href=#dream_trainer.trainer.world.DistributedWorld.get_mesh class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>get_mesh</span><span class=p>(</span><span class=n>mesh_dim_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>DeviceMesh</span> <span class=o>|</span> <span class=kc>None</span>
</span></code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-145>145</a></span>
<span class=normal><a href=#__codelineno-0-146>146</a></span>
<span class=normal><a href=#__codelineno-0-147>147</a></span>
<span class=normal><a href=#__codelineno-0-148>148</a></span>
<span class=normal><a href=#__codelineno-0-149>149</a></span>
<span class=normal><a href=#__codelineno-0-150>150</a></span>
<span class=normal><a href=#__codelineno-0-151>151</a></span>
<span class=normal><a href=#__codelineno-0-152>152</a></span>
<span class=normal><a href=#__codelineno-0-153>153</a></span>
<span class=normal><a href=#__codelineno-0-154>154</a></span>
<span class=normal><a href=#__codelineno-0-155>155</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-145><a id=__codelineno-0-145 name=__codelineno-0-145></a><span class=k>def</span><span class=w> </span><span class=nf>get_mesh</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>mesh_dim_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>DeviceMesh</span> <span class=o>|</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-146><a id=__codelineno-0-146 name=__codelineno-0-146></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-147><a id=__codelineno-0-147 name=__codelineno-0-147></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&quot;World mesh not yet initialized. Call `launch` first.&quot;</span><span class=p>)</span>
</span><span id=__span-0-148><a id=__codelineno-0-148 name=__codelineno-0-148></a>
</span><span id=__span-0-149><a id=__codelineno-0-149 name=__codelineno-0-149></a>    <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span><span class=o>.</span><span class=n>mesh_dim_names</span><span class=p>:</span>
</span><span id=__span-0-150><a id=__codelineno-0-150 name=__codelineno-0-150></a>        <span class=k>return</span> <span class=kc>None</span>
</span><span id=__span-0-151><a id=__codelineno-0-151 name=__codelineno-0-151></a>
</span><span id=__span-0-152><a id=__codelineno-0-152 name=__codelineno-0-152></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-0-153><a id=__codelineno-0-153 name=__codelineno-0-153></a>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span><span class=p>[</span><span class=n>mesh_dim_name</span><span class=p>]</span>
</span><span id=__span-0-154><a id=__codelineno-0-154 name=__codelineno-0-154></a>    <span class=k>except</span> <span class=ne>KeyError</span><span class=p>:</span>
</span><span id=__span-0-155><a id=__codelineno-0-155 name=__codelineno-0-155></a>        <span class=k>return</span> <span class=kc>None</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.all_reduce class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">all_reduce</span> <a href=#dream_trainer.trainer.world.DistributedWorld.all_reduce class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>all_reduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>op</span><span class=p>:</span> <span class=n>RedOpType</span> <span class=o>|</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>ReduceOp</span><span class=o>.</span><span class=n>SUM</span><span class=p>,</span> <span class=n>group</span><span class=p>:</span> <span class=n>ProcessGroup</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>async_op</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>All-reduce the given tensor across all processes in the specified process group.</p> <p>This method performs an all-reduce operation on the input tensor, summing its values across all processes.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-446>446</a></span>
<span class=normal><a href=#__codelineno-0-447>447</a></span>
<span class=normal><a href=#__codelineno-0-448>448</a></span>
<span class=normal><a href=#__codelineno-0-449>449</a></span>
<span class=normal><a href=#__codelineno-0-450>450</a></span>
<span class=normal><a href=#__codelineno-0-451>451</a></span>
<span class=normal><a href=#__codelineno-0-452>452</a></span>
<span class=normal><a href=#__codelineno-0-453>453</a></span>
<span class=normal><a href=#__codelineno-0-454>454</a></span>
<span class=normal><a href=#__codelineno-0-455>455</a></span>
<span class=normal><a href=#__codelineno-0-456>456</a></span>
<span class=normal><a href=#__codelineno-0-457>457</a></span>
<span class=normal><a href=#__codelineno-0-458>458</a></span>
<span class=normal><a href=#__codelineno-0-459>459</a></span>
<span class=normal><a href=#__codelineno-0-460>460</a></span>
<span class=normal><a href=#__codelineno-0-461>461</a></span>
<span class=normal><a href=#__codelineno-0-462>462</a></span>
<span class=normal><a href=#__codelineno-0-463>463</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-446><a id=__codelineno-0-446 name=__codelineno-0-446></a><span class=k>def</span><span class=w> </span><span class=nf>all_reduce</span><span class=p>(</span>
</span><span id=__span-0-447><a id=__codelineno-0-447 name=__codelineno-0-447></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-448><a id=__codelineno-0-448 name=__codelineno-0-448></a>    <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-449><a id=__codelineno-0-449 name=__codelineno-0-449></a>    <span class=n>op</span><span class=p>:</span> <span class=n>ReduceOp</span><span class=o>.</span><span class=n>RedOpType</span> <span class=o>|</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>ReduceOp</span><span class=o>.</span><span class=n>SUM</span><span class=p>,</span>
</span><span id=__span-0-450><a id=__codelineno-0-450 name=__codelineno-0-450></a>    <span class=n>group</span><span class=p>:</span> <span class=n>ProcessGroup</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-451><a id=__codelineno-0-451 name=__codelineno-0-451></a>    <span class=n>async_op</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-452><a id=__codelineno-0-452 name=__codelineno-0-452></a><span class=p>):</span>
</span><span id=__span-0-453><a id=__codelineno-0-453 name=__codelineno-0-453></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-454><a id=__codelineno-0-454 name=__codelineno-0-454></a><span class=sd>    All-reduce the given tensor across all processes in the specified process group.</span>
</span><span id=__span-0-455><a id=__codelineno-0-455 name=__codelineno-0-455></a>
</span><span id=__span-0-456><a id=__codelineno-0-456 name=__codelineno-0-456></a><span class=sd>    This method performs an all-reduce operation on the input tensor, summing its values across all processes.</span>
</span><span id=__span-0-457><a id=__codelineno-0-457 name=__codelineno-0-457></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-458><a id=__codelineno-0-458 name=__codelineno-0-458></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-459><a id=__codelineno-0-459 name=__codelineno-0-459></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&quot;World mesh not yet initialized. Call `launch` first.&quot;</span><span class=p>)</span>
</span><span id=__span-0-460><a id=__codelineno-0-460 name=__codelineno-0-460></a>
</span><span id=__span-0-461><a id=__codelineno-0-461 name=__codelineno-0-461></a>    <span class=n>dist</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span>
</span><span id=__span-0-462><a id=__codelineno-0-462 name=__codelineno-0-462></a>        <span class=n>tensor</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=n>cast</span><span class=p>(</span><span class=n>dist</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>RedOpType</span><span class=p>,</span> <span class=n>op</span><span class=p>),</span> <span class=n>group</span><span class=o>=</span><span class=n>group</span><span class=p>,</span> <span class=n>async_op</span><span class=o>=</span><span class=n>async_op</span>
</span><span id=__span-0-463><a id=__codelineno-0-463 name=__codelineno-0-463></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.barrier class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">barrier</span> <a href=#dream_trainer.trainer.world.DistributedWorld.barrier class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>barrier</span><span class=p>(</span><span class=n>process_group</span><span class=p>:</span> <span class=n>ProcessGroup</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Synchronizes all processes in the specified process group.</p> <p>This method blocks until all processes in the group reach this barrier. If no process group is specified, the world mesh group is used by default.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>process_group</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>The process group to synchronize. Defaults to None, in which case the world mesh group is used.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=torch.distributed.distributed_c10d.ProcessGroup>ProcessGroup</span> | None</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-430>430</a></span>
<span class=normal><a href=#__codelineno-0-431>431</a></span>
<span class=normal><a href=#__codelineno-0-432>432</a></span>
<span class=normal><a href=#__codelineno-0-433>433</a></span>
<span class=normal><a href=#__codelineno-0-434>434</a></span>
<span class=normal><a href=#__codelineno-0-435>435</a></span>
<span class=normal><a href=#__codelineno-0-436>436</a></span>
<span class=normal><a href=#__codelineno-0-437>437</a></span>
<span class=normal><a href=#__codelineno-0-438>438</a></span>
<span class=normal><a href=#__codelineno-0-439>439</a></span>
<span class=normal><a href=#__codelineno-0-440>440</a></span>
<span class=normal><a href=#__codelineno-0-441>441</a></span>
<span class=normal><a href=#__codelineno-0-442>442</a></span>
<span class=normal><a href=#__codelineno-0-443>443</a></span>
<span class=normal><a href=#__codelineno-0-444>444</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-430><a id=__codelineno-0-430 name=__codelineno-0-430></a><span class=k>def</span><span class=w> </span><span class=nf>barrier</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>process_group</span><span class=p>:</span> <span class=n>ProcessGroup</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
</span><span id=__span-0-431><a id=__codelineno-0-431 name=__codelineno-0-431></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-432><a id=__codelineno-0-432 name=__codelineno-0-432></a><span class=sd>    Synchronizes all processes in the specified process group.</span>
</span><span id=__span-0-433><a id=__codelineno-0-433 name=__codelineno-0-433></a>
</span><span id=__span-0-434><a id=__codelineno-0-434 name=__codelineno-0-434></a><span class=sd>    This method blocks until all processes in the group reach this barrier.</span>
</span><span id=__span-0-435><a id=__codelineno-0-435 name=__codelineno-0-435></a><span class=sd>    If no process group is specified, the world mesh group is used by default.</span>
</span><span id=__span-0-436><a id=__codelineno-0-436 name=__codelineno-0-436></a>
</span><span id=__span-0-437><a id=__codelineno-0-437 name=__codelineno-0-437></a><span class=sd>    Args:</span>
</span><span id=__span-0-438><a id=__codelineno-0-438 name=__codelineno-0-438></a><span class=sd>        process_group (ProcessGroup | None, optional): The process group to synchronize.</span>
</span><span id=__span-0-439><a id=__codelineno-0-439 name=__codelineno-0-439></a><span class=sd>            Defaults to None, in which case the world mesh group is used.</span>
</span><span id=__span-0-440><a id=__codelineno-0-440 name=__codelineno-0-440></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-441><a id=__codelineno-0-441 name=__codelineno-0-441></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_mesh</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-442><a id=__codelineno-0-442 name=__codelineno-0-442></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&quot;World mesh not yet initialized. Call `launch` first.&quot;</span><span class=p>)</span>
</span><span id=__span-0-443><a id=__codelineno-0-443 name=__codelineno-0-443></a>
</span><span id=__span-0-444><a id=__codelineno-0-444 name=__codelineno-0-444></a>    <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>(</span><span class=n>group</span><span class=o>=</span><span class=n>process_group</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.get_total_norm class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">get_total_norm</span> <a href=#dream_trainer.trainer.world.DistributedWorld.get_total_norm class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>get_total_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>norm_type</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>2.0</span><span class=p>,</span> <span class=n>error_if_nonfinite</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span> <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>async_op</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Get the total gradient norm of an iterable of parameters.</p> <p>Gradient norm clipping requires computing the gradient norm over the entire model. <code>torch.nn.utils.clip_grad_norm_</code> only computes gradient norm along DP/FSDP/TP dimensions. We need to manually reduce the gradient norm across PP stages.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>parameters</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>an iterable of Tensors or a single Tensor that will have gradients normalized</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=torch.Tensor>Tensor</span> | <span title=typing.Iterable>Iterable</span>[<span title=torch.Tensor>Tensor</span>]</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>max_norm</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>max norm of the gradients</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=float>float</span></code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>norm_type</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>type of the used p-norm. Can be <code>'inf'</code> for infinity norm.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=float>float</span></code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code>2.0</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>error_if_nonfinite</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>if True, an error is thrown if the total norm of the gradients from :attr:<code>parameters</code> is <code>nan</code>, <code>inf</code>, or <code>-inf</code>. Default: False (will switch to True in the future)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=bool>bool</span></code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code>False</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>foreach</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>use the faster foreach-based implementation. If <code>None</code>, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types. Default: <code>None</code></p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=bool>bool</span></code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-returns-annotation> <code><span title=torch.Tensor>Tensor</span></code> </span> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>Total norm of the parameter gradients (viewed as a single vector).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-499>499</a></span>
<span class=normal><a href=#__codelineno-0-500>500</a></span>
<span class=normal><a href=#__codelineno-0-501>501</a></span>
<span class=normal><a href=#__codelineno-0-502>502</a></span>
<span class=normal><a href=#__codelineno-0-503>503</a></span>
<span class=normal><a href=#__codelineno-0-504>504</a></span>
<span class=normal><a href=#__codelineno-0-505>505</a></span>
<span class=normal><a href=#__codelineno-0-506>506</a></span>
<span class=normal><a href=#__codelineno-0-507>507</a></span>
<span class=normal><a href=#__codelineno-0-508>508</a></span>
<span class=normal><a href=#__codelineno-0-509>509</a></span>
<span class=normal><a href=#__codelineno-0-510>510</a></span>
<span class=normal><a href=#__codelineno-0-511>511</a></span>
<span class=normal><a href=#__codelineno-0-512>512</a></span>
<span class=normal><a href=#__codelineno-0-513>513</a></span>
<span class=normal><a href=#__codelineno-0-514>514</a></span>
<span class=normal><a href=#__codelineno-0-515>515</a></span>
<span class=normal><a href=#__codelineno-0-516>516</a></span>
<span class=normal><a href=#__codelineno-0-517>517</a></span>
<span class=normal><a href=#__codelineno-0-518>518</a></span>
<span class=normal><a href=#__codelineno-0-519>519</a></span>
<span class=normal><a href=#__codelineno-0-520>520</a></span>
<span class=normal><a href=#__codelineno-0-521>521</a></span>
<span class=normal><a href=#__codelineno-0-522>522</a></span>
<span class=normal><a href=#__codelineno-0-523>523</a></span>
<span class=normal><a href=#__codelineno-0-524>524</a></span>
<span class=normal><a href=#__codelineno-0-525>525</a></span>
<span class=normal><a href=#__codelineno-0-526>526</a></span>
<span class=normal><a href=#__codelineno-0-527>527</a></span>
<span class=normal><a href=#__codelineno-0-528>528</a></span>
<span class=normal><a href=#__codelineno-0-529>529</a></span>
<span class=normal><a href=#__codelineno-0-530>530</a></span>
<span class=normal><a href=#__codelineno-0-531>531</a></span>
<span class=normal><a href=#__codelineno-0-532>532</a></span>
<span class=normal><a href=#__codelineno-0-533>533</a></span>
<span class=normal><a href=#__codelineno-0-534>534</a></span>
<span class=normal><a href=#__codelineno-0-535>535</a></span>
<span class=normal><a href=#__codelineno-0-536>536</a></span>
<span class=normal><a href=#__codelineno-0-537>537</a></span>
<span class=normal><a href=#__codelineno-0-538>538</a></span>
<span class=normal><a href=#__codelineno-0-539>539</a></span>
<span class=normal><a href=#__codelineno-0-540>540</a></span>
<span class=normal><a href=#__codelineno-0-541>541</a></span>
<span class=normal><a href=#__codelineno-0-542>542</a></span>
<span class=normal><a href=#__codelineno-0-543>543</a></span>
<span class=normal><a href=#__codelineno-0-544>544</a></span>
<span class=normal><a href=#__codelineno-0-545>545</a></span>
<span class=normal><a href=#__codelineno-0-546>546</a></span>
<span class=normal><a href=#__codelineno-0-547>547</a></span>
<span class=normal><a href=#__codelineno-0-548>548</a></span>
<span class=normal><a href=#__codelineno-0-549>549</a></span>
<span class=normal><a href=#__codelineno-0-550>550</a></span>
<span class=normal><a href=#__codelineno-0-551>551</a></span>
<span class=normal><a href=#__codelineno-0-552>552</a></span>
<span class=normal><a href=#__codelineno-0-553>553</a></span>
<span class=normal><a href=#__codelineno-0-554>554</a></span>
<span class=normal><a href=#__codelineno-0-555>555</a></span>
<span class=normal><a href=#__codelineno-0-556>556</a></span>
<span class=normal><a href=#__codelineno-0-557>557</a></span>
<span class=normal><a href=#__codelineno-0-558>558</a></span>
<span class=normal><a href=#__codelineno-0-559>559</a></span>
<span class=normal><a href=#__codelineno-0-560>560</a></span>
<span class=normal><a href=#__codelineno-0-561>561</a></span>
<span class=normal><a href=#__codelineno-0-562>562</a></span>
<span class=normal><a href=#__codelineno-0-563>563</a></span>
<span class=normal><a href=#__codelineno-0-564>564</a></span>
<span class=normal><a href=#__codelineno-0-565>565</a></span>
<span class=normal><a href=#__codelineno-0-566>566</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-499><a id=__codelineno-0-499 name=__codelineno-0-499></a><span class=nd>@torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>()</span>
</span><span id=__span-0-500><a id=__codelineno-0-500 name=__codelineno-0-500></a><span class=k>def</span><span class=w> </span><span class=nf>get_total_norm</span><span class=p>(</span>
</span><span id=__span-0-501><a id=__codelineno-0-501 name=__codelineno-0-501></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-502><a id=__codelineno-0-502 name=__codelineno-0-502></a>    <span class=n>parameters</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>|</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span>
</span><span id=__span-0-503><a id=__codelineno-0-503 name=__codelineno-0-503></a>    <span class=n>norm_type</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>2.0</span><span class=p>,</span>
</span><span id=__span-0-504><a id=__codelineno-0-504 name=__codelineno-0-504></a>    <span class=n>error_if_nonfinite</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-505><a id=__codelineno-0-505 name=__codelineno-0-505></a>    <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-506><a id=__codelineno-0-506 name=__codelineno-0-506></a>    <span class=n>async_op</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-507><a id=__codelineno-0-507 name=__codelineno-0-507></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-508><a id=__codelineno-0-508 name=__codelineno-0-508></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-509><a id=__codelineno-0-509 name=__codelineno-0-509></a><span class=sd>    Get the total gradient norm of an iterable of parameters.</span>
</span><span id=__span-0-510><a id=__codelineno-0-510 name=__codelineno-0-510></a>
</span><span id=__span-0-511><a id=__codelineno-0-511 name=__codelineno-0-511></a><span class=sd>    Gradient norm clipping requires computing the gradient norm over the entire model.</span>
</span><span id=__span-0-512><a id=__codelineno-0-512 name=__codelineno-0-512></a><span class=sd>    `torch.nn.utils.clip_grad_norm_` only computes gradient norm along DP/FSDP/TP dimensions.</span>
</span><span id=__span-0-513><a id=__codelineno-0-513 name=__codelineno-0-513></a><span class=sd>    We need to manually reduce the gradient norm across PP stages.</span>
</span><span id=__span-0-514><a id=__codelineno-0-514 name=__codelineno-0-514></a>
</span><span id=__span-0-515><a id=__codelineno-0-515 name=__codelineno-0-515></a><span class=sd>    Args:</span>
</span><span id=__span-0-516><a id=__codelineno-0-516 name=__codelineno-0-516></a><span class=sd>        parameters: an iterable of Tensors or a single Tensor that will have gradients normalized</span>
</span><span id=__span-0-517><a id=__codelineno-0-517 name=__codelineno-0-517></a><span class=sd>        max_norm (float): max norm of the gradients</span>
</span><span id=__span-0-518><a id=__codelineno-0-518 name=__codelineno-0-518></a><span class=sd>        norm_type (float): type of the used p-norm. Can be ``&#39;inf&#39;`` for</span>
</span><span id=__span-0-519><a id=__codelineno-0-519 name=__codelineno-0-519></a><span class=sd>            infinity norm.</span>
</span><span id=__span-0-520><a id=__codelineno-0-520 name=__codelineno-0-520></a><span class=sd>        error_if_nonfinite (bool): if True, an error is thrown if the total</span>
</span><span id=__span-0-521><a id=__codelineno-0-521 name=__codelineno-0-521></a><span class=sd>            norm of the gradients from :attr:`parameters` is ``nan``,</span>
</span><span id=__span-0-522><a id=__codelineno-0-522 name=__codelineno-0-522></a><span class=sd>            ``inf``, or ``-inf``. Default: False (will switch to True in the future)</span>
</span><span id=__span-0-523><a id=__codelineno-0-523 name=__codelineno-0-523></a><span class=sd>        foreach (bool): use the faster foreach-based implementation.</span>
</span><span id=__span-0-524><a id=__codelineno-0-524 name=__codelineno-0-524></a><span class=sd>            If ``None``, use the foreach implementation for CUDA and CPU native tensors and silently</span>
</span><span id=__span-0-525><a id=__codelineno-0-525 name=__codelineno-0-525></a><span class=sd>            fall back to the slow implementation for other device types.</span>
</span><span id=__span-0-526><a id=__codelineno-0-526 name=__codelineno-0-526></a><span class=sd>            Default: ``None``</span>
</span><span id=__span-0-527><a id=__codelineno-0-527 name=__codelineno-0-527></a>
</span><span id=__span-0-528><a id=__codelineno-0-528 name=__codelineno-0-528></a><span class=sd>    Returns:</span>
</span><span id=__span-0-529><a id=__codelineno-0-529 name=__codelineno-0-529></a><span class=sd>        Total norm of the parameter gradients (viewed as a single vector).</span>
</span><span id=__span-0-530><a id=__codelineno-0-530 name=__codelineno-0-530></a>
</span><span id=__span-0-531><a id=__codelineno-0-531 name=__codelineno-0-531></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-532><a id=__codelineno-0-532 name=__codelineno-0-532></a>    <span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>]</span>
</span><span id=__span-0-533><a id=__codelineno-0-533 name=__codelineno-0-533></a>    <span class=n>total_norm</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>get_total_norm</span><span class=p>(</span>
</span><span id=__span-0-534><a id=__codelineno-0-534 name=__codelineno-0-534></a>        <span class=n>grads</span><span class=p>,</span> <span class=n>norm_type</span><span class=p>,</span> <span class=n>error_if_nonfinite</span><span class=p>,</span> <span class=n>foreach</span>
</span><span id=__span-0-535><a id=__codelineno-0-535 name=__codelineno-0-535></a>    <span class=p>)</span>
</span><span id=__span-0-536><a id=__codelineno-0-536 name=__codelineno-0-536></a>
</span><span id=__span-0-537><a id=__codelineno-0-537 name=__codelineno-0-537></a>    <span class=c1># If total_norm is a DTensor, the placements must be `torch.distributed._tensor.ops.math_ops._NormPartial`.</span>
</span><span id=__span-0-538><a id=__codelineno-0-538 name=__codelineno-0-538></a>    <span class=c1># We can simply reduce the DTensor to get the total norm in this tensor&#39;s process group</span>
</span><span id=__span-0-539><a id=__codelineno-0-539 name=__codelineno-0-539></a>    <span class=c1># and then convert it to a local tensor.</span>
</span><span id=__span-0-540><a id=__codelineno-0-540 name=__codelineno-0-540></a>    <span class=c1># NOTE: It has two purposes:</span>
</span><span id=__span-0-541><a id=__codelineno-0-541 name=__codelineno-0-541></a>    <span class=c1>#       1. to make sure the total norm is computed correctly when PP is used (see below)</span>
</span><span id=__span-0-542><a id=__codelineno-0-542 name=__codelineno-0-542></a>    <span class=c1>#       2. to return a reduced total_norm tensor whose .item() would return the correct value</span>
</span><span id=__span-0-543><a id=__codelineno-0-543 name=__codelineno-0-543></a>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>total_norm</span><span class=p>,</span> <span class=n>DTensor</span><span class=p>):</span>
</span><span id=__span-0-544><a id=__codelineno-0-544 name=__codelineno-0-544></a>        <span class=c1># Will reach here if any non-PP parallelism is used.</span>
</span><span id=__span-0-545><a id=__codelineno-0-545 name=__codelineno-0-545></a>        <span class=c1># If only using PP, total_norm will be a local tensor.</span>
</span><span id=__span-0-546><a id=__codelineno-0-546 name=__codelineno-0-546></a>        <span class=n>total_norm</span> <span class=o>=</span> <span class=n>total_norm</span><span class=o>.</span><span class=n>full_tensor</span><span class=p>()</span>
</span><span id=__span-0-547><a id=__codelineno-0-547 name=__codelineno-0-547></a>
</span><span id=__span-0-548><a id=__codelineno-0-548 name=__codelineno-0-548></a>    <span class=k>if</span> <span class=p>(</span><span class=n>pp_mesh</span> <span class=o>:=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s2>&quot;pp&quot;</span><span class=p>))</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-549><a id=__codelineno-0-549 name=__codelineno-0-549></a>        <span class=k>if</span> <span class=n>math</span><span class=o>.</span><span class=n>isinf</span><span class=p>(</span><span class=n>norm_type</span><span class=p>):</span>
</span><span id=__span-0-550><a id=__codelineno-0-550 name=__codelineno-0-550></a>            <span class=n>dist</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span>
</span><span id=__span-0-551><a id=__codelineno-0-551 name=__codelineno-0-551></a>                <span class=n>total_norm</span><span class=p>,</span>
</span><span id=__span-0-552><a id=__codelineno-0-552 name=__codelineno-0-552></a>                <span class=n>op</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>MAX</span><span class=p>,</span>
</span><span id=__span-0-553><a id=__codelineno-0-553 name=__codelineno-0-553></a>                <span class=n>group</span><span class=o>=</span><span class=n>pp_mesh</span><span class=o>.</span><span class=n>get_group</span><span class=p>(),</span>
</span><span id=__span-0-554><a id=__codelineno-0-554 name=__codelineno-0-554></a>                <span class=n>async_op</span><span class=o>=</span><span class=n>async_op</span><span class=p>,</span>
</span><span id=__span-0-555><a id=__codelineno-0-555 name=__codelineno-0-555></a>            <span class=p>)</span>
</span><span id=__span-0-556><a id=__codelineno-0-556 name=__codelineno-0-556></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-557><a id=__codelineno-0-557 name=__codelineno-0-557></a>            <span class=n>total_norm</span> <span class=o>**=</span> <span class=n>norm_type</span>
</span><span id=__span-0-558><a id=__codelineno-0-558 name=__codelineno-0-558></a>            <span class=n>dist</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span>
</span><span id=__span-0-559><a id=__codelineno-0-559 name=__codelineno-0-559></a>                <span class=n>total_norm</span><span class=p>,</span>
</span><span id=__span-0-560><a id=__codelineno-0-560 name=__codelineno-0-560></a>                <span class=n>op</span><span class=o>=</span><span class=n>dist</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>SUM</span><span class=p>,</span>
</span><span id=__span-0-561><a id=__codelineno-0-561 name=__codelineno-0-561></a>                <span class=n>group</span><span class=o>=</span><span class=n>pp_mesh</span><span class=o>.</span><span class=n>get_group</span><span class=p>(),</span>
</span><span id=__span-0-562><a id=__codelineno-0-562 name=__codelineno-0-562></a>                <span class=n>async_op</span><span class=o>=</span><span class=n>async_op</span><span class=p>,</span>
</span><span id=__span-0-563><a id=__codelineno-0-563 name=__codelineno-0-563></a>            <span class=p>)</span>
</span><span id=__span-0-564><a id=__codelineno-0-564 name=__codelineno-0-564></a>            <span class=n>total_norm</span> <span class=o>**=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=n>norm_type</span>
</span><span id=__span-0-565><a id=__codelineno-0-565 name=__codelineno-0-565></a>
</span><span id=__span-0-566><a id=__codelineno-0-566 name=__codelineno-0-566></a>    <span class=k>return</span> <span class=n>total_norm</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.train_context class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">train_context</span> <a href=#dream_trainer.trainer.world.DistributedWorld.train_context class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>train_context</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Returns a context manager for training that sets up optional distributed context features.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=this-context-manager-enables open> <summary><p>This context manager enables</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p></summary> <ul> <li>Compiled autograd if configured (<code>self.config.enable_compiled_autograd</code>)</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </details> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>contextlib._GeneratorContextManager: A context manager that sets up the training context.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-407>407</a></span>
<span class=normal><a href=#__codelineno-0-408>408</a></span>
<span class=normal><a href=#__codelineno-0-409>409</a></span>
<span class=normal><a href=#__codelineno-0-410>410</a></span>
<span class=normal><a href=#__codelineno-0-411>411</a></span>
<span class=normal><a href=#__codelineno-0-412>412</a></span>
<span class=normal><a href=#__codelineno-0-413>413</a></span>
<span class=normal><a href=#__codelineno-0-414>414</a></span>
<span class=normal><a href=#__codelineno-0-415>415</a></span>
<span class=normal><a href=#__codelineno-0-416>416</a></span>
<span class=normal><a href=#__codelineno-0-417>417</a></span>
<span class=normal><a href=#__codelineno-0-418>418</a></span>
<span class=normal><a href=#__codelineno-0-419>419</a></span>
<span class=normal><a href=#__codelineno-0-420>420</a></span>
<span class=normal><a href=#__codelineno-0-421>421</a></span>
<span class=normal><a href=#__codelineno-0-422>422</a></span>
<span class=normal><a href=#__codelineno-0-423>423</a></span>
<span class=normal><a href=#__codelineno-0-424>424</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-407><a id=__codelineno-0-407 name=__codelineno-0-407></a><span class=nd>@contextlib</span><span class=o>.</span><span class=n>contextmanager</span>
</span><span id=__span-0-408><a id=__codelineno-0-408 name=__codelineno-0-408></a><span class=k>def</span><span class=w> </span><span class=nf>train_context</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-409><a id=__codelineno-0-409 name=__codelineno-0-409></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-410><a id=__codelineno-0-410 name=__codelineno-0-410></a><span class=sd>    Returns a context manager for training that sets up optional distributed context features.</span>
</span><span id=__span-0-411><a id=__codelineno-0-411 name=__codelineno-0-411></a>
</span><span id=__span-0-412><a id=__codelineno-0-412 name=__codelineno-0-412></a><span class=sd>    This context manager enables:</span>
</span><span id=__span-0-413><a id=__codelineno-0-413 name=__codelineno-0-413></a><span class=sd>        - Compiled autograd if configured (`self.config.enable_compiled_autograd`)</span>
</span><span id=__span-0-414><a id=__codelineno-0-414 name=__codelineno-0-414></a>
</span><span id=__span-0-415><a id=__codelineno-0-415 name=__codelineno-0-415></a><span class=sd>    Returns:</span>
</span><span id=__span-0-416><a id=__codelineno-0-416 name=__codelineno-0-416></a><span class=sd>        contextlib._GeneratorContextManager: A context manager that sets up the training context.</span>
</span><span id=__span-0-417><a id=__codelineno-0-417 name=__codelineno-0-417></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-418><a id=__codelineno-0-418 name=__codelineno-0-418></a>
</span><span id=__span-0-419><a id=__codelineno-0-419 name=__codelineno-0-419></a>    <span class=n>contexts</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-420><a id=__codelineno-0-420 name=__codelineno-0-420></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>enable_compiled_autograd</span><span class=p>:</span>
</span><span id=__span-0-421><a id=__codelineno-0-421 name=__codelineno-0-421></a>        <span class=n>contexts</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>_dynamo</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>maybe_enable_compiled_autograd</span><span class=p>(</span><span class=kc>True</span><span class=p>))</span>
</span><span id=__span-0-422><a id=__codelineno-0-422 name=__codelineno-0-422></a>
</span><span id=__span-0-423><a id=__codelineno-0-423 name=__codelineno-0-423></a>    <span class=k>with</span> <span class=n>stacked_context</span><span class=p>(</span><span class=n>contexts</span><span class=p>):</span>
</span><span id=__span-0-424><a id=__codelineno-0-424 name=__codelineno-0-424></a>        <span class=k>yield</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.world.DistributedWorld.loss_parallel class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">loss_parallel</span> <a href=#dream_trainer.trainer.world.DistributedWorld.loss_parallel class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>loss_parallel</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Context manager for loss parallelism.</p> <p>This context manager enables loss parallelism by setting up the appropriate distributed environment, but does nothing if tensor parallelism (tp) is not enabled.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>YIELDS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> </td> <td class=doc-yields-details> <div class=doc-md-description> <p>contextlib._GeneratorContextManager: A context manager that sets up the loss parallelism.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/distributed_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-389>389</a></span>
<span class=normal><a href=#__codelineno-0-390>390</a></span>
<span class=normal><a href=#__codelineno-0-391>391</a></span>
<span class=normal><a href=#__codelineno-0-392>392</a></span>
<span class=normal><a href=#__codelineno-0-393>393</a></span>
<span class=normal><a href=#__codelineno-0-394>394</a></span>
<span class=normal><a href=#__codelineno-0-395>395</a></span>
<span class=normal><a href=#__codelineno-0-396>396</a></span>
<span class=normal><a href=#__codelineno-0-397>397</a></span>
<span class=normal><a href=#__codelineno-0-398>398</a></span>
<span class=normal><a href=#__codelineno-0-399>399</a></span>
<span class=normal><a href=#__codelineno-0-400>400</a></span>
<span class=normal><a href=#__codelineno-0-401>401</a></span>
<span class=normal><a href=#__codelineno-0-402>402</a></span>
<span class=normal><a href=#__codelineno-0-403>403</a></span>
<span class=normal><a href=#__codelineno-0-404>404</a></span>
<span class=normal><a href=#__codelineno-0-405>405</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-389><a id=__codelineno-0-389 name=__codelineno-0-389></a><span class=nd>@contextlib</span><span class=o>.</span><span class=n>contextmanager</span>
</span><span id=__span-0-390><a id=__codelineno-0-390 name=__codelineno-0-390></a><span class=k>def</span><span class=w> </span><span class=nf>loss_parallel</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-391><a id=__codelineno-0-391 name=__codelineno-0-391></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-392><a id=__codelineno-0-392 name=__codelineno-0-392></a><span class=sd>    Context manager for loss parallelism.</span>
</span><span id=__span-0-393><a id=__codelineno-0-393 name=__codelineno-0-393></a>
</span><span id=__span-0-394><a id=__codelineno-0-394 name=__codelineno-0-394></a><span class=sd>    This context manager enables loss parallelism by setting up the appropriate distributed environment,</span>
</span><span id=__span-0-395><a id=__codelineno-0-395 name=__codelineno-0-395></a><span class=sd>    but does nothing if tensor parallelism (tp) is not enabled.</span>
</span><span id=__span-0-396><a id=__codelineno-0-396 name=__codelineno-0-396></a>
</span><span id=__span-0-397><a id=__codelineno-0-397 name=__codelineno-0-397></a><span class=sd>    Yields:</span>
</span><span id=__span-0-398><a id=__codelineno-0-398 name=__codelineno-0-398></a><span class=sd>        contextlib._GeneratorContextManager: A context manager that sets up the loss parallelism.</span>
</span><span id=__span-0-399><a id=__codelineno-0-399 name=__codelineno-0-399></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-400><a id=__codelineno-0-400 name=__codelineno-0-400></a>    <span class=k>with</span> <span class=p>(</span>
</span><span id=__span-0-401><a id=__codelineno-0-401 name=__codelineno-0-401></a>        <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>tensor</span><span class=o>.</span><span class=n>parallel</span><span class=o>.</span><span class=n>loss_parallel</span><span class=p>()</span>
</span><span id=__span-0-402><a id=__codelineno-0-402 name=__codelineno-0-402></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_parallel_enabled</span>
</span><span id=__span-0-403><a id=__codelineno-0-403 name=__codelineno-0-403></a>        <span class=k>else</span> <span class=n>contextlib</span><span class=o>.</span><span class=n>nullcontext</span><span class=p>()</span>
</span><span id=__span-0-404><a id=__codelineno-0-404 name=__codelineno-0-404></a>    <span class=p>):</span>
</span><span id=__span-0-405><a id=__codelineno-0-405 name=__codelineno-0-405></a>        <span class=k>yield</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div><h3 id=properties>Properties<a class=headerlink href=#properties title="Permanent link">&para;</a></h3> <ul> <li><code>world_size</code> - Total number of processes</li> <li><code>rank</code> - Current process rank</li> <li><code>local_rank</code> - Rank within the node</li> <li><code>device</code> - Current device (cuda:X or cpu)</li> <li><code>dp_size</code> - Data parallel world size</li> <li><code>dp_rank</code> - Data parallel rank</li> <li><code>tp_enabled</code> - Whether tensor parallelism is enabled</li> <li><code>pp_enabled</code> - Whether pipeline parallelism is enabled</li> <li><code>cp_enabled</code> - Whether context parallelism is enabled</li> </ul> <h2 id=usage-examples>Usage Examples<a class=headerlink href=#usage-examples title="Permanent link">&para;</a></h2> <h3 id=basic-usage>Basic Usage<a class=headerlink href=#basic-usage title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.world</span><span class=w> </span><span class=kn>import</span> <span class=n>DistributedWorld</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.configs</span><span class=w> </span><span class=kn>import</span> <span class=n>DeviceParameters</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=c1># Create world for FSDP training</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=n>device_params</span> <span class=o>=</span> <span class=n>DeviceParameters</span><span class=o>.</span><span class=n>FSDP</span><span class=p>()</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=n>world</span> <span class=o>=</span> <span class=n>DistributedWorld</span><span class=p>(</span><span class=n>device_params</span><span class=p>)</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=c1># Setup distributed environment</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=n>world</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=c1># Access world properties</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;World size: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>world_size</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Current rank: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>rank</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Device: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=collective-operations>Collective Operations<a class=headerlink href=#collective-operations title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># All-reduce a tensor across all processes</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>world</span><span class=o>.</span><span class=n>rank</span><span class=p>],</span> <span class=n>device</span><span class=o>=</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>reduced</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=s2>&quot;sum&quot;</span><span class=p>)</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=c1># reduced will be sum of all ranks</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=c1># All-gather tensors from all processes</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=n>gathered</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>all_gather</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=c1># gathered will contain tensors from all ranks</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=c1># Synchronize all processes</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=n>world</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></code></pre></div> <h3 id=device-meshes>Device Meshes<a class=headerlink href=#device-meshes title="Permanent link">&para;</a></h3> <p>The world manages device meshes for different parallelism strategies:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1># Get specific mesh</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>tp_mesh</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s2>&quot;tp&quot;</span><span class=p>)</span>  <span class=c1># Tensor parallel mesh</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>dp_mesh</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s2>&quot;dp_shard&quot;</span><span class=p>)</span>  <span class=c1># Data parallel shard mesh</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># Check if parallelism is enabled</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>tp_enabled</span><span class=p>:</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Using tensor parallelism&quot;</span><span class=p>)</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>dp_shard_enabled</span><span class=p>:</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Using FSDP sharding&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=training-context>Training Context<a class=headerlink href=#training-context title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Use the training context for mixed precision</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=k>with</span> <span class=n>world</span><span class=o>.</span><span class=n>train_context</span><span class=p>():</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=c1># Training code here - automatically handles autocast</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=c1># Loss parallelism for TP</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=k>with</span> <span class=n>world</span><span class=o>.</span><span class=n>loss_parallel</span><span class=p>():</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    <span class=c1># Loss computation with proper TP handling</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=n>loss</span> <span class=o>=</span> <span class=n>parallel_cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></code></pre></div> <h2 id=faulttolerantworld>FaultTolerantWorld<a class=headerlink href=#faulttolerantworld title="Permanent link">&para;</a></h2> <p>Extended world for fault-tolerant training with torchft:</p> <div class="doc doc-object doc-class"> <h2 id=dream_trainer.trainer.world.FaultTolerantWorld class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">FaultTolerantWorld</span> <a href=#dream_trainer.trainer.world.FaultTolerantWorld class=headerlink title="Permanent link">&para;</a></h2> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>FaultTolerantWorld</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>,</span> <span class=n>ft_config</span><span class=p>:</span> <span class=n>FaultToleranceParameters</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents first"> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/world/fault_tolerant_world.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-31>31</a></span>
<span class=normal><a href=#__codelineno-0-32>32</a></span>
<span class=normal><a href=#__codelineno-0-33>33</a></span>
<span class=normal><a href=#__codelineno-0-34>34</a></span>
<span class=normal><a href=#__codelineno-0-35>35</a></span>
<span class=normal><a href=#__codelineno-0-36>36</a></span>
<span class=normal><a href=#__codelineno-0-37>37</a></span>
<span class=normal><a href=#__codelineno-0-38>38</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-31><a id=__codelineno-0-31 name=__codelineno-0-31></a><span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>,</span> <span class=n>ft_config</span><span class=p>:</span> <span class=n>FaultToleranceParameters</span><span class=p>):</span>
</span><span id=__span-0-32><a id=__codelineno-0-32 name=__codelineno-0-32></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-0-33><a id=__codelineno-0-33 name=__codelineno-0-33></a>
</span><span id=__span-0-34><a id=__codelineno-0-34 name=__codelineno-0-34></a>    <span class=bp>self</span><span class=o>.</span><span class=n>ft_config</span> <span class=o>=</span> <span class=n>ft_config</span>
</span><span id=__span-0-35><a id=__codelineno-0-35 name=__codelineno-0-35></a>    <span class=bp>self</span><span class=o>.</span><span class=n>group_rank</span> <span class=o>=</span> <span class=n>dist_util</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>get_dist_local_rank</span><span class=p>()</span>
</span><span id=__span-0-36><a id=__codelineno-0-36 name=__codelineno-0-36></a>    <span class=bp>self</span><span class=o>.</span><span class=n>group_size</span> <span class=o>=</span> <span class=n>dist_util</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>get_dist_local_world_size</span><span class=p>()</span>
</span><span id=__span-0-37><a id=__codelineno-0-37 name=__codelineno-0-37></a>
</span><span id=__span-0-38><a id=__codelineno-0-38 name=__codelineno-0-38></a>    <span class=bp>self</span><span class=o>.</span><span class=n>replica_id</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>ft_config</span><span class=o>.</span><span class=n>replica_prefix</span><span class=w> </span><span class=ow>or</span><span class=w> </span><span class=s1>&#39;ft&#39;</span><span class=si>}</span><span class=s2>_</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>group_rank</span><span class=si>}</span><span class=s2>&quot;</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> </div> </div> </div><h3 id=usage>Usage<a class=headerlink href=#usage title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer.world</span><span class=w> </span><span class=kn>import</span> <span class=n>FaultTolerantWorld</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.configs</span><span class=w> </span><span class=kn>import</span> <span class=n>FaultToleranceParameters</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>ft_params</span> <span class=o>=</span> <span class=n>FaultToleranceParameters</span><span class=p>(</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>    <span class=n>enable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>    <span class=n>min_replica_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>    <span class=n>max_consecutive_failures</span><span class=o>=</span><span class=mi>3</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a><span class=p>)</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a><span class=n>world</span> <span class=o>=</span> <span class=n>FaultTolerantWorld</span><span class=p>(</span><span class=n>device_params</span><span class=p>,</span> <span class=n>ft_params</span><span class=p>)</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a><span class=n>world</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a><span class=c1># Access fault tolerance manager</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a><span class=n>ft_manager</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>ft_manager</span>
</span></code></pre></div> <h2 id=fsdp-configuration>FSDP Configuration<a class=headerlink href=#fsdp-configuration title="Permanent link">&para;</a></h2> <p>The world provides FSDP configuration based on device parameters:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Get FSDP configuration</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=n>fsdp_config</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_fsdp_config</span><span class=p>()</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=c1># Configuration includes:</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=c1># - Sharding strategy</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=c1># - CPU offloading</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a><span class=c1># - Auto-wrap policy</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=c1># - Backward prefetch</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a><span class=c1># - Forward prefetch</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=c1># Get mixed precision policy</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=n>mp_policy</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_fsdp_mp_policy</span><span class=p>()</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a><span class=c1># Configures param_dtype and reduce_dtype</span>
</span></code></pre></div> <h2 id=gradient-operations>Gradient Operations<a class=headerlink href=#gradient-operations title="Permanent link">&para;</a></h2> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># Compute total gradient norm across distributed parameters</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=n>parameters</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=n>total_norm</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_total_norm</span><span class=p>(</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    <span class=n>parameters</span><span class=o>=</span><span class=n>parameters</span><span class=p>,</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    <span class=n>norm_type</span><span class=o>=</span><span class=mf>2.0</span><span class=p>,</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>    <span class=n>error_if_nonfinite</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=p>)</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Gradient norm: </span><span class=si>{</span><span class=n>total_norm</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=advanced-features>Advanced Features<a class=headerlink href=#advanced-features title="Permanent link">&para;</a></h2> <h3 id=custom-mesh-construction>Custom Mesh Construction<a class=headerlink href=#custom-mesh-construction title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># The world automatically constructs meshes based on config</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=c1># Mesh dimensions: [pp, cp, dp_replicate, dp_shard, tp]</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=c1># Example 4D mesh for HSDP + TP</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=c1># pp=1, cp=1, dp_replicate=2, dp_shard=4, tp=8</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a><span class=c1># Total world size = 1 * 1 * 2 * 4 * 8 = 64</span>
</span></code></pre></div> <h3 id=context-parallelism>Context Parallelism<a class=headerlink href=#context-parallelism title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>cp_enabled</span><span class=p>:</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>    <span class=n>cp_mesh</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s2>&quot;cp&quot;</span><span class=p>)</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>    <span class=c1># Use for sequence parallelism</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>    <span class=n>cp_rank</span> <span class=o>=</span> <span class=n>cp_mesh</span><span class=o>.</span><span class=n>get_local_rank</span><span class=p>()</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>    <span class=n>cp_size</span> <span class=o>=</span> <span class=n>cp_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></code></pre></div> <h3 id=pipeline-parallelism>Pipeline Parallelism<a class=headerlink href=#pipeline-parallelism title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>pp_enabled</span><span class=p>:</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>    <span class=n>pp_mesh</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s2>&quot;pp&quot;</span><span class=p>)</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>    <span class=n>pp_rank</span> <span class=o>=</span> <span class=n>pp_mesh</span><span class=o>.</span><span class=n>get_local_rank</span><span class=p>()</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>    <span class=c1># Check if this rank has first/last stage</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>    <span class=n>has_first_stage</span> <span class=o>=</span> <span class=p>(</span><span class=n>pp_rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>    <span class=n>has_last_stage</span> <span class=o>=</span> <span class=p>(</span><span class=n>pp_rank</span> <span class=o>==</span> <span class=n>pp_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></code></pre></div> <h2 id=integration-with-trainer>Integration with Trainer<a class=headerlink href=#integration-with-trainer title="Permanent link">&para;</a></h2> <p>The world is automatically created and managed by trainers:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>):</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>        <span class=c1># self.world is automatically created</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>    <span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>        <span class=c1># Use world for distributed operations</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>is_global_zero</span><span class=p>:</span>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a>            <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Running on main process&quot;</span><span class=p>)</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a>        <span class=c1># Collective operations</span>
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-10-13><a id=__codelineno-10-13 name=__codelineno-10-13 href=#__codelineno-10-13></a>        <span class=n>avg_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=s2>&quot;mean&quot;</span><span class=p>)</span>
</span><span id=__span-10-14><a id=__codelineno-10-14 name=__codelineno-10-14 href=#__codelineno-10-14></a>
</span><span id=__span-10-15><a id=__codelineno-10-15 name=__codelineno-10-15 href=#__codelineno-10-15></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;loss&quot;</span><span class=p>:</span> <span class=n>avg_loss</span><span class=p>}</span>
</span></code></pre></div> <h2 id=best-practices>Best Practices<a class=headerlink href=#best-practices title="Permanent link">&para;</a></h2> <h3 id=1-check-rank-for-io>1. Check Rank for I/O<a class=headerlink href=#1-check-rank-for-io title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=c1># Only perform I/O on rank 0</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>is_global_zero</span><span class=p>:</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>    <span class=n>save_checkpoint</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>    <span class=n>write_logs</span><span class=p>(</span><span class=n>metrics</span><span class=p>)</span>
</span></code></pre></div> <h3 id=2-use-appropriate-contexts>2. Use Appropriate Contexts<a class=headerlink href=#2-use-appropriate-contexts title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=c1># Training with autocast</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a><span class=k>with</span> <span class=n>world</span><span class=o>.</span><span class=n>train_context</span><span class=p>():</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>    <span class=n>loss</span> <span class=o>=</span> <span class=n>training_step</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a><span class=c1># Loss computation with TP</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a><span class=k>if</span> <span class=n>world</span><span class=o>.</span><span class=n>loss_parallel_enabled</span><span class=p>:</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>    <span class=k>with</span> <span class=n>world</span><span class=o>.</span><span class=n>loss_parallel</span><span class=p>():</span>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>compute_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></code></pre></div> <h3 id=3-handle-device-placement>3. Handle Device Placement<a class=headerlink href=#3-handle-device-placement title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># Always use world.device for tensors</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=c1># Move model to device</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></code></pre></div> <h3 id=4-synchronize-when-needed>4. Synchronize When Needed<a class=headerlink href=#4-synchronize-when-needed title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=c1># Ensure all processes are ready</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=n>world</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=c1># Synchronize metrics</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=n>metric</span> <span class=o>=</span> <span class=n>compute_metric</span><span class=p>()</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a><span class=n>synced_metric</span> <span class=o>=</span> <span class=n>world</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span><span class=n>metric</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=s2>&quot;mean&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=troubleshooting>Troubleshooting<a class=headerlink href=#troubleshooting title="Permanent link">&para;</a></h2> <h3 id=common-issues>Common Issues<a class=headerlink href=#common-issues title="Permanent link">&para;</a></h3> <ol> <li><strong>Timeout errors</strong>: Increase <code>comm.init_timeout_seconds</code></li> <li><strong>NCCL errors</strong>: Check network configuration</li> <li><strong>Device mismatch</strong>: Ensure all tensors use <code>world.device</code></li> <li><strong>Rank mismatch</strong>: Verify WORLD_SIZE and RANK env vars</li> </ol> <h3 id=debugging>Debugging<a class=headerlink href=#debugging title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=c1># Print debug info</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Rank </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>rank</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>world_size</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Device: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;DP size: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>dp_size</span><span class=si>}</span><span class=s2>, TP size: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>get_mesh</span><span class=p>(</span><span class=s1>&#39;tp&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a><span class=c1># Check mesh configuration</span>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Mesh dims: </span><span class=si>{</span><span class=n>world</span><span class=o>.</span><span class=n>world_mesh</span><span class=o>.</span><span class=n>mesh_dim_names</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=see-also>See Also<a class=headerlink href=#see-also title="Permanent link">&para;</a></h2> <ul> <li><a href=../../configuration/parameters/#deviceparameters>DeviceParameters</a> - Parallelism configuration</li> <li><a href=../../trainers/abstract/ >AbstractTrainer</a> - Trainer integration</li> <li><a href=../../../parallelism/ >Parallelism Guide</a> - Parallelism strategies </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../configuration/training/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Training Config"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Training Config </div> </div> </a> <a href=../data/ class="md-footer__link md-footer__link--next" aria-label="Next: Data Utilities"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Data Utilities </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dream3d/dream-trainer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://discord.gg/dream-trainer target=_blank rel=noopener title=discord.gg class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> <a href=https://twitter.com/dream3d_ai target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.prune", "navigation.indexes", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tooltips", "toc.follow", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.56ea9cef.min.js></script> <script src=../../../js/timeago.min.js></script> <script src=../../../js/timeago_mkdocs_material.js></script> <script src=../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>