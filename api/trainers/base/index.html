<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Composable distributed training framework built around PyTorch DTensor abstractions"><link href=https://dream3d.ai/trainer/api/trainers/base/ rel=canonical><link href=../abstract/ rel=prev><link href=../dream/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.15"><title>BaseTrainer - dream-trainer</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style><link rel=stylesheet href=../../../css/timeago.css><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#basetrainer class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=dream-trainer class="md-header__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> dream-trainer </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> BaseTrainer </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../installation/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../../configuration/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../../../tutorials/first-trainer.md class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../../../examples/vision.md class=md-tabs__link> Examples </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../../contributing.md class=md-tabs__link> Community </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=dream-trainer class="md-nav__button md-logo" aria-label=dream-trainer data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> dream-trainer </label> <div class=md-nav__source> <a href=https://github.com/dream3d/dream-trainer title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../installation/ class=md-nav__link> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../configuration/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../tutorials/first-trainer.md class=md-nav__link> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../examples/vision.md class=md-nav__link> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> API Reference </span> </a> <label class="md-nav__link " for=__nav_6 id=__nav_6_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=true> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2 checked> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex> <span class=md-ellipsis> Trainers </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=true> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> Trainers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../abstract/ class=md-nav__link> <span class=md-ellipsis> AbstractTrainer </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> BaseTrainer </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> BaseTrainer </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#class-reference class=md-nav__link> <span class=md-ellipsis> Class Reference </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;BaseTrainer </span> </a> <nav class=md-nav aria-label= BaseTrainer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer-attributes class=md-nav__link> <span class=md-ellipsis> Attributes </span> </a> <nav class=md-nav aria-label=Attributes> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.is_accumulating_gradients class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;is_accumulating_gradients </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer-functions class=md-nav__link> <span class=md-ellipsis> Functions </span> </a> <nav class=md-nav aria-label=Functions> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.state_dict class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;state_dict </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.load_state_dict class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;load_state_dict </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.fit class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.training_step class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;training_step </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.validation_step class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;validation_step </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.eval class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;eval </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.train class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.step class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;step </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.loss_parallel class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;loss_parallel </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.backward class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;backward </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.no_gradient_sync class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;no_gradient_sync </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.total_gradient_norm class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;total_gradient_norm </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.clip_gradient_norm class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;clip_gradient_norm </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.train_context class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;train_context </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.perform_training_epoch class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;perform_training_epoch </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.perform_validation_epoch class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;perform_validation_epoch </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainer.perform_sanity_validation_steps class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;perform_sanity_validation_steps </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configuration class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=#dream_trainer.trainer.BaseTrainerConfig class=md-nav__link> <span class=md-ellipsis> <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;BaseTrainerConfig </span> </a> </li> <li class=md-nav__item> <a href=#usage-example class=md-nav__link> <span class=md-ellipsis> Usage Example </span> </a> </li> <li class=md-nav__item> <a href=#key-features class=md-nav__link> <span class=md-ellipsis> Key Features </span> </a> <nav class=md-nav aria-label="Key Features"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gradient-accumulation class=md-nav__link> <span class=md-ellipsis> Gradient Accumulation </span> </a> </li> <li class=md-nav__item> <a href=#mixed-precision-training class=md-nav__link> <span class=md-ellipsis> Mixed Precision Training </span> </a> </li> <li class=md-nav__item> <a href=#validation class=md-nav__link> <span class=md-ellipsis> Validation </span> </a> </li> <li class=md-nav__item> <a href=#callbacks class=md-nav__link> <span class=md-ellipsis> Callbacks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-usage class=md-nav__link> <span class=md-ellipsis> Advanced Usage </span> </a> <nav class=md-nav aria-label="Advanced Usage"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#custom-gradient-clipping class=md-nav__link> <span class=md-ellipsis> Custom Gradient Clipping </span> </a> </li> <li class=md-nav__item> <a href=#distributed-training class=md-nav__link> <span class=md-ellipsis> Distributed Training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#see-also class=md-nav__link> <span class=md-ellipsis> See Also </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../dream/ class=md-nav__link> <span class=md-ellipsis> DreamTrainer </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_3> <label class=md-nav__link for=__nav_6_3 id=__nav_6_3_label tabindex> <span class=md-ellipsis> Mixins </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_3_label aria-expanded=false> <label class=md-nav__title for=__nav_6_3> <span class="md-nav__icon md-icon"></span> Mixins </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../mixins/setup/ class=md-nav__link> <span class=md-ellipsis> Setup Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/eval_metric/ class=md-nav__link> <span class=md-ellipsis> Evaluation Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/loggers/ class=md-nav__link> <span class=md-ellipsis> Logger Mixins </span> </a> </li> <li class=md-nav__item> <a href=../../mixins/quantize/ class=md-nav__link> <span class=md-ellipsis> Quantization Mixins </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_4> <label class=md-nav__link for=__nav_6_4 id=__nav_6_4_label tabindex> <span class=md-ellipsis> Callbacks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_4_label aria-expanded=false> <label class=md-nav__title for=__nav_6_4> <span class="md-nav__icon md-icon"></span> Callbacks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../callbacks/base/ class=md-nav__link> <span class=md-ellipsis> Callback Base </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/checkpoint/ class=md-nav__link> <span class=md-ellipsis> Checkpoint Callbacks </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/monitoring/ class=md-nav__link> <span class=md-ellipsis> Monitoring Callbacks </span> </a> </li> <li class=md-nav__item> <a href=../../callbacks/performance/ class=md-nav__link> <span class=md-ellipsis> Performance Callbacks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_5> <label class=md-nav__link for=__nav_6_5 id=__nav_6_5_label tabindex> <span class=md-ellipsis> Configuration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_5_label aria-expanded=false> <label class=md-nav__title for=__nav_6_5> <span class="md-nav__icon md-icon"></span> Configuration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../configuration/parameters/ class=md-nav__link> <span class=md-ellipsis> Parameter Classes </span> </a> </li> <li class=md-nav__item> <a href=../../configuration/device/ class=md-nav__link> <span class=md-ellipsis> Device Config </span> </a> </li> <li class=md-nav__item> <a href=../../configuration/training/ class=md-nav__link> <span class=md-ellipsis> Training Config </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6_6> <label class=md-nav__link for=__nav_6_6 id=__nav_6_6_label tabindex> <span class=md-ellipsis> Utilities </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6_6> <span class="md-nav__icon md-icon"></span> Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../utilities/world/ class=md-nav__link> <span class=md-ellipsis> World Management </span> </a> </li> <li class=md-nav__item> <a href=../../utilities/data/ class=md-nav__link> <span class=md-ellipsis> Data Utilities </span> </a> </li> <li class=md-nav__item> <a href=../../utilities/common.md class=md-nav__link> <span class=md-ellipsis> Common Utilities </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../contributing.md class=md-nav__link> <span class=md-ellipsis> Community </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dream3d/dream-trainer/edit/main/dream-trainer/pages/docs/api/trainers/base.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dream3d/dream-trainer/raw/main/dream-trainer/pages/docs/api/trainers/base.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=basetrainer>BaseTrainer<a class=headerlink href=#basetrainer title="Permanent link">&para;</a></h1> <p>The <code>BaseTrainer</code> class provides a complete implementation of the training loop with support for gradient accumulation, validation, callbacks, and distributed training.</p> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">&para;</a></h2> <p><code>BaseTrainer</code> extends <code>AbstractTrainer</code> with: - Complete training and validation loops - Gradient accumulation and clipping - Callback integration - Mixed precision training support - Distributed training synchronization - Learning rate scheduling</p> <h2 id=class-reference>Class Reference<a class=headerlink href=#class-reference title="Permanent link">&para;</a></h2> <div class="doc doc-object doc-class"> <h2 id=dream_trainer.trainer.BaseTrainer class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">BaseTrainer</span> <a href=#dream_trainer.trainer.BaseTrainer class=headerlink title="Permanent link">&para;</a></h2> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>BaseTrainer</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>BaseTrainerConfig</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title='<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">AbstractTrainer</span> (<code>dream_trainer.trainer.abstract.AbstractTrainer</code>)' href=../abstract/#dream_trainer.trainer.AbstractTrainer>AbstractTrainer</a></code></p> <p>An implementation of a basic training loop, taking into account gradient accumulation, validation, callbacks, and contains bindings for backwards calls and optimizer steps.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <p>Initialize the BaseTrainer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>config</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Configuration object containing training parameters and callbacks.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><a class="autorefs autorefs-internal" title='<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">BaseTrainerConfig</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span> (<code>dream_trainer.trainer.base.BaseTrainerConfig</code>)' href=#dream_trainer.trainer.BaseTrainerConfig>BaseTrainerConfig</a></code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>*args</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Additional positional arguments passed to parent class.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-default> <b>DEFAULT:</b> <code>()</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>**kwargs</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Additional keyword arguments passed to parent class.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-default> <b>DEFAULT:</b> <code>{}</code> </span> </p> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-69>69</a></span>
<span class=normal><a href=#__codelineno-0-70>70</a></span>
<span class=normal><a href=#__codelineno-0-71>71</a></span>
<span class=normal><a href=#__codelineno-0-72>72</a></span>
<span class=normal><a href=#__codelineno-0-73>73</a></span>
<span class=normal><a href=#__codelineno-0-74>74</a></span>
<span class=normal><a href=#__codelineno-0-75>75</a></span>
<span class=normal><a href=#__codelineno-0-76>76</a></span>
<span class=normal><a href=#__codelineno-0-77>77</a></span>
<span class=normal><a href=#__codelineno-0-78>78</a></span>
<span class=normal><a href=#__codelineno-0-79>79</a></span>
<span class=normal><a href=#__codelineno-0-80>80</a></span>
<span class=normal><a href=#__codelineno-0-81>81</a></span>
<span class=normal><a href=#__codelineno-0-82>82</a></span>
<span class=normal><a href=#__codelineno-0-83>83</a></span>
<span class=normal><a href=#__codelineno-0-84>84</a></span>
<span class=normal><a href=#__codelineno-0-85>85</a></span>
<span class=normal><a href=#__codelineno-0-86>86</a></span>
<span class=normal><a href=#__codelineno-0-87>87</a></span>
<span class=normal><a href=#__codelineno-0-88>88</a></span>
<span class=normal><a href=#__codelineno-0-89>89</a></span>
<span class=normal><a href=#__codelineno-0-90>90</a></span>
<span class=normal><a href=#__codelineno-0-91>91</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-69><a id=__codelineno-0-69 name=__codelineno-0-69></a><span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BaseTrainerConfig</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-70><a id=__codelineno-0-70 name=__codelineno-0-70></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-71><a id=__codelineno-0-71 name=__codelineno-0-71></a><span class=sd>    Initialize the BaseTrainer.</span>
</span><span id=__span-0-72><a id=__codelineno-0-72 name=__codelineno-0-72></a>
</span><span id=__span-0-73><a id=__codelineno-0-73 name=__codelineno-0-73></a><span class=sd>    Args:</span>
</span><span id=__span-0-74><a id=__codelineno-0-74 name=__codelineno-0-74></a><span class=sd>        config: Configuration object containing training parameters and callbacks.</span>
</span><span id=__span-0-75><a id=__codelineno-0-75 name=__codelineno-0-75></a><span class=sd>        *args: Additional positional arguments passed to parent class.</span>
</span><span id=__span-0-76><a id=__codelineno-0-76 name=__codelineno-0-76></a><span class=sd>        **kwargs: Additional keyword arguments passed to parent class.</span>
</span><span id=__span-0-77><a id=__codelineno-0-77 name=__codelineno-0-77></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-78><a id=__codelineno-0-78 name=__codelineno-0-78></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span><span id=__span-0-79><a id=__codelineno-0-79 name=__codelineno-0-79></a>
</span><span id=__span-0-80><a id=__codelineno-0-80 name=__codelineno-0-80></a>    <span class=bp>self</span><span class=o>.</span><span class=n>training_parameters</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>training_parameters</span>
</span><span id=__span-0-81><a id=__codelineno-0-81 name=__codelineno-0-81></a>
</span><span id=__span-0-82><a id=__codelineno-0-82 name=__codelineno-0-82></a>    <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>callbacks</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-83><a id=__codelineno-0-83 name=__codelineno-0-83></a>        <span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>CallbackCollection</span>
</span><span id=__span-0-84><a id=__codelineno-0-84 name=__codelineno-0-84></a>
</span><span id=__span-0-85><a id=__codelineno-0-85 name=__codelineno-0-85></a>        <span class=n>config</span><span class=o>.</span><span class=n>callbacks</span> <span class=o>=</span> <span class=n>CallbackCollection</span><span class=p>()</span>
</span><span id=__span-0-86><a id=__codelineno-0-86 name=__codelineno-0-86></a>
</span><span id=__span-0-87><a id=__codelineno-0-87 name=__codelineno-0-87></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>callbacks</span>
</span><span id=__span-0-88><a id=__codelineno-0-88 name=__codelineno-0-88></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>initialize</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-0-89><a id=__codelineno-0-89 name=__codelineno-0-89></a>
</span><span id=__span-0-90><a id=__codelineno-0-90 name=__codelineno-0-90></a>    <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-91><a id=__codelineno-0-91 name=__codelineno-0-91></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_local_step</span> <span class=o>=</span> <span class=mi>0</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <h3 id=dream_trainer.trainer.BaseTrainer-attributes>Attributes<a href=#dream_trainer.trainer.BaseTrainer-attributes class=headerlink title="Permanent link">&para;</a></h3> <div class="doc doc-object doc-attribute"> <h4 id=dream_trainer.trainer.BaseTrainer.is_accumulating_gradients class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code> <span class="doc doc-object-name doc-attribute-name">is_accumulating_gradients</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-property"><code>property</code></small> </span> <a href=#dream_trainer.trainer.BaseTrainer.is_accumulating_gradients class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>is_accumulating_gradients</span><span class=p>:</span> <span class=nb>bool</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Check if currently accumulating gradients.</p> <p>Returns True if the current step is a gradient accumulation step (i.e., gradients are being accumulated but not yet applied). Returns False if this is the step where accumulated gradients will be applied, or if we're on the last training batch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>bool</code> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>True if accumulating gradients, False if applying them.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-returns-annotation> <b>TYPE:</b> <code><span title=bool>bool</span></code> </span> </p> </td> </tr> </tbody> </table> </div> </div> <h3 id=dream_trainer.trainer.BaseTrainer-functions>Functions<a href=#dream_trainer.trainer.BaseTrainer-functions class=headerlink title="Permanent link">&para;</a></h3> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.state_dict class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">state_dict</span> <a href=#dream_trainer.trainer.BaseTrainer.state_dict class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>state_dict</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Return the complete state dictionary of the trainer.</p> <p>This method captures the entire training state including: - Trainer metadata (global step, current epoch, callbacks state) - All model states - All optimizer states - All scheduler states - Dataloader states (if stateful)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-returns-annotation> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>dict[str, Any]: A dictionary containing the complete trainer state that can be used to resume training from a checkpoint.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-97> 97</a></span>
<span class=normal><a href=#__codelineno-0-98> 98</a></span>
<span class=normal><a href=#__codelineno-0-99> 99</a></span>
<span class=normal><a href=#__codelineno-0-100>100</a></span>
<span class=normal><a href=#__codelineno-0-101>101</a></span>
<span class=normal><a href=#__codelineno-0-102>102</a></span>
<span class=normal><a href=#__codelineno-0-103>103</a></span>
<span class=normal><a href=#__codelineno-0-104>104</a></span>
<span class=normal><a href=#__codelineno-0-105>105</a></span>
<span class=normal><a href=#__codelineno-0-106>106</a></span>
<span class=normal><a href=#__codelineno-0-107>107</a></span>
<span class=normal><a href=#__codelineno-0-108>108</a></span>
<span class=normal><a href=#__codelineno-0-109>109</a></span>
<span class=normal><a href=#__codelineno-0-110>110</a></span>
<span class=normal><a href=#__codelineno-0-111>111</a></span>
<span class=normal><a href=#__codelineno-0-112>112</a></span>
<span class=normal><a href=#__codelineno-0-113>113</a></span>
<span class=normal><a href=#__codelineno-0-114>114</a></span>
<span class=normal><a href=#__codelineno-0-115>115</a></span>
<span class=normal><a href=#__codelineno-0-116>116</a></span>
<span class=normal><a href=#__codelineno-0-117>117</a></span>
<span class=normal><a href=#__codelineno-0-118>118</a></span>
<span class=normal><a href=#__codelineno-0-119>119</a></span>
<span class=normal><a href=#__codelineno-0-120>120</a></span>
<span class=normal><a href=#__codelineno-0-121>121</a></span>
<span class=normal><a href=#__codelineno-0-122>122</a></span>
<span class=normal><a href=#__codelineno-0-123>123</a></span>
<span class=normal><a href=#__codelineno-0-124>124</a></span>
<span class=normal><a href=#__codelineno-0-125>125</a></span>
<span class=normal><a href=#__codelineno-0-126>126</a></span>
<span class=normal><a href=#__codelineno-0-127>127</a></span>
<span class=normal><a href=#__codelineno-0-128>128</a></span>
<span class=normal><a href=#__codelineno-0-129>129</a></span>
<span class=normal><a href=#__codelineno-0-130>130</a></span>
<span class=normal><a href=#__codelineno-0-131>131</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-97><a id=__codelineno-0-97 name=__codelineno-0-97></a><span class=k>def</span><span class=w> </span><span class=nf>state_dict</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span><span id=__span-0-98><a id=__codelineno-0-98 name=__codelineno-0-98></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-99><a id=__codelineno-0-99 name=__codelineno-0-99></a><span class=sd>    Return the complete state dictionary of the trainer.</span>
</span><span id=__span-0-100><a id=__codelineno-0-100 name=__codelineno-0-100></a>
</span><span id=__span-0-101><a id=__codelineno-0-101 name=__codelineno-0-101></a><span class=sd>    This method captures the entire training state including:</span>
</span><span id=__span-0-102><a id=__codelineno-0-102 name=__codelineno-0-102></a><span class=sd>    - Trainer metadata (global step, current epoch, callbacks state)</span>
</span><span id=__span-0-103><a id=__codelineno-0-103 name=__codelineno-0-103></a><span class=sd>    - All model states</span>
</span><span id=__span-0-104><a id=__codelineno-0-104 name=__codelineno-0-104></a><span class=sd>    - All optimizer states</span>
</span><span id=__span-0-105><a id=__codelineno-0-105 name=__codelineno-0-105></a><span class=sd>    - All scheduler states</span>
</span><span id=__span-0-106><a id=__codelineno-0-106 name=__codelineno-0-106></a><span class=sd>    - Dataloader states (if stateful)</span>
</span><span id=__span-0-107><a id=__codelineno-0-107 name=__codelineno-0-107></a>
</span><span id=__span-0-108><a id=__codelineno-0-108 name=__codelineno-0-108></a><span class=sd>    Returns:</span>
</span><span id=__span-0-109><a id=__codelineno-0-109 name=__codelineno-0-109></a><span class=sd>        dict[str, Any]: A dictionary containing the complete trainer state</span>
</span><span id=__span-0-110><a id=__codelineno-0-110 name=__codelineno-0-110></a><span class=sd>            that can be used to resume training from a checkpoint.</span>
</span><span id=__span-0-111><a id=__codelineno-0-111 name=__codelineno-0-111></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-112><a id=__codelineno-0-112 name=__codelineno-0-112></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-0-113><a id=__codelineno-0-113 name=__codelineno-0-113></a>        <span class=s2>&quot;trainer&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-0-114><a id=__codelineno-0-114 name=__codelineno-0-114></a>            <span class=s2>&quot;global_step&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span><span class=p>,</span>
</span><span id=__span-0-115><a id=__codelineno-0-115 name=__codelineno-0-115></a>            <span class=s2>&quot;current_epoch&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_epoch</span><span class=p>,</span>
</span><span id=__span-0-116><a id=__codelineno-0-116 name=__codelineno-0-116></a>            <span class=s2>&quot;callbacks&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span><span id=__span-0-117><a id=__codelineno-0-117 name=__codelineno-0-117></a>        <span class=p>},</span>
</span><span id=__span-0-118><a id=__codelineno-0-118 name=__codelineno-0-118></a>        <span class=s2>&quot;models&quot;</span><span class=p>:</span> <span class=p>{</span><span class=n>name</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span> <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_models</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()},</span>
</span><span id=__span-0-119><a id=__codelineno-0-119 name=__codelineno-0-119></a>        <span class=s2>&quot;optimizers&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-0-120><a id=__codelineno-0-120 name=__codelineno-0-120></a>            <span class=n>name</span><span class=p>:</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span><span id=__span-0-121><a id=__codelineno-0-121 name=__codelineno-0-121></a>            <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>optimizer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_optimizers</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-0-122><a id=__codelineno-0-122 name=__codelineno-0-122></a>        <span class=p>},</span>
</span><span id=__span-0-123><a id=__codelineno-0-123 name=__codelineno-0-123></a>        <span class=s2>&quot;schedulers&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-0-124><a id=__codelineno-0-124 name=__codelineno-0-124></a>            <span class=n>name</span><span class=p>:</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span><span id=__span-0-125><a id=__codelineno-0-125 name=__codelineno-0-125></a>            <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>scheduler</span> <span class=ow>in</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>named_schedulers</span><span class=p>()</span> <span class=ow>or</span> <span class=p>{})</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span><span id=__span-0-126><a id=__codelineno-0-126 name=__codelineno-0-126></a>        <span class=p>},</span>
</span><span id=__span-0-127><a id=__codelineno-0-127 name=__codelineno-0-127></a>        <span class=s2>&quot;dataloaders&quot;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-0-128><a id=__codelineno-0-128 name=__codelineno-0-128></a>            <span class=s2>&quot;train&quot;</span><span class=p>:</span> <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>,</span> <span class=s2>&quot;state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=p>{})(),</span>
</span><span id=__span-0-129><a id=__codelineno-0-129 name=__codelineno-0-129></a>            <span class=s2>&quot;val&quot;</span><span class=p>:</span> <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>val_dataloader</span><span class=p>,</span> <span class=s2>&quot;state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=p>{})(),</span>
</span><span id=__span-0-130><a id=__codelineno-0-130 name=__codelineno-0-130></a>        <span class=p>},</span>
</span><span id=__span-0-131><a id=__codelineno-0-131 name=__codelineno-0-131></a>    <span class=p>}</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.load_state_dict class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">load_state_dict</span> <a href=#dream_trainer.trainer.BaseTrainer.load_state_dict class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>load_state_dict</span><span class=p>(</span><span class=n>state_dict</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>strict</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Load a complete state dictionary into the trainer.</p> <p>This method restores the entire training state from a checkpoint, including trainer metadata, model states, optimizer states, scheduler states, and dataloader states.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>state_dict</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Dictionary containing the complete trainer state, typically obtained from a previous call to state_dict().</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>strict</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>If True, raises ValueError when state_dict contains keys that don't match the current trainer setup. If False, logs warnings for mismatched keys instead.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=bool>bool</span></code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code>True</code> </span> </p> </td> </tr> </tbody> </table> <table> <thead> <tr> <th><span class=doc-section-title>RAISES</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-raises-annotation> <code><span title=ValueError>ValueError</span></code> </span> </td> <td class=doc-raises-details> <div class=doc-md-description> <p>If strict=True and state_dict contains unexpected keys.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-133>133</a></span>
<span class=normal><a href=#__codelineno-0-134>134</a></span>
<span class=normal><a href=#__codelineno-0-135>135</a></span>
<span class=normal><a href=#__codelineno-0-136>136</a></span>
<span class=normal><a href=#__codelineno-0-137>137</a></span>
<span class=normal><a href=#__codelineno-0-138>138</a></span>
<span class=normal><a href=#__codelineno-0-139>139</a></span>
<span class=normal><a href=#__codelineno-0-140>140</a></span>
<span class=normal><a href=#__codelineno-0-141>141</a></span>
<span class=normal><a href=#__codelineno-0-142>142</a></span>
<span class=normal><a href=#__codelineno-0-143>143</a></span>
<span class=normal><a href=#__codelineno-0-144>144</a></span>
<span class=normal><a href=#__codelineno-0-145>145</a></span>
<span class=normal><a href=#__codelineno-0-146>146</a></span>
<span class=normal><a href=#__codelineno-0-147>147</a></span>
<span class=normal><a href=#__codelineno-0-148>148</a></span>
<span class=normal><a href=#__codelineno-0-149>149</a></span>
<span class=normal><a href=#__codelineno-0-150>150</a></span>
<span class=normal><a href=#__codelineno-0-151>151</a></span>
<span class=normal><a href=#__codelineno-0-152>152</a></span>
<span class=normal><a href=#__codelineno-0-153>153</a></span>
<span class=normal><a href=#__codelineno-0-154>154</a></span>
<span class=normal><a href=#__codelineno-0-155>155</a></span>
<span class=normal><a href=#__codelineno-0-156>156</a></span>
<span class=normal><a href=#__codelineno-0-157>157</a></span>
<span class=normal><a href=#__codelineno-0-158>158</a></span>
<span class=normal><a href=#__codelineno-0-159>159</a></span>
<span class=normal><a href=#__codelineno-0-160>160</a></span>
<span class=normal><a href=#__codelineno-0-161>161</a></span>
<span class=normal><a href=#__codelineno-0-162>162</a></span>
<span class=normal><a href=#__codelineno-0-163>163</a></span>
<span class=normal><a href=#__codelineno-0-164>164</a></span>
<span class=normal><a href=#__codelineno-0-165>165</a></span>
<span class=normal><a href=#__codelineno-0-166>166</a></span>
<span class=normal><a href=#__codelineno-0-167>167</a></span>
<span class=normal><a href=#__codelineno-0-168>168</a></span>
<span class=normal><a href=#__codelineno-0-169>169</a></span>
<span class=normal><a href=#__codelineno-0-170>170</a></span>
<span class=normal><a href=#__codelineno-0-171>171</a></span>
<span class=normal><a href=#__codelineno-0-172>172</a></span>
<span class=normal><a href=#__codelineno-0-173>173</a></span>
<span class=normal><a href=#__codelineno-0-174>174</a></span>
<span class=normal><a href=#__codelineno-0-175>175</a></span>
<span class=normal><a href=#__codelineno-0-176>176</a></span>
<span class=normal><a href=#__codelineno-0-177>177</a></span>
<span class=normal><a href=#__codelineno-0-178>178</a></span>
<span class=normal><a href=#__codelineno-0-179>179</a></span>
<span class=normal><a href=#__codelineno-0-180>180</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-133><a id=__codelineno-0-133 name=__codelineno-0-133></a><span class=k>def</span><span class=w> </span><span class=nf>load_state_dict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state_dict</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>strict</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-134><a id=__codelineno-0-134 name=__codelineno-0-134></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-135><a id=__codelineno-0-135 name=__codelineno-0-135></a><span class=sd>    Load a complete state dictionary into the trainer.</span>
</span><span id=__span-0-136><a id=__codelineno-0-136 name=__codelineno-0-136></a>
</span><span id=__span-0-137><a id=__codelineno-0-137 name=__codelineno-0-137></a><span class=sd>    This method restores the entire training state from a checkpoint,</span>
</span><span id=__span-0-138><a id=__codelineno-0-138 name=__codelineno-0-138></a><span class=sd>    including trainer metadata, model states, optimizer states, scheduler states,</span>
</span><span id=__span-0-139><a id=__codelineno-0-139 name=__codelineno-0-139></a><span class=sd>    and dataloader states.</span>
</span><span id=__span-0-140><a id=__codelineno-0-140 name=__codelineno-0-140></a>
</span><span id=__span-0-141><a id=__codelineno-0-141 name=__codelineno-0-141></a><span class=sd>    Args:</span>
</span><span id=__span-0-142><a id=__codelineno-0-142 name=__codelineno-0-142></a><span class=sd>        state_dict: Dictionary containing the complete trainer state,</span>
</span><span id=__span-0-143><a id=__codelineno-0-143 name=__codelineno-0-143></a><span class=sd>            typically obtained from a previous call to state_dict().</span>
</span><span id=__span-0-144><a id=__codelineno-0-144 name=__codelineno-0-144></a><span class=sd>        strict: If True, raises ValueError when state_dict contains keys</span>
</span><span id=__span-0-145><a id=__codelineno-0-145 name=__codelineno-0-145></a><span class=sd>            that don&#39;t match the current trainer setup. If False, logs</span>
</span><span id=__span-0-146><a id=__codelineno-0-146 name=__codelineno-0-146></a><span class=sd>            warnings for mismatched keys instead.</span>
</span><span id=__span-0-147><a id=__codelineno-0-147 name=__codelineno-0-147></a>
</span><span id=__span-0-148><a id=__codelineno-0-148 name=__codelineno-0-148></a><span class=sd>    Raises:</span>
</span><span id=__span-0-149><a id=__codelineno-0-149 name=__codelineno-0-149></a><span class=sd>        ValueError: If strict=True and state_dict contains unexpected keys.</span>
</span><span id=__span-0-150><a id=__codelineno-0-150 name=__codelineno-0-150></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-151><a id=__codelineno-0-151 name=__codelineno-0-151></a>    <span class=c1># Load Trainer State</span>
</span><span id=__span-0-152><a id=__codelineno-0-152 name=__codelineno-0-152></a>    <span class=n>trainer_state</span> <span class=o>=</span> <span class=n>state_dict</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;trainer&quot;</span><span class=p>)</span>
</span><span id=__span-0-153><a id=__codelineno-0-153 name=__codelineno-0-153></a>    <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span> <span class=o>=</span> <span class=n>trainer_state</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;global_step&quot;</span><span class=p>)</span>
</span><span id=__span-0-154><a id=__codelineno-0-154 name=__codelineno-0-154></a>    <span class=bp>self</span><span class=o>.</span><span class=n>current_epoch</span> <span class=o>=</span> <span class=n>trainer_state</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;current_epoch&quot;</span><span class=p>)</span>
</span><span id=__span-0-155><a id=__codelineno-0-155 name=__codelineno-0-155></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>trainer_state</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;callbacks&quot;</span><span class=p>),</span> <span class=bp>self</span><span class=p>)</span>
</span><span id=__span-0-156><a id=__codelineno-0-156 name=__codelineno-0-156></a>
</span><span id=__span-0-157><a id=__codelineno-0-157 name=__codelineno-0-157></a>    <span class=c1># Load Model State</span>
</span><span id=__span-0-158><a id=__codelineno-0-158 name=__codelineno-0-158></a>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_models</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-0-159><a id=__codelineno-0-159 name=__codelineno-0-159></a>        <span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>state_dict</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;models&quot;</span><span class=p>)[</span><span class=n>name</span><span class=p>],</span> <span class=n>strict</span><span class=o>=</span><span class=n>strict</span><span class=p>)</span>
</span><span id=__span-0-160><a id=__codelineno-0-160 name=__codelineno-0-160></a>
</span><span id=__span-0-161><a id=__codelineno-0-161 name=__codelineno-0-161></a>    <span class=c1># Load Optimizer State</span>
</span><span id=__span-0-162><a id=__codelineno-0-162 name=__codelineno-0-162></a>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>optimizer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_optimizers</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-0-163><a id=__codelineno-0-163 name=__codelineno-0-163></a>        <span class=n>optimizer</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>state_dict</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;optimizers&quot;</span><span class=p>)[</span><span class=n>name</span><span class=p>])</span>
</span><span id=__span-0-164><a id=__codelineno-0-164 name=__codelineno-0-164></a>
</span><span id=__span-0-165><a id=__codelineno-0-165 name=__codelineno-0-165></a>    <span class=c1># Load Scheduler State</span>
</span><span id=__span-0-166><a id=__codelineno-0-166 name=__codelineno-0-166></a>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>scheduler</span> <span class=ow>in</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>named_schedulers</span><span class=p>()</span> <span class=ow>or</span> <span class=p>{})</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-0-167><a id=__codelineno-0-167 name=__codelineno-0-167></a>        <span class=n>scheduler</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>state_dict</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;schedulers&quot;</span><span class=p>)[</span><span class=n>name</span><span class=p>])</span>
</span><span id=__span-0-168><a id=__codelineno-0-168 name=__codelineno-0-168></a>
</span><span id=__span-0-169><a id=__codelineno-0-169 name=__codelineno-0-169></a>    <span class=c1># Load Dataloader State</span>
</span><span id=__span-0-170><a id=__codelineno-0-170 name=__codelineno-0-170></a>    <span class=n>dataloader_state</span> <span class=o>=</span> <span class=n>state_dict</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;dataloaders&quot;</span><span class=p>)</span>
</span><span id=__span-0-171><a id=__codelineno-0-171 name=__codelineno-0-171></a>    <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>,</span> <span class=s2>&quot;load_state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>_</span><span class=p>:</span> <span class=kc>None</span><span class=p>)(</span>
</span><span id=__span-0-172><a id=__codelineno-0-172 name=__codelineno-0-172></a>        <span class=n>dataloader_state</span><span class=p>[</span><span class=s2>&quot;train&quot;</span><span class=p>]</span>
</span><span id=__span-0-173><a id=__codelineno-0-173 name=__codelineno-0-173></a>    <span class=p>)</span>
</span><span id=__span-0-174><a id=__codelineno-0-174 name=__codelineno-0-174></a>    <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>val_dataloader</span><span class=p>,</span> <span class=s2>&quot;load_state_dict&quot;</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>_</span><span class=p>:</span> <span class=kc>None</span><span class=p>)(</span><span class=n>dataloader_state</span><span class=p>[</span><span class=s2>&quot;val&quot;</span><span class=p>])</span>
</span><span id=__span-0-175><a id=__codelineno-0-175 name=__codelineno-0-175></a>
</span><span id=__span-0-176><a id=__codelineno-0-176 name=__codelineno-0-176></a>    <span class=k>if</span> <span class=n>state_dict</span><span class=p>:</span>
</span><span id=__span-0-177><a id=__codelineno-0-177 name=__codelineno-0-177></a>        <span class=k>if</span> <span class=n>strict</span><span class=p>:</span>
</span><span id=__span-0-178><a id=__codelineno-0-178 name=__codelineno-0-178></a>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Missing keys in state_dict: </span><span class=si>{</span><span class=n>state_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-179><a id=__codelineno-0-179 name=__codelineno-0-179></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-180><a id=__codelineno-0-180 name=__codelineno-0-180></a>            <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Missing keys in state_dict: </span><span class=si>{</span><span class=n>state_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.fit class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">fit</span> <a href=#dream_trainer.trainer.BaseTrainer.fit class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>fit</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Execute the complete training pipeline.</p> <p>This is the main entry point for training. It handles the entire training lifecycle including setup, training loops, validation, and cleanup.</p> <p>The method ensures proper cleanup by destroying the distributed process group in the finally block, even if training is interrupted.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-182>182</a></span>
<span class=normal><a href=#__codelineno-0-183>183</a></span>
<span class=normal><a href=#__codelineno-0-184>184</a></span>
<span class=normal><a href=#__codelineno-0-185>185</a></span>
<span class=normal><a href=#__codelineno-0-186>186</a></span>
<span class=normal><a href=#__codelineno-0-187>187</a></span>
<span class=normal><a href=#__codelineno-0-188>188</a></span>
<span class=normal><a href=#__codelineno-0-189>189</a></span>
<span class=normal><a href=#__codelineno-0-190>190</a></span>
<span class=normal><a href=#__codelineno-0-191>191</a></span>
<span class=normal><a href=#__codelineno-0-192>192</a></span>
<span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-182><a id=__codelineno-0-182 name=__codelineno-0-182></a><span class=nd>@override</span>
</span><span id=__span-0-183><a id=__codelineno-0-183 name=__codelineno-0-183></a><span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-184><a id=__codelineno-0-184 name=__codelineno-0-184></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-185><a id=__codelineno-0-185 name=__codelineno-0-185></a><span class=sd>    Execute the complete training pipeline.</span>
</span><span id=__span-0-186><a id=__codelineno-0-186 name=__codelineno-0-186></a>
</span><span id=__span-0-187><a id=__codelineno-0-187 name=__codelineno-0-187></a><span class=sd>    This is the main entry point for training. It handles the entire training</span>
</span><span id=__span-0-188><a id=__codelineno-0-188 name=__codelineno-0-188></a><span class=sd>    lifecycle including setup, training loops, validation, and cleanup.</span>
</span><span id=__span-0-189><a id=__codelineno-0-189 name=__codelineno-0-189></a>
</span><span id=__span-0-190><a id=__codelineno-0-190 name=__codelineno-0-190></a><span class=sd>    The method ensures proper cleanup by destroying the distributed process</span>
</span><span id=__span-0-191><a id=__codelineno-0-191 name=__codelineno-0-191></a><span class=sd>    group in the finally block, even if training is interrupted.</span>
</span><span id=__span-0-192><a id=__codelineno-0-192 name=__codelineno-0-192></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-193><a id=__codelineno-0-193 name=__codelineno-0-193></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-0-194><a id=__codelineno-0-194 name=__codelineno-0-194></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_fit</span><span class=p>()</span>
</span><span id=__span-0-195><a id=__codelineno-0-195 name=__codelineno-0-195></a>    <span class=k>finally</span><span class=p>:</span>
</span><span id=__span-0-196><a id=__codelineno-0-196 name=__codelineno-0-196></a>        <span class=c1># TODO: close the checkpointer</span>
</span><span id=__span-0-197><a id=__codelineno-0-197 name=__codelineno-0-197></a>
</span><span id=__span-0-198><a id=__codelineno-0-198 name=__codelineno-0-198></a>        <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>is_initialized</span><span class=p>():</span>
</span><span id=__span-0-199><a id=__codelineno-0-199 name=__codelineno-0-199></a>            <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>destroy_process_group</span><span class=p>()</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.training_step class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">training_step</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small> </span> <a href=#dream_trainer.trainer.BaseTrainer.training_step class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>training_step</span><span class=p>(</span><span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>batch_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Execute a single training step.</p> <p>This method should implement the forward pass, loss computation, and backward pass for a single batch of training data.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>batch</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Dictionary containing the batch data, typically with keys like 'input', 'target', etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>batch_idx</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Index of the current batch within the epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=int>int</span></code> </span> </p> </td> </tr> </tbody> </table> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-returns-annotation> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>dict[str, Any]: Dictionary containing at minimum the computed loss and any other metrics or values to log.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-205><a id=__codelineno-0-205 name=__codelineno-0-205></a><span class=nd>@abstractmethod</span>
</span><span id=__span-0-206><a id=__codelineno-0-206 name=__codelineno-0-206></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>batch_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span><span id=__span-0-207><a id=__codelineno-0-207 name=__codelineno-0-207></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-208><a id=__codelineno-0-208 name=__codelineno-0-208></a><span class=sd>    Execute a single training step.</span>
</span><span id=__span-0-209><a id=__codelineno-0-209 name=__codelineno-0-209></a>
</span><span id=__span-0-210><a id=__codelineno-0-210 name=__codelineno-0-210></a><span class=sd>    This method should implement the forward pass, loss computation,</span>
</span><span id=__span-0-211><a id=__codelineno-0-211 name=__codelineno-0-211></a><span class=sd>    and backward pass for a single batch of training data.</span>
</span><span id=__span-0-212><a id=__codelineno-0-212 name=__codelineno-0-212></a>
</span><span id=__span-0-213><a id=__codelineno-0-213 name=__codelineno-0-213></a><span class=sd>    Args:</span>
</span><span id=__span-0-214><a id=__codelineno-0-214 name=__codelineno-0-214></a><span class=sd>        batch: Dictionary containing the batch data, typically with keys</span>
</span><span id=__span-0-215><a id=__codelineno-0-215 name=__codelineno-0-215></a><span class=sd>            like &#39;input&#39;, &#39;target&#39;, etc.</span>
</span><span id=__span-0-216><a id=__codelineno-0-216 name=__codelineno-0-216></a><span class=sd>        batch_idx: Index of the current batch within the epoch.</span>
</span><span id=__span-0-217><a id=__codelineno-0-217 name=__codelineno-0-217></a>
</span><span id=__span-0-218><a id=__codelineno-0-218 name=__codelineno-0-218></a><span class=sd>    Returns:</span>
</span><span id=__span-0-219><a id=__codelineno-0-219 name=__codelineno-0-219></a><span class=sd>        dict[str, Any]: Dictionary containing at minimum the computed loss</span>
</span><span id=__span-0-220><a id=__codelineno-0-220 name=__codelineno-0-220></a><span class=sd>            and any other metrics or values to log.</span>
</span><span id=__span-0-221><a id=__codelineno-0-221 name=__codelineno-0-221></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-222><a id=__codelineno-0-222 name=__codelineno-0-222></a>    <span class=k>pass</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.validation_step class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">validation_step</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small> </span> <a href=#dream_trainer.trainer.BaseTrainer.validation_step class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>validation_step</span><span class=p>(</span><span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>batch_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Execute a single validation step.</p> <p>This method should implement the forward pass and metric computation for a single batch of validation data. No gradients should be computed.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>batch</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Dictionary containing the batch data, typically with keys like 'input', 'target', etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>batch_idx</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>Index of the current batch within the validation epoch.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=int>int</span></code> </span> </p> </td> </tr> </tbody> </table> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-returns-annotation> <code><span title=dict>dict</span>[<span title=str>str</span>, <span title=typing.Any>Any</span>]</code> </span> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>dict[str, Any]: Dictionary containing validation metrics and any other values to log.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-224><a id=__codelineno-0-224 name=__codelineno-0-224></a><span class=nd>@abstractmethod</span>
</span><span id=__span-0-225><a id=__codelineno-0-225 name=__codelineno-0-225></a><span class=k>def</span><span class=w> </span><span class=nf>validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=n>batch_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span><span id=__span-0-226><a id=__codelineno-0-226 name=__codelineno-0-226></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-227><a id=__codelineno-0-227 name=__codelineno-0-227></a><span class=sd>    Execute a single validation step.</span>
</span><span id=__span-0-228><a id=__codelineno-0-228 name=__codelineno-0-228></a>
</span><span id=__span-0-229><a id=__codelineno-0-229 name=__codelineno-0-229></a><span class=sd>    This method should implement the forward pass and metric computation</span>
</span><span id=__span-0-230><a id=__codelineno-0-230 name=__codelineno-0-230></a><span class=sd>    for a single batch of validation data. No gradients should be computed.</span>
</span><span id=__span-0-231><a id=__codelineno-0-231 name=__codelineno-0-231></a>
</span><span id=__span-0-232><a id=__codelineno-0-232 name=__codelineno-0-232></a><span class=sd>    Args:</span>
</span><span id=__span-0-233><a id=__codelineno-0-233 name=__codelineno-0-233></a><span class=sd>        batch: Dictionary containing the batch data, typically with keys</span>
</span><span id=__span-0-234><a id=__codelineno-0-234 name=__codelineno-0-234></a><span class=sd>            like &#39;input&#39;, &#39;target&#39;, etc.</span>
</span><span id=__span-0-235><a id=__codelineno-0-235 name=__codelineno-0-235></a><span class=sd>        batch_idx: Index of the current batch within the validation epoch.</span>
</span><span id=__span-0-236><a id=__codelineno-0-236 name=__codelineno-0-236></a>
</span><span id=__span-0-237><a id=__codelineno-0-237 name=__codelineno-0-237></a><span class=sd>    Returns:</span>
</span><span id=__span-0-238><a id=__codelineno-0-238 name=__codelineno-0-238></a><span class=sd>        dict[str, Any]: Dictionary containing validation metrics and any</span>
</span><span id=__span-0-239><a id=__codelineno-0-239 name=__codelineno-0-239></a><span class=sd>            other values to log.</span>
</span><span id=__span-0-240><a id=__codelineno-0-240 name=__codelineno-0-240></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-241><a id=__codelineno-0-241 name=__codelineno-0-241></a>    <span class=k>pass</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.eval class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">eval</span> <a href=#dream_trainer.trainer.BaseTrainer.eval class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>eval</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Set the trainer and all models to evaluation mode.</p> <p>This method: - Sets the trainer's training flag to False - Calls eval() on all registered models</p> <p>This should be called before validation or inference to disable dropout, batch normalization updates, and other training-specific behaviors.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-247><a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=k>def</span><span class=w> </span><span class=nf>eval</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-248><a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-249><a id=__codelineno-0-249 name=__codelineno-0-249></a><span class=sd>    Set the trainer and all models to evaluation mode.</span>
</span><span id=__span-0-250><a id=__codelineno-0-250 name=__codelineno-0-250></a>
</span><span id=__span-0-251><a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>    This method:</span>
</span><span id=__span-0-252><a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>    - Sets the trainer&#39;s training flag to False</span>
</span><span id=__span-0-253><a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>    - Calls eval() on all registered models</span>
</span><span id=__span-0-254><a id=__codelineno-0-254 name=__codelineno-0-254></a>
</span><span id=__span-0-255><a id=__codelineno-0-255 name=__codelineno-0-255></a><span class=sd>    This should be called before validation or inference to disable</span>
</span><span id=__span-0-256><a id=__codelineno-0-256 name=__codelineno-0-256></a><span class=sd>    dropout, batch normalization updates, and other training-specific</span>
</span><span id=__span-0-257><a id=__codelineno-0-257 name=__codelineno-0-257></a><span class=sd>    behaviors.</span>
</span><span id=__span-0-258><a id=__codelineno-0-258 name=__codelineno-0-258></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-259><a id=__codelineno-0-259 name=__codelineno-0-259></a>    <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-260><a id=__codelineno-0-260 name=__codelineno-0-260></a>    <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_models</span><span class=p>()</span><span class=o>.</span><span class=n>values</span><span class=p>():</span>
</span><span id=__span-0-261><a id=__codelineno-0-261 name=__codelineno-0-261></a>        <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.train class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">train</span> <a href=#dream_trainer.trainer.BaseTrainer.train class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>train</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Set the trainer and models to training mode.</p> <p>This method: - Sets the trainer's training flag to True - Calls train() on all registered models that have trainable parameters</p> <p>Models without any parameters requiring gradients are kept in eval mode to avoid unnecessary computation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span>
<span class=normal><a href=#__codelineno-0-270>270</a></span>
<span class=normal><a href=#__codelineno-0-271>271</a></span>
<span class=normal><a href=#__codelineno-0-272>272</a></span>
<span class=normal><a href=#__codelineno-0-273>273</a></span>
<span class=normal><a href=#__codelineno-0-274>274</a></span>
<span class=normal><a href=#__codelineno-0-275>275</a></span>
<span class=normal><a href=#__codelineno-0-276>276</a></span>
<span class=normal><a href=#__codelineno-0-277>277</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-263><a id=__codelineno-0-263 name=__codelineno-0-263></a><span class=k>def</span><span class=w> </span><span class=nf>train</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-264><a id=__codelineno-0-264 name=__codelineno-0-264></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-265><a id=__codelineno-0-265 name=__codelineno-0-265></a><span class=sd>    Set the trainer and models to training mode.</span>
</span><span id=__span-0-266><a id=__codelineno-0-266 name=__codelineno-0-266></a>
</span><span id=__span-0-267><a id=__codelineno-0-267 name=__codelineno-0-267></a><span class=sd>    This method:</span>
</span><span id=__span-0-268><a id=__codelineno-0-268 name=__codelineno-0-268></a><span class=sd>    - Sets the trainer&#39;s training flag to True</span>
</span><span id=__span-0-269><a id=__codelineno-0-269 name=__codelineno-0-269></a><span class=sd>    - Calls train() on all registered models that have trainable parameters</span>
</span><span id=__span-0-270><a id=__codelineno-0-270 name=__codelineno-0-270></a>
</span><span id=__span-0-271><a id=__codelineno-0-271 name=__codelineno-0-271></a><span class=sd>    Models without any parameters requiring gradients are kept in eval mode</span>
</span><span id=__span-0-272><a id=__codelineno-0-272 name=__codelineno-0-272></a><span class=sd>    to avoid unnecessary computation.</span>
</span><span id=__span-0-273><a id=__codelineno-0-273 name=__codelineno-0-273></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-274><a id=__codelineno-0-274 name=__codelineno-0-274></a>    <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=o>=</span> <span class=kc>True</span>
</span><span id=__span-0-275><a id=__codelineno-0-275 name=__codelineno-0-275></a>    <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_models</span><span class=p>()</span><span class=o>.</span><span class=n>values</span><span class=p>():</span>
</span><span id=__span-0-276><a id=__codelineno-0-276 name=__codelineno-0-276></a>        <span class=k>if</span> <span class=nb>any</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>()):</span>
</span><span id=__span-0-277><a id=__codelineno-0-277 name=__codelineno-0-277></a>            <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.step class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">step</span> <a href=#dream_trainer.trainer.BaseTrainer.step class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>step</span><span class=p>(</span><span class=n>model</span><span class=p>:</span> <span class=n>Module</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>Optimizer</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Performs a single optimization step for the given model and optimizer.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=this-method open> <summary><p>This method</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p></summary> <ul> <li>Computes the total gradient norm for all parameters with gradients.</li> <li>Clips gradients to the configured norm.</li> <li>Calls pre- and post-optimizer step callbacks.</li> <li>Performs the optimizer step.</li> <li>Calls pre- and post-optimizer zero_grad callbacks.</li> <li>Zeros the gradients.</li> <li>Steps the learning rate scheduler if one is associated with the optimizer.</li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </details> <p>The step is performed with autocast disabled to ensure numerical stability.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>model</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>The model whose parameters are being optimized.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=torch.nn.Module>Module</span></code> </span> </p> </td> </tr> <tr class=doc-section-item> <td> <code>optimizer</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>The optimizer used to update the model parameters.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=torch.optim.optimizer.Optimizer>Optimizer</span></code> </span> </p> </td> </tr> </tbody> </table> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-returns-annotation> <code><span title=torch.Tensor>Tensor</span></code> </span> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>torch.Tensor: The total norm of the gradients before clipping.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span>
<span class=normal><a href=#__codelineno-0-310>310</a></span>
<span class=normal><a href=#__codelineno-0-311>311</a></span>
<span class=normal><a href=#__codelineno-0-312>312</a></span>
<span class=normal><a href=#__codelineno-0-313>313</a></span>
<span class=normal><a href=#__codelineno-0-314>314</a></span>
<span class=normal><a href=#__codelineno-0-315>315</a></span>
<span class=normal><a href=#__codelineno-0-316>316</a></span>
<span class=normal><a href=#__codelineno-0-317>317</a></span>
<span class=normal><a href=#__codelineno-0-318>318</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-279><a id=__codelineno-0-279 name=__codelineno-0-279></a><span class=k>def</span><span class=w> </span><span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>:</span> <span class=n>Optimizer</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-280><a id=__codelineno-0-280 name=__codelineno-0-280></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-281><a id=__codelineno-0-281 name=__codelineno-0-281></a><span class=sd>    Performs a single optimization step for the given model and optimizer.</span>
</span><span id=__span-0-282><a id=__codelineno-0-282 name=__codelineno-0-282></a>
</span><span id=__span-0-283><a id=__codelineno-0-283 name=__codelineno-0-283></a><span class=sd>    This method:</span>
</span><span id=__span-0-284><a id=__codelineno-0-284 name=__codelineno-0-284></a><span class=sd>        - Computes the total gradient norm for all parameters with gradients.</span>
</span><span id=__span-0-285><a id=__codelineno-0-285 name=__codelineno-0-285></a><span class=sd>        - Clips gradients to the configured norm.</span>
</span><span id=__span-0-286><a id=__codelineno-0-286 name=__codelineno-0-286></a><span class=sd>        - Calls pre- and post-optimizer step callbacks.</span>
</span><span id=__span-0-287><a id=__codelineno-0-287 name=__codelineno-0-287></a><span class=sd>        - Performs the optimizer step.</span>
</span><span id=__span-0-288><a id=__codelineno-0-288 name=__codelineno-0-288></a><span class=sd>        - Calls pre- and post-optimizer zero_grad callbacks.</span>
</span><span id=__span-0-289><a id=__codelineno-0-289 name=__codelineno-0-289></a><span class=sd>        - Zeros the gradients.</span>
</span><span id=__span-0-290><a id=__codelineno-0-290 name=__codelineno-0-290></a><span class=sd>        - Steps the learning rate scheduler if one is associated with the optimizer.</span>
</span><span id=__span-0-291><a id=__codelineno-0-291 name=__codelineno-0-291></a>
</span><span id=__span-0-292><a id=__codelineno-0-292 name=__codelineno-0-292></a><span class=sd>    The step is performed with autocast disabled to ensure numerical stability.</span>
</span><span id=__span-0-293><a id=__codelineno-0-293 name=__codelineno-0-293></a>
</span><span id=__span-0-294><a id=__codelineno-0-294 name=__codelineno-0-294></a><span class=sd>    Args:</span>
</span><span id=__span-0-295><a id=__codelineno-0-295 name=__codelineno-0-295></a><span class=sd>        model (nn.Module): The model whose parameters are being optimized.</span>
</span><span id=__span-0-296><a id=__codelineno-0-296 name=__codelineno-0-296></a><span class=sd>        optimizer (Optimizer): The optimizer used to update the model parameters.</span>
</span><span id=__span-0-297><a id=__codelineno-0-297 name=__codelineno-0-297></a>
</span><span id=__span-0-298><a id=__codelineno-0-298 name=__codelineno-0-298></a><span class=sd>    Returns:</span>
</span><span id=__span-0-299><a id=__codelineno-0-299 name=__codelineno-0-299></a><span class=sd>        torch.Tensor: The total norm of the gradients before clipping.</span>
</span><span id=__span-0-300><a id=__codelineno-0-300 name=__codelineno-0-300></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-301><a id=__codelineno-0-301 name=__codelineno-0-301></a>    <span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>]</span>
</span><span id=__span-0-302><a id=__codelineno-0-302 name=__codelineno-0-302></a>    <span class=n>total_norm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>total_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>foreach</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-0-303><a id=__codelineno-0-303 name=__codelineno-0-303></a>    <span class=bp>self</span><span class=o>.</span><span class=n>clip_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>,</span> <span class=n>total_norm</span><span class=p>,</span> <span class=n>foreach</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-0-304><a id=__codelineno-0-304 name=__codelineno-0-304></a>
</span><span id=__span-0-305><a id=__codelineno-0-305 name=__codelineno-0-305></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_optimizer_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-0-306><a id=__codelineno-0-306 name=__codelineno-0-306></a>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-0-307><a id=__codelineno-0-307 name=__codelineno-0-307></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_optimizer_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-0-308><a id=__codelineno-0-308 name=__codelineno-0-308></a>
</span><span id=__span-0-309><a id=__codelineno-0-309 name=__codelineno-0-309></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_optimizer_zero_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-0-310><a id=__codelineno-0-310 name=__codelineno-0-310></a>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-0-311><a id=__codelineno-0-311 name=__codelineno-0-311></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_optimizer_zero_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-0-312><a id=__codelineno-0-312 name=__codelineno-0-312></a>
</span><span id=__span-0-313><a id=__codelineno-0-313 name=__codelineno-0-313></a>    <span class=k>if</span> <span class=p>(</span><span class=n>scheduler</span> <span class=o>:=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_scheduler_from_optimizer</span><span class=p>(</span><span class=n>optimizer</span><span class=p>))</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-314><a id=__codelineno-0-314 name=__codelineno-0-314></a>        <span class=k>with</span> <span class=n>warnings</span><span class=o>.</span><span class=n>catch_warnings</span><span class=p>():</span>
</span><span id=__span-0-315><a id=__codelineno-0-315 name=__codelineno-0-315></a>            <span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s2>&quot;ignore&quot;</span><span class=p>,</span> <span class=n>category</span><span class=o>=</span><span class=ne>UserWarning</span><span class=p>)</span>
</span><span id=__span-0-316><a id=__codelineno-0-316 name=__codelineno-0-316></a>            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-0-317><a id=__codelineno-0-317 name=__codelineno-0-317></a>
</span><span id=__span-0-318><a id=__codelineno-0-318 name=__codelineno-0-318></a>    <span class=k>return</span> <span class=n>total_norm</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.loss_parallel class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">loss_parallel</span> <a href=#dream_trainer.trainer.BaseTrainer.loss_parallel class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>loss_parallel</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Context manager for loss parallelism.</p> <p>This provides a context where loss computation can be parallelized with tensor parallelism on dim=-1.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>YIELDS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>None</code> </td> <td class=doc-yields-details> <div class=doc-md-description> <p>Context manager for loss parallel computation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=example open> <summary><p>Example</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p></summary> <p>with self.loss_parallel(): loss = self.compute_loss(outputs, targets)</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </details> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-320>320</a></span>
<span class=normal><a href=#__codelineno-0-321>321</a></span>
<span class=normal><a href=#__codelineno-0-322>322</a></span>
<span class=normal><a href=#__codelineno-0-323>323</a></span>
<span class=normal><a href=#__codelineno-0-324>324</a></span>
<span class=normal><a href=#__codelineno-0-325>325</a></span>
<span class=normal><a href=#__codelineno-0-326>326</a></span>
<span class=normal><a href=#__codelineno-0-327>327</a></span>
<span class=normal><a href=#__codelineno-0-328>328</a></span>
<span class=normal><a href=#__codelineno-0-329>329</a></span>
<span class=normal><a href=#__codelineno-0-330>330</a></span>
<span class=normal><a href=#__codelineno-0-331>331</a></span>
<span class=normal><a href=#__codelineno-0-332>332</a></span>
<span class=normal><a href=#__codelineno-0-333>333</a></span>
<span class=normal><a href=#__codelineno-0-334>334</a></span>
<span class=normal><a href=#__codelineno-0-335>335</a></span>
<span class=normal><a href=#__codelineno-0-336>336</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-320><a id=__codelineno-0-320 name=__codelineno-0-320></a><span class=nd>@contextlib</span><span class=o>.</span><span class=n>contextmanager</span>
</span><span id=__span-0-321><a id=__codelineno-0-321 name=__codelineno-0-321></a><span class=k>def</span><span class=w> </span><span class=nf>loss_parallel</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-322><a id=__codelineno-0-322 name=__codelineno-0-322></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-323><a id=__codelineno-0-323 name=__codelineno-0-323></a><span class=sd>    Context manager for loss parallelism.</span>
</span><span id=__span-0-324><a id=__codelineno-0-324 name=__codelineno-0-324></a>
</span><span id=__span-0-325><a id=__codelineno-0-325 name=__codelineno-0-325></a><span class=sd>    This provides a context where loss computation can be parallelized</span>
</span><span id=__span-0-326><a id=__codelineno-0-326 name=__codelineno-0-326></a><span class=sd>    with tensor parallelism on dim=-1.</span>
</span><span id=__span-0-327><a id=__codelineno-0-327 name=__codelineno-0-327></a>
</span><span id=__span-0-328><a id=__codelineno-0-328 name=__codelineno-0-328></a><span class=sd>    Yields:</span>
</span><span id=__span-0-329><a id=__codelineno-0-329 name=__codelineno-0-329></a><span class=sd>        None: Context manager for loss parallel computation.</span>
</span><span id=__span-0-330><a id=__codelineno-0-330 name=__codelineno-0-330></a>
</span><span id=__span-0-331><a id=__codelineno-0-331 name=__codelineno-0-331></a><span class=sd>    Example:</span>
</span><span id=__span-0-332><a id=__codelineno-0-332 name=__codelineno-0-332></a><span class=sd>        with self.loss_parallel():</span>
</span><span id=__span-0-333><a id=__codelineno-0-333 name=__codelineno-0-333></a><span class=sd>            loss = self.compute_loss(outputs, targets)</span>
</span><span id=__span-0-334><a id=__codelineno-0-334 name=__codelineno-0-334></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-335><a id=__codelineno-0-335 name=__codelineno-0-335></a>    <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>loss_parallel</span><span class=p>():</span>
</span><span id=__span-0-336><a id=__codelineno-0-336 name=__codelineno-0-336></a>        <span class=k>yield</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.backward class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">backward</span> <a href=#dream_trainer.trainer.BaseTrainer.backward class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Backward pass for loss, with gradient accumulation scaling and autocast disabled.</p> <p>This function is intended to be called inside a training step that is already wrapped in autocast (mixed precision). We explicitly disable autocast here to avoid calling backward in an autocast context, which can cause issues.</p> <p>The loss is divided by the number of gradient accumulation steps to ensure correct gradient scaling when using gradient accumulation.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>PARAMETER</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>loss</code> </td> <td class=doc-param-details> <div class=doc-md-description> <p>The computed loss tensor to backpropagate.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title=torch.Tensor>Tensor</span></code> </span> </p> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-338>338</a></span>
<span class=normal><a href=#__codelineno-0-339>339</a></span>
<span class=normal><a href=#__codelineno-0-340>340</a></span>
<span class=normal><a href=#__codelineno-0-341>341</a></span>
<span class=normal><a href=#__codelineno-0-342>342</a></span>
<span class=normal><a href=#__codelineno-0-343>343</a></span>
<span class=normal><a href=#__codelineno-0-344>344</a></span>
<span class=normal><a href=#__codelineno-0-345>345</a></span>
<span class=normal><a href=#__codelineno-0-346>346</a></span>
<span class=normal><a href=#__codelineno-0-347>347</a></span>
<span class=normal><a href=#__codelineno-0-348>348</a></span>
<span class=normal><a href=#__codelineno-0-349>349</a></span>
<span class=normal><a href=#__codelineno-0-350>350</a></span>
<span class=normal><a href=#__codelineno-0-351>351</a></span>
<span class=normal><a href=#__codelineno-0-352>352</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-338><a id=__codelineno-0-338 name=__codelineno-0-338></a><span class=k>def</span><span class=w> </span><span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>loss</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span><span id=__span-0-339><a id=__codelineno-0-339 name=__codelineno-0-339></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-340><a id=__codelineno-0-340 name=__codelineno-0-340></a><span class=sd>    Backward pass for loss, with gradient accumulation scaling and autocast disabled.</span>
</span><span id=__span-0-341><a id=__codelineno-0-341 name=__codelineno-0-341></a>
</span><span id=__span-0-342><a id=__codelineno-0-342 name=__codelineno-0-342></a><span class=sd>    This function is intended to be called inside a training step that is already</span>
</span><span id=__span-0-343><a id=__codelineno-0-343 name=__codelineno-0-343></a><span class=sd>    wrapped in autocast (mixed precision). We explicitly disable autocast here to</span>
</span><span id=__span-0-344><a id=__codelineno-0-344 name=__codelineno-0-344></a><span class=sd>    avoid calling backward in an autocast context, which can cause issues.</span>
</span><span id=__span-0-345><a id=__codelineno-0-345 name=__codelineno-0-345></a>
</span><span id=__span-0-346><a id=__codelineno-0-346 name=__codelineno-0-346></a><span class=sd>    The loss is divided by the number of gradient accumulation steps to ensure</span>
</span><span id=__span-0-347><a id=__codelineno-0-347 name=__codelineno-0-347></a><span class=sd>    correct gradient scaling when using gradient accumulation.</span>
</span><span id=__span-0-348><a id=__codelineno-0-348 name=__codelineno-0-348></a>
</span><span id=__span-0-349><a id=__codelineno-0-349 name=__codelineno-0-349></a><span class=sd>    Args:</span>
</span><span id=__span-0-350><a id=__codelineno-0-350 name=__codelineno-0-350></a><span class=sd>        loss (torch.Tensor): The computed loss tensor to backpropagate.</span>
</span><span id=__span-0-351><a id=__codelineno-0-351 name=__codelineno-0-351></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-352><a id=__codelineno-0-352 name=__codelineno-0-352></a>    <span class=p>(</span><span class=n>loss</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_gradient_accumulation_steps</span><span class=p>)</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.no_gradient_sync class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">no_gradient_sync</span> <a href=#dream_trainer.trainer.BaseTrainer.no_gradient_sync class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>no_gradient_sync</span><span class=p>(</span><span class=o>*</span><span class=n>models</span><span class=p>:</span> <span class=n>Module</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Disable gradient sync during accumulation steps and mark the final backward for FSDP.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=usage open> <summary><p>Usage</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p></summary> <p>with self.no_gradient_sync(self.model): loss.backward()</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </details> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-354>354</a></span>
<span class=normal><a href=#__codelineno-0-355>355</a></span>
<span class=normal><a href=#__codelineno-0-356>356</a></span>
<span class=normal><a href=#__codelineno-0-357>357</a></span>
<span class=normal><a href=#__codelineno-0-358>358</a></span>
<span class=normal><a href=#__codelineno-0-359>359</a></span>
<span class=normal><a href=#__codelineno-0-360>360</a></span>
<span class=normal><a href=#__codelineno-0-361>361</a></span>
<span class=normal><a href=#__codelineno-0-362>362</a></span>
<span class=normal><a href=#__codelineno-0-363>363</a></span>
<span class=normal><a href=#__codelineno-0-364>364</a></span>
<span class=normal><a href=#__codelineno-0-365>365</a></span>
<span class=normal><a href=#__codelineno-0-366>366</a></span>
<span class=normal><a href=#__codelineno-0-367>367</a></span>
<span class=normal><a href=#__codelineno-0-368>368</a></span>
<span class=normal><a href=#__codelineno-0-369>369</a></span>
<span class=normal><a href=#__codelineno-0-370>370</a></span>
<span class=normal><a href=#__codelineno-0-371>371</a></span>
<span class=normal><a href=#__codelineno-0-372>372</a></span>
<span class=normal><a href=#__codelineno-0-373>373</a></span>
<span class=normal><a href=#__codelineno-0-374>374</a></span>
<span class=normal><a href=#__codelineno-0-375>375</a></span>
<span class=normal><a href=#__codelineno-0-376>376</a></span>
<span class=normal><a href=#__codelineno-0-377>377</a></span>
<span class=normal><a href=#__codelineno-0-378>378</a></span>
<span class=normal><a href=#__codelineno-0-379>379</a></span>
<span class=normal><a href=#__codelineno-0-380>380</a></span>
<span class=normal><a href=#__codelineno-0-381>381</a></span>
<span class=normal><a href=#__codelineno-0-382>382</a></span>
<span class=normal><a href=#__codelineno-0-383>383</a></span>
<span class=normal><a href=#__codelineno-0-384>384</a></span>
<span class=normal><a href=#__codelineno-0-385>385</a></span>
<span class=normal><a href=#__codelineno-0-386>386</a></span>
<span class=normal><a href=#__codelineno-0-387>387</a></span>
<span class=normal><a href=#__codelineno-0-388>388</a></span>
<span class=normal><a href=#__codelineno-0-389>389</a></span>
<span class=normal><a href=#__codelineno-0-390>390</a></span>
<span class=normal><a href=#__codelineno-0-391>391</a></span>
<span class=normal><a href=#__codelineno-0-392>392</a></span>
<span class=normal><a href=#__codelineno-0-393>393</a></span>
<span class=normal><a href=#__codelineno-0-394>394</a></span>
<span class=normal><a href=#__codelineno-0-395>395</a></span>
<span class=normal><a href=#__codelineno-0-396>396</a></span>
<span class=normal><a href=#__codelineno-0-397>397</a></span>
<span class=normal><a href=#__codelineno-0-398>398</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-354><a id=__codelineno-0-354 name=__codelineno-0-354></a><span class=nd>@contextlib</span><span class=o>.</span><span class=n>contextmanager</span>
</span><span id=__span-0-355><a id=__codelineno-0-355 name=__codelineno-0-355></a><span class=k>def</span><span class=w> </span><span class=nf>no_gradient_sync</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>models</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-356><a id=__codelineno-0-356 name=__codelineno-0-356></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-357><a id=__codelineno-0-357 name=__codelineno-0-357></a><span class=sd>    Disable gradient sync during accumulation steps</span>
</span><span id=__span-0-358><a id=__codelineno-0-358 name=__codelineno-0-358></a><span class=sd>    and mark the final backward for FSDP.</span>
</span><span id=__span-0-359><a id=__codelineno-0-359 name=__codelineno-0-359></a>
</span><span id=__span-0-360><a id=__codelineno-0-360 name=__codelineno-0-360></a><span class=sd>    Usage:</span>
</span><span id=__span-0-361><a id=__codelineno-0-361 name=__codelineno-0-361></a><span class=sd>        with self.no_gradient_sync(self.model):</span>
</span><span id=__span-0-362><a id=__codelineno-0-362 name=__codelineno-0-362></a><span class=sd>            loss.backward()</span>
</span><span id=__span-0-363><a id=__codelineno-0-363 name=__codelineno-0-363></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-364><a id=__codelineno-0-364 name=__codelineno-0-364></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>world_size</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_gradient_accumulation_steps</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-0-365><a id=__codelineno-0-365 name=__codelineno-0-365></a>        <span class=c1># If no gradient accumulation or in single process environment, don&#39;t sync gradients</span>
</span><span id=__span-0-366><a id=__codelineno-0-366 name=__codelineno-0-366></a>        <span class=k>yield</span>
</span><span id=__span-0-367><a id=__codelineno-0-367 name=__codelineno-0-367></a>        <span class=k>return</span>
</span><span id=__span-0-368><a id=__codelineno-0-368 name=__codelineno-0-368></a>
</span><span id=__span-0-369><a id=__codelineno-0-369 name=__codelineno-0-369></a>    <span class=k>assert</span> <span class=nb>all</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=p>(</span><span class=n>FSDPModule</span><span class=p>,</span> <span class=n>DDPModule</span><span class=p>))</span> <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>models</span><span class=p>),</span> <span class=p>(</span>
</span><span id=__span-0-370><a id=__codelineno-0-370 name=__codelineno-0-370></a>        <span class=sa>f</span><span class=s2>&quot;Expected all modules to be FSDPModule or DDPModule, got </span><span class=si>{</span><span class=p>[</span><span class=nb>type</span><span class=p>(</span><span class=n>model</span><span class=p>)</span><span class=o>.</span><span class=vm>__name__</span><span class=w> </span><span class=k>for</span><span class=w> </span><span class=n>model</span><span class=w> </span><span class=ow>in</span><span class=w> </span><span class=n>models</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-0-371><a id=__codelineno-0-371 name=__codelineno-0-371></a>    <span class=p>)</span>
</span><span id=__span-0-372><a id=__codelineno-0-372 name=__codelineno-0-372></a>    <span class=n>distributed_modules</span> <span class=o>=</span> <span class=n>cast</span><span class=p>(</span><span class=nb>tuple</span><span class=p>[</span><span class=n>FSDPModule</span> <span class=o>|</span> <span class=n>DDPModule</span><span class=p>,</span> <span class=o>...</span><span class=p>],</span> <span class=n>models</span><span class=p>)</span>
</span><span id=__span-0-373><a id=__codelineno-0-373 name=__codelineno-0-373></a>
</span><span id=__span-0-374><a id=__codelineno-0-374 name=__codelineno-0-374></a>    <span class=n>current_accumulation_step</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-375><a id=__codelineno-0-375 name=__codelineno-0-375></a>        <span class=bp>self</span><span class=o>.</span><span class=n>local_batches</span> <span class=o>+</span> <span class=mi>1</span>
</span><span id=__span-0-376><a id=__codelineno-0-376 name=__codelineno-0-376></a>    <span class=p>)</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_gradient_accumulation_steps</span>
</span><span id=__span-0-377><a id=__codelineno-0-377 name=__codelineno-0-377></a>
</span><span id=__span-0-378><a id=__codelineno-0-378 name=__codelineno-0-378></a>    <span class=c1># Only update flags when transitioning between states</span>
</span><span id=__span-0-379><a id=__codelineno-0-379 name=__codelineno-0-379></a>    <span class=n>is_first_accumulation_step</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-380><a id=__codelineno-0-380 name=__codelineno-0-380></a>        <span class=n>current_accumulation_step</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_last_training_batch</span>
</span><span id=__span-0-381><a id=__codelineno-0-381 name=__codelineno-0-381></a>    <span class=p>)</span>
</span><span id=__span-0-382><a id=__codelineno-0-382 name=__codelineno-0-382></a>    <span class=n>is_last_accumulation_step</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-383><a id=__codelineno-0-383 name=__codelineno-0-383></a>        <span class=n>current_accumulation_step</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_last_training_batch</span>
</span><span id=__span-0-384><a id=__codelineno-0-384 name=__codelineno-0-384></a>    <span class=p>)</span>
</span><span id=__span-0-385><a id=__codelineno-0-385 name=__codelineno-0-385></a>
</span><span id=__span-0-386><a id=__codelineno-0-386 name=__codelineno-0-386></a>    <span class=k>if</span> <span class=n>is_first_accumulation_step</span><span class=p>:</span>
</span><span id=__span-0-387><a id=__codelineno-0-387 name=__codelineno-0-387></a>        <span class=c1># Set requires_gradient_sync to False only on first accumulation step (unless last batch)</span>
</span><span id=__span-0-388><a id=__codelineno-0-388 name=__codelineno-0-388></a>        <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>distributed_modules</span><span class=p>:</span>
</span><span id=__span-0-389><a id=__codelineno-0-389 name=__codelineno-0-389></a>            <span class=n>model</span><span class=o>.</span><span class=n>set_requires_gradient_sync</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-0-390><a id=__codelineno-0-390 name=__codelineno-0-390></a>
</span><span id=__span-0-391><a id=__codelineno-0-391 name=__codelineno-0-391></a>    <span class=c1># Set is_last_backward to True on second-to-last step OR if it&#39;s the last training batch</span>
</span><span id=__span-0-392><a id=__codelineno-0-392 name=__codelineno-0-392></a>    <span class=k>if</span> <span class=n>is_last_accumulation_step</span><span class=p>:</span>
</span><span id=__span-0-393><a id=__codelineno-0-393 name=__codelineno-0-393></a>        <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>distributed_modules</span><span class=p>:</span>
</span><span id=__span-0-394><a id=__codelineno-0-394 name=__codelineno-0-394></a>            <span class=n>model</span><span class=o>.</span><span class=n>set_requires_gradient_sync</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-0-395><a id=__codelineno-0-395 name=__codelineno-0-395></a>            <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>FSDPModule</span><span class=p>):</span>
</span><span id=__span-0-396><a id=__codelineno-0-396 name=__codelineno-0-396></a>                <span class=n>model</span><span class=o>.</span><span class=n>set_is_last_backward</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-0-397><a id=__codelineno-0-397 name=__codelineno-0-397></a>
</span><span id=__span-0-398><a id=__codelineno-0-398 name=__codelineno-0-398></a>    <span class=k>yield</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.total_gradient_norm class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">total_gradient_norm</span> <a href=#dream_trainer.trainer.BaseTrainer.total_gradient_norm class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>total_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>error_if_nonfinite</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-400>400</a></span>
<span class=normal><a href=#__codelineno-0-401>401</a></span>
<span class=normal><a href=#__codelineno-0-402>402</a></span>
<span class=normal><a href=#__codelineno-0-403>403</a></span>
<span class=normal><a href=#__codelineno-0-404>404</a></span>
<span class=normal><a href=#__codelineno-0-405>405</a></span>
<span class=normal><a href=#__codelineno-0-406>406</a></span>
<span class=normal><a href=#__codelineno-0-407>407</a></span>
<span class=normal><a href=#__codelineno-0-408>408</a></span>
<span class=normal><a href=#__codelineno-0-409>409</a></span>
<span class=normal><a href=#__codelineno-0-410>410</a></span>
<span class=normal><a href=#__codelineno-0-411>411</a></span>
<span class=normal><a href=#__codelineno-0-412>412</a></span>
<span class=normal><a href=#__codelineno-0-413>413</a></span>
<span class=normal><a href=#__codelineno-0-414>414</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-400><a id=__codelineno-0-400 name=__codelineno-0-400></a><span class=nd>@torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>()</span>
</span><span id=__span-0-401><a id=__codelineno-0-401 name=__codelineno-0-401></a><span class=k>def</span><span class=w> </span><span class=nf>total_gradient_norm</span><span class=p>(</span>
</span><span id=__span-0-402><a id=__codelineno-0-402 name=__codelineno-0-402></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-403><a id=__codelineno-0-403 name=__codelineno-0-403></a>    <span class=n>parameters</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span>
</span><span id=__span-0-404><a id=__codelineno-0-404 name=__codelineno-0-404></a>    <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-0-405><a id=__codelineno-0-405 name=__codelineno-0-405></a>    <span class=n>error_if_nonfinite</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-406><a id=__codelineno-0-406 name=__codelineno-0-406></a>    <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-407><a id=__codelineno-0-407 name=__codelineno-0-407></a><span class=p>):</span>
</span><span id=__span-0-408><a id=__codelineno-0-408 name=__codelineno-0-408></a>    <span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>param</span> <span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>parameters</span> <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>]</span>
</span><span id=__span-0-409><a id=__codelineno-0-409 name=__codelineno-0-409></a>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>get_total_norm</span><span class=p>(</span>
</span><span id=__span-0-410><a id=__codelineno-0-410 name=__codelineno-0-410></a>        <span class=n>parameters</span><span class=o>=</span><span class=n>grads</span><span class=p>,</span>
</span><span id=__span-0-411><a id=__codelineno-0-411 name=__codelineno-0-411></a>        <span class=n>norm_type</span><span class=o>=</span><span class=n>p</span><span class=p>,</span>
</span><span id=__span-0-412><a id=__codelineno-0-412 name=__codelineno-0-412></a>        <span class=n>error_if_nonfinite</span><span class=o>=</span><span class=n>error_if_nonfinite</span><span class=p>,</span>
</span><span id=__span-0-413><a id=__codelineno-0-413 name=__codelineno-0-413></a>        <span class=n>foreach</span><span class=o>=</span><span class=n>foreach</span><span class=p>,</span>
</span><span id=__span-0-414><a id=__codelineno-0-414 name=__codelineno-0-414></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.clip_gradient_norm class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">clip_gradient_norm</span> <a href=#dream_trainer.trainer.BaseTrainer.clip_gradient_norm class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>clip_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>total_norm</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span>
</span></code></pre></div> <div class="doc doc-contents "> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-416>416</a></span>
<span class=normal><a href=#__codelineno-0-417>417</a></span>
<span class=normal><a href=#__codelineno-0-418>418</a></span>
<span class=normal><a href=#__codelineno-0-419>419</a></span>
<span class=normal><a href=#__codelineno-0-420>420</a></span>
<span class=normal><a href=#__codelineno-0-421>421</a></span>
<span class=normal><a href=#__codelineno-0-422>422</a></span>
<span class=normal><a href=#__codelineno-0-423>423</a></span>
<span class=normal><a href=#__codelineno-0-424>424</a></span>
<span class=normal><a href=#__codelineno-0-425>425</a></span>
<span class=normal><a href=#__codelineno-0-426>426</a></span>
<span class=normal><a href=#__codelineno-0-427>427</a></span>
<span class=normal><a href=#__codelineno-0-428>428</a></span>
<span class=normal><a href=#__codelineno-0-429>429</a></span>
<span class=normal><a href=#__codelineno-0-430>430</a></span>
<span class=normal><a href=#__codelineno-0-431>431</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-416><a id=__codelineno-0-416 name=__codelineno-0-416></a><span class=nd>@torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>()</span>
</span><span id=__span-0-417><a id=__codelineno-0-417 name=__codelineno-0-417></a><span class=k>def</span><span class=w> </span><span class=nf>clip_gradient_norm</span><span class=p>(</span>
</span><span id=__span-0-418><a id=__codelineno-0-418 name=__codelineno-0-418></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-419><a id=__codelineno-0-419 name=__codelineno-0-419></a>    <span class=n>parameters</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span>
</span><span id=__span-0-420><a id=__codelineno-0-420 name=__codelineno-0-420></a>    <span class=n>total_norm</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-421><a id=__codelineno-0-421 name=__codelineno-0-421></a>    <span class=n>foreach</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-422><a id=__codelineno-0-422 name=__codelineno-0-422></a><span class=p>):</span>
</span><span id=__span-0-423><a id=__codelineno-0-423 name=__codelineno-0-423></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_parameters</span><span class=o>.</span><span class=n>gradient_clip_val</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-424><a id=__codelineno-0-424 name=__codelineno-0-424></a>        <span class=k>return</span>
</span><span id=__span-0-425><a id=__codelineno-0-425 name=__codelineno-0-425></a>
</span><span id=__span-0-426><a id=__codelineno-0-426 name=__codelineno-0-426></a>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grads_with_norm_</span><span class=p>(</span>
</span><span id=__span-0-427><a id=__codelineno-0-427 name=__codelineno-0-427></a>        <span class=n>parameters</span><span class=o>=</span><span class=n>parameters</span><span class=p>,</span>
</span><span id=__span-0-428><a id=__codelineno-0-428 name=__codelineno-0-428></a>        <span class=n>max_norm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training_parameters</span><span class=o>.</span><span class=n>gradient_clip_val</span><span class=p>,</span>
</span><span id=__span-0-429><a id=__codelineno-0-429 name=__codelineno-0-429></a>        <span class=n>total_norm</span><span class=o>=</span><span class=n>total_norm</span><span class=p>,</span>
</span><span id=__span-0-430><a id=__codelineno-0-430 name=__codelineno-0-430></a>        <span class=n>foreach</span><span class=o>=</span><span class=n>foreach</span><span class=p>,</span>
</span><span id=__span-0-431><a id=__codelineno-0-431 name=__codelineno-0-431></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.train_context class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">train_context</span> <a href=#dream_trainer.trainer.BaseTrainer.train_context class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>train_context</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Create a stacked context manager for training.</p> <p>This method combines the world's training context with any additional contexts provided by callbacks into a single stacked context manager.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>RETURNS</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> </td> <td class=doc-returns-details> <div class=doc-md-description> <p>contextlib.ExitStack: A stacked context manager that applies all training-related contexts.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-454>454</a></span>
<span class=normal><a href=#__codelineno-0-455>455</a></span>
<span class=normal><a href=#__codelineno-0-456>456</a></span>
<span class=normal><a href=#__codelineno-0-457>457</a></span>
<span class=normal><a href=#__codelineno-0-458>458</a></span>
<span class=normal><a href=#__codelineno-0-459>459</a></span>
<span class=normal><a href=#__codelineno-0-460>460</a></span>
<span class=normal><a href=#__codelineno-0-461>461</a></span>
<span class=normal><a href=#__codelineno-0-462>462</a></span>
<span class=normal><a href=#__codelineno-0-463>463</a></span>
<span class=normal><a href=#__codelineno-0-464>464</a></span>
<span class=normal><a href=#__codelineno-0-465>465</a></span>
<span class=normal><a href=#__codelineno-0-466>466</a></span>
<span class=normal><a href=#__codelineno-0-467>467</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-454><a id=__codelineno-0-454 name=__codelineno-0-454></a><span class=k>def</span><span class=w> </span><span class=nf>train_context</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-455><a id=__codelineno-0-455 name=__codelineno-0-455></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-456><a id=__codelineno-0-456 name=__codelineno-0-456></a><span class=sd>    Create a stacked context manager for training.</span>
</span><span id=__span-0-457><a id=__codelineno-0-457 name=__codelineno-0-457></a>
</span><span id=__span-0-458><a id=__codelineno-0-458 name=__codelineno-0-458></a><span class=sd>    This method combines the world&#39;s training context with any additional</span>
</span><span id=__span-0-459><a id=__codelineno-0-459 name=__codelineno-0-459></a><span class=sd>    contexts provided by callbacks into a single stacked context manager.</span>
</span><span id=__span-0-460><a id=__codelineno-0-460 name=__codelineno-0-460></a>
</span><span id=__span-0-461><a id=__codelineno-0-461 name=__codelineno-0-461></a><span class=sd>    Returns:</span>
</span><span id=__span-0-462><a id=__codelineno-0-462 name=__codelineno-0-462></a><span class=sd>        contextlib.ExitStack: A stacked context manager that applies all</span>
</span><span id=__span-0-463><a id=__codelineno-0-463 name=__codelineno-0-463></a><span class=sd>            training-related contexts.</span>
</span><span id=__span-0-464><a id=__codelineno-0-464 name=__codelineno-0-464></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-465><a id=__codelineno-0-465 name=__codelineno-0-465></a>    <span class=k>return</span> <span class=n>stacked_context</span><span class=p>(</span>
</span><span id=__span-0-466><a id=__codelineno-0-466 name=__codelineno-0-466></a>        <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>train_context</span><span class=p>()]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>train_context</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-0-467><a id=__codelineno-0-467 name=__codelineno-0-467></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.perform_training_epoch class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">perform_training_epoch</span> <a href=#dream_trainer.trainer.BaseTrainer.perform_training_epoch class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>perform_training_epoch</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Execute a complete training epoch.</p> <p>This method: 1. Sets the trainer to training mode 2. Iterates through the training dataloader 3. Executes training steps with gradient accumulation 4. Manages callbacks before/after each step 5. Performs validation at specified intervals 6. Updates process group timeouts after the first step</p> <p>The method handles gradient accumulation by only incrementing the global step when gradients are applied (not during accumulation).</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>RAISES</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-raises-annotation> <code><span title=RuntimeError>RuntimeError</span></code> </span> </td> <td class=doc-raises-details> <div class=doc-md-description> <p>If fewer batches are received than expected, which may indicate data loading issues in distributed training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-469>469</a></span>
<span class=normal><a href=#__codelineno-0-470>470</a></span>
<span class=normal><a href=#__codelineno-0-471>471</a></span>
<span class=normal><a href=#__codelineno-0-472>472</a></span>
<span class=normal><a href=#__codelineno-0-473>473</a></span>
<span class=normal><a href=#__codelineno-0-474>474</a></span>
<span class=normal><a href=#__codelineno-0-475>475</a></span>
<span class=normal><a href=#__codelineno-0-476>476</a></span>
<span class=normal><a href=#__codelineno-0-477>477</a></span>
<span class=normal><a href=#__codelineno-0-478>478</a></span>
<span class=normal><a href=#__codelineno-0-479>479</a></span>
<span class=normal><a href=#__codelineno-0-480>480</a></span>
<span class=normal><a href=#__codelineno-0-481>481</a></span>
<span class=normal><a href=#__codelineno-0-482>482</a></span>
<span class=normal><a href=#__codelineno-0-483>483</a></span>
<span class=normal><a href=#__codelineno-0-484>484</a></span>
<span class=normal><a href=#__codelineno-0-485>485</a></span>
<span class=normal><a href=#__codelineno-0-486>486</a></span>
<span class=normal><a href=#__codelineno-0-487>487</a></span>
<span class=normal><a href=#__codelineno-0-488>488</a></span>
<span class=normal><a href=#__codelineno-0-489>489</a></span>
<span class=normal><a href=#__codelineno-0-490>490</a></span>
<span class=normal><a href=#__codelineno-0-491>491</a></span>
<span class=normal><a href=#__codelineno-0-492>492</a></span>
<span class=normal><a href=#__codelineno-0-493>493</a></span>
<span class=normal><a href=#__codelineno-0-494>494</a></span>
<span class=normal><a href=#__codelineno-0-495>495</a></span>
<span class=normal><a href=#__codelineno-0-496>496</a></span>
<span class=normal><a href=#__codelineno-0-497>497</a></span>
<span class=normal><a href=#__codelineno-0-498>498</a></span>
<span class=normal><a href=#__codelineno-0-499>499</a></span>
<span class=normal><a href=#__codelineno-0-500>500</a></span>
<span class=normal><a href=#__codelineno-0-501>501</a></span>
<span class=normal><a href=#__codelineno-0-502>502</a></span>
<span class=normal><a href=#__codelineno-0-503>503</a></span>
<span class=normal><a href=#__codelineno-0-504>504</a></span>
<span class=normal><a href=#__codelineno-0-505>505</a></span>
<span class=normal><a href=#__codelineno-0-506>506</a></span>
<span class=normal><a href=#__codelineno-0-507>507</a></span>
<span class=normal><a href=#__codelineno-0-508>508</a></span>
<span class=normal><a href=#__codelineno-0-509>509</a></span>
<span class=normal><a href=#__codelineno-0-510>510</a></span>
<span class=normal><a href=#__codelineno-0-511>511</a></span>
<span class=normal><a href=#__codelineno-0-512>512</a></span>
<span class=normal><a href=#__codelineno-0-513>513</a></span>
<span class=normal><a href=#__codelineno-0-514>514</a></span>
<span class=normal><a href=#__codelineno-0-515>515</a></span>
<span class=normal><a href=#__codelineno-0-516>516</a></span>
<span class=normal><a href=#__codelineno-0-517>517</a></span>
<span class=normal><a href=#__codelineno-0-518>518</a></span>
<span class=normal><a href=#__codelineno-0-519>519</a></span>
<span class=normal><a href=#__codelineno-0-520>520</a></span>
<span class=normal><a href=#__codelineno-0-521>521</a></span>
<span class=normal><a href=#__codelineno-0-522>522</a></span>
<span class=normal><a href=#__codelineno-0-523>523</a></span>
<span class=normal><a href=#__codelineno-0-524>524</a></span>
<span class=normal><a href=#__codelineno-0-525>525</a></span>
<span class=normal><a href=#__codelineno-0-526>526</a></span>
<span class=normal><a href=#__codelineno-0-527>527</a></span>
<span class=normal><a href=#__codelineno-0-528>528</a></span>
<span class=normal><a href=#__codelineno-0-529>529</a></span>
<span class=normal><a href=#__codelineno-0-530>530</a></span>
<span class=normal><a href=#__codelineno-0-531>531</a></span>
<span class=normal><a href=#__codelineno-0-532>532</a></span>
<span class=normal><a href=#__codelineno-0-533>533</a></span>
<span class=normal><a href=#__codelineno-0-534>534</a></span>
<span class=normal><a href=#__codelineno-0-535>535</a></span>
<span class=normal><a href=#__codelineno-0-536>536</a></span>
<span class=normal><a href=#__codelineno-0-537>537</a></span>
<span class=normal><a href=#__codelineno-0-538>538</a></span>
<span class=normal><a href=#__codelineno-0-539>539</a></span>
<span class=normal><a href=#__codelineno-0-540>540</a></span>
<span class=normal><a href=#__codelineno-0-541>541</a></span>
<span class=normal><a href=#__codelineno-0-542>542</a></span>
<span class=normal><a href=#__codelineno-0-543>543</a></span>
<span class=normal><a href=#__codelineno-0-544>544</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-469><a id=__codelineno-0-469 name=__codelineno-0-469></a><span class=k>def</span><span class=w> </span><span class=nf>perform_training_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-470><a id=__codelineno-0-470 name=__codelineno-0-470></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-471><a id=__codelineno-0-471 name=__codelineno-0-471></a><span class=sd>    Execute a complete training epoch.</span>
</span><span id=__span-0-472><a id=__codelineno-0-472 name=__codelineno-0-472></a>
</span><span id=__span-0-473><a id=__codelineno-0-473 name=__codelineno-0-473></a><span class=sd>    This method:</span>
</span><span id=__span-0-474><a id=__codelineno-0-474 name=__codelineno-0-474></a><span class=sd>    1. Sets the trainer to training mode</span>
</span><span id=__span-0-475><a id=__codelineno-0-475 name=__codelineno-0-475></a><span class=sd>    2. Iterates through the training dataloader</span>
</span><span id=__span-0-476><a id=__codelineno-0-476 name=__codelineno-0-476></a><span class=sd>    3. Executes training steps with gradient accumulation</span>
</span><span id=__span-0-477><a id=__codelineno-0-477 name=__codelineno-0-477></a><span class=sd>    4. Manages callbacks before/after each step</span>
</span><span id=__span-0-478><a id=__codelineno-0-478 name=__codelineno-0-478></a><span class=sd>    5. Performs validation at specified intervals</span>
</span><span id=__span-0-479><a id=__codelineno-0-479 name=__codelineno-0-479></a><span class=sd>    6. Updates process group timeouts after the first step</span>
</span><span id=__span-0-480><a id=__codelineno-0-480 name=__codelineno-0-480></a>
</span><span id=__span-0-481><a id=__codelineno-0-481 name=__codelineno-0-481></a><span class=sd>    The method handles gradient accumulation by only incrementing the</span>
</span><span id=__span-0-482><a id=__codelineno-0-482 name=__codelineno-0-482></a><span class=sd>    global step when gradients are applied (not during accumulation).</span>
</span><span id=__span-0-483><a id=__codelineno-0-483 name=__codelineno-0-483></a>
</span><span id=__span-0-484><a id=__codelineno-0-484 name=__codelineno-0-484></a><span class=sd>    Raises:</span>
</span><span id=__span-0-485><a id=__codelineno-0-485 name=__codelineno-0-485></a><span class=sd>        RuntimeError: If fewer batches are received than expected, which</span>
</span><span id=__span-0-486><a id=__codelineno-0-486 name=__codelineno-0-486></a><span class=sd>            may indicate data loading issues in distributed training.</span>
</span><span id=__span-0-487><a id=__codelineno-0-487 name=__codelineno-0-487></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-488><a id=__codelineno-0-488 name=__codelineno-0-488></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span> <span class=o>&lt;=</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-489><a id=__codelineno-0-489 name=__codelineno-0-489></a>        <span class=k>return</span>
</span><span id=__span-0-490><a id=__codelineno-0-490 name=__codelineno-0-490></a>
</span><span id=__span-0-491><a id=__codelineno-0-491 name=__codelineno-0-491></a>    <span class=bp>self</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-0-492><a id=__codelineno-0-492 name=__codelineno-0-492></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-0-493><a id=__codelineno-0-493 name=__codelineno-0-493></a>
</span><span id=__span-0-494><a id=__codelineno-0-494 name=__codelineno-0-494></a>    <span class=n>batch_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-495><a id=__codelineno-0-495 name=__codelineno-0-495></a>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_dataloader</span><span class=p>:</span>
</span><span id=__span-0-496><a id=__codelineno-0-496 name=__codelineno-0-496></a>        <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span><span class=p>:</span>
</span><span id=__span-0-497><a id=__codelineno-0-497 name=__codelineno-0-497></a>            <span class=k>break</span>
</span><span id=__span-0-498><a id=__codelineno-0-498 name=__codelineno-0-498></a>
</span><span id=__span-0-499><a id=__codelineno-0-499 name=__codelineno-0-499></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_last_training_batch</span> <span class=o>=</span> <span class=n>batch_idx</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span> <span class=o>-</span> <span class=mi>1</span>
</span><span id=__span-0-500><a id=__codelineno-0-500 name=__codelineno-0-500></a>
</span><span id=__span-0-501><a id=__codelineno-0-501 name=__codelineno-0-501></a>        <span class=c1># Move batch to device, non-blocking</span>
</span><span id=__span-0-502><a id=__codelineno-0-502 name=__codelineno-0-502></a>        <span class=n>batch</span> <span class=o>=</span> <span class=n>apply_to_collection</span><span class=p>(</span>
</span><span id=__span-0-503><a id=__codelineno-0-503 name=__codelineno-0-503></a>            <span class=n>cast</span><span class=p>(</span><span class=n>Batch</span><span class=p>,</span> <span class=n>batch</span><span class=p>),</span>
</span><span id=__span-0-504><a id=__codelineno-0-504 name=__codelineno-0-504></a>            <span class=n>function</span><span class=o>=</span><span class=k>lambda</span> <span class=n>t</span><span class=p>:</span> <span class=n>t</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span><span id=__span-0-505><a id=__codelineno-0-505 name=__codelineno-0-505></a>            <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-506><a id=__codelineno-0-506 name=__codelineno-0-506></a>        <span class=p>)</span>
</span><span id=__span-0-507><a id=__codelineno-0-507 name=__codelineno-0-507></a>
</span><span id=__span-0-508><a id=__codelineno-0-508 name=__codelineno-0-508></a>        <span class=c1># Train Step</span>
</span><span id=__span-0-509><a id=__codelineno-0-509 name=__codelineno-0-509></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-510><a id=__codelineno-0-510 name=__codelineno-0-510></a>        <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_context</span><span class=p>():</span>
</span><span id=__span-0-511><a id=__codelineno-0-511 name=__codelineno-0-511></a>            <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_step</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-512><a id=__codelineno-0-512 name=__codelineno-0-512></a>
</span><span id=__span-0-513><a id=__codelineno-0-513 name=__codelineno-0-513></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-514><a id=__codelineno-0-514 name=__codelineno-0-514></a>        <span class=bp>self</span><span class=o>.</span><span class=n>local_batches</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-0-515><a id=__codelineno-0-515 name=__codelineno-0-515></a>
</span><span id=__span-0-516><a id=__codelineno-0-516 name=__codelineno-0-516></a>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-0-517><a id=__codelineno-0-517 name=__codelineno-0-517></a>            <span class=bp>self</span><span class=o>.</span><span class=n>_local_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-0-518><a id=__codelineno-0-518 name=__codelineno-0-518></a>            <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-0-519><a id=__codelineno-0-519 name=__codelineno-0-519></a>        <span class=n>batch_idx</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-0-520><a id=__codelineno-0-520 name=__codelineno-0-520></a>
</span><span id=__span-0-521><a id=__codelineno-0-521 name=__codelineno-0-521></a>        <span class=c1># Reduce timeout after first train step for faster signal</span>
</span><span id=__span-0-522><a id=__codelineno-0-522 name=__codelineno-0-522></a>        <span class=c1># (assuming lazy init and compilation are finished)</span>
</span><span id=__span-0-523><a id=__codelineno-0-523 name=__codelineno-0-523></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_local_step</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-0-524><a id=__codelineno-0-524 name=__codelineno-0-524></a>            <span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>set_pg_timeouts</span><span class=p>(</span>
</span><span id=__span-0-525><a id=__codelineno-0-525 name=__codelineno-0-525></a>                <span class=n>timeout</span><span class=o>=</span><span class=n>dt</span><span class=o>.</span><span class=n>timedelta</span><span class=p>(</span>
</span><span id=__span-0-526><a id=__codelineno-0-526 name=__codelineno-0-526></a>                    <span class=n>seconds</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_parameters</span><span class=o>.</span><span class=n>comm</span><span class=o>.</span><span class=n>train_timeout_seconds</span><span class=p>,</span>
</span><span id=__span-0-527><a id=__codelineno-0-527 name=__codelineno-0-527></a>                <span class=p>),</span>
</span><span id=__span-0-528><a id=__codelineno-0-528 name=__codelineno-0-528></a>            <span class=p>)</span>
</span><span id=__span-0-529><a id=__codelineno-0-529 name=__codelineno-0-529></a>
</span><span id=__span-0-530><a id=__codelineno-0-530 name=__codelineno-0-530></a>        <span class=c1># Validation Epoch</span>
</span><span id=__span-0-531><a id=__codelineno-0-531 name=__codelineno-0-531></a>        <span class=k>if</span> <span class=p>(</span>
</span><span id=__span-0-532><a id=__codelineno-0-532 name=__codelineno-0-532></a>            <span class=bp>self</span><span class=o>.</span><span class=n>global_step</span>
</span><span id=__span-0-533><a id=__codelineno-0-533 name=__codelineno-0-533></a>            <span class=o>%</span> <span class=nb>int</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_parameters</span><span class=o>.</span><span class=n>val_frequency</span><span class=p>)</span>
</span><span id=__span-0-534><a id=__codelineno-0-534 name=__codelineno-0-534></a>        <span class=p>)</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-0-535><a id=__codelineno-0-535 name=__codelineno-0-535></a>            <span class=bp>self</span><span class=o>.</span><span class=n>perform_validation_epoch</span><span class=p>()</span>
</span><span id=__span-0-536><a id=__codelineno-0-536 name=__codelineno-0-536></a>            <span class=bp>self</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span><span id=__span-0-537><a id=__codelineno-0-537 name=__codelineno-0-537></a>
</span><span id=__span-0-538><a id=__codelineno-0-538 name=__codelineno-0-538></a>    <span class=k>if</span> <span class=p>(</span><span class=n>batch_idx</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span><span class=p>:</span>
</span><span id=__span-0-539><a id=__codelineno-0-539 name=__codelineno-0-539></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
</span><span id=__span-0-540><a id=__codelineno-0-540 name=__codelineno-0-540></a>            <span class=sa>f</span><span class=s2>&quot;Worker </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>world_mesh</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span><span class=w> </span><span class=k>if</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>world_mesh</span><span class=w> </span><span class=ow>is</span><span class=w> </span><span class=ow>not</span><span class=w> </span><span class=kc>None</span><span class=w> </span><span class=k>else</span><span class=w> </span><span class=s1>&#39;unknown&#39;</span><span class=si>}</span><span class=s2> received fewer training batches than expected. &quot;</span>
</span><span id=__span-0-541><a id=__codelineno-0-541 name=__codelineno-0-541></a>            <span class=sa>f</span><span class=s2>&quot;Expected </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_num_train_steps</span><span class=si>}</span><span class=s2> batches, received </span><span class=si>{</span><span class=n>batch_idx</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=mi>1</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-0-542><a id=__codelineno-0-542 name=__codelineno-0-542></a>        <span class=p>)</span>
</span><span id=__span-0-543><a id=__codelineno-0-543 name=__codelineno-0-543></a>
</span><span id=__span-0-544><a id=__codelineno-0-544 name=__codelineno-0-544></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.perform_validation_epoch class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">perform_validation_epoch</span> <a href=#dream_trainer.trainer.BaseTrainer.perform_validation_epoch class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>perform_validation_epoch</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Execute a complete validation epoch.</p> <p>This method: 1. Sets the trainer to evaluation mode 2. Disables gradient computation with <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/torch title="GitHub User: torch">@torch</a>.no_grad() 3. Iterates through the validation dataloader 4. Executes validation steps 5. Manages callbacks before/after each step and epoch</p> <p>All operations are performed without gradient computation for efficiency.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>RAISES</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <span class=doc-raises-annotation> <code><span title=RuntimeError>RuntimeError</span></code> </span> </td> <td class=doc-raises-details> <div class=doc-md-description> <p>If fewer batches are received than expected, which may indicate data loading issues in distributed training.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-546>546</a></span>
<span class=normal><a href=#__codelineno-0-547>547</a></span>
<span class=normal><a href=#__codelineno-0-548>548</a></span>
<span class=normal><a href=#__codelineno-0-549>549</a></span>
<span class=normal><a href=#__codelineno-0-550>550</a></span>
<span class=normal><a href=#__codelineno-0-551>551</a></span>
<span class=normal><a href=#__codelineno-0-552>552</a></span>
<span class=normal><a href=#__codelineno-0-553>553</a></span>
<span class=normal><a href=#__codelineno-0-554>554</a></span>
<span class=normal><a href=#__codelineno-0-555>555</a></span>
<span class=normal><a href=#__codelineno-0-556>556</a></span>
<span class=normal><a href=#__codelineno-0-557>557</a></span>
<span class=normal><a href=#__codelineno-0-558>558</a></span>
<span class=normal><a href=#__codelineno-0-559>559</a></span>
<span class=normal><a href=#__codelineno-0-560>560</a></span>
<span class=normal><a href=#__codelineno-0-561>561</a></span>
<span class=normal><a href=#__codelineno-0-562>562</a></span>
<span class=normal><a href=#__codelineno-0-563>563</a></span>
<span class=normal><a href=#__codelineno-0-564>564</a></span>
<span class=normal><a href=#__codelineno-0-565>565</a></span>
<span class=normal><a href=#__codelineno-0-566>566</a></span>
<span class=normal><a href=#__codelineno-0-567>567</a></span>
<span class=normal><a href=#__codelineno-0-568>568</a></span>
<span class=normal><a href=#__codelineno-0-569>569</a></span>
<span class=normal><a href=#__codelineno-0-570>570</a></span>
<span class=normal><a href=#__codelineno-0-571>571</a></span>
<span class=normal><a href=#__codelineno-0-572>572</a></span>
<span class=normal><a href=#__codelineno-0-573>573</a></span>
<span class=normal><a href=#__codelineno-0-574>574</a></span>
<span class=normal><a href=#__codelineno-0-575>575</a></span>
<span class=normal><a href=#__codelineno-0-576>576</a></span>
<span class=normal><a href=#__codelineno-0-577>577</a></span>
<span class=normal><a href=#__codelineno-0-578>578</a></span>
<span class=normal><a href=#__codelineno-0-579>579</a></span>
<span class=normal><a href=#__codelineno-0-580>580</a></span>
<span class=normal><a href=#__codelineno-0-581>581</a></span>
<span class=normal><a href=#__codelineno-0-582>582</a></span>
<span class=normal><a href=#__codelineno-0-583>583</a></span>
<span class=normal><a href=#__codelineno-0-584>584</a></span>
<span class=normal><a href=#__codelineno-0-585>585</a></span>
<span class=normal><a href=#__codelineno-0-586>586</a></span>
<span class=normal><a href=#__codelineno-0-587>587</a></span>
<span class=normal><a href=#__codelineno-0-588>588</a></span>
<span class=normal><a href=#__codelineno-0-589>589</a></span>
<span class=normal><a href=#__codelineno-0-590>590</a></span>
<span class=normal><a href=#__codelineno-0-591>591</a></span>
<span class=normal><a href=#__codelineno-0-592>592</a></span>
<span class=normal><a href=#__codelineno-0-593>593</a></span>
<span class=normal><a href=#__codelineno-0-594>594</a></span>
<span class=normal><a href=#__codelineno-0-595>595</a></span>
<span class=normal><a href=#__codelineno-0-596>596</a></span>
<span class=normal><a href=#__codelineno-0-597>597</a></span>
<span class=normal><a href=#__codelineno-0-598>598</a></span>
<span class=normal><a href=#__codelineno-0-599>599</a></span>
<span class=normal><a href=#__codelineno-0-600>600</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-546><a id=__codelineno-0-546 name=__codelineno-0-546></a><span class=nd>@torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>()</span>
</span><span id=__span-0-547><a id=__codelineno-0-547 name=__codelineno-0-547></a><span class=k>def</span><span class=w> </span><span class=nf>perform_validation_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-548><a id=__codelineno-0-548 name=__codelineno-0-548></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-549><a id=__codelineno-0-549 name=__codelineno-0-549></a><span class=sd>    Execute a complete validation epoch.</span>
</span><span id=__span-0-550><a id=__codelineno-0-550 name=__codelineno-0-550></a>
</span><span id=__span-0-551><a id=__codelineno-0-551 name=__codelineno-0-551></a><span class=sd>    This method:</span>
</span><span id=__span-0-552><a id=__codelineno-0-552 name=__codelineno-0-552></a><span class=sd>    1. Sets the trainer to evaluation mode</span>
</span><span id=__span-0-553><a id=__codelineno-0-553 name=__codelineno-0-553></a><span class=sd>    2. Disables gradient computation with @torch.no_grad()</span>
</span><span id=__span-0-554><a id=__codelineno-0-554 name=__codelineno-0-554></a><span class=sd>    3. Iterates through the validation dataloader</span>
</span><span id=__span-0-555><a id=__codelineno-0-555 name=__codelineno-0-555></a><span class=sd>    4. Executes validation steps</span>
</span><span id=__span-0-556><a id=__codelineno-0-556 name=__codelineno-0-556></a><span class=sd>    5. Manages callbacks before/after each step and epoch</span>
</span><span id=__span-0-557><a id=__codelineno-0-557 name=__codelineno-0-557></a>
</span><span id=__span-0-558><a id=__codelineno-0-558 name=__codelineno-0-558></a><span class=sd>    All operations are performed without gradient computation for efficiency.</span>
</span><span id=__span-0-559><a id=__codelineno-0-559 name=__codelineno-0-559></a>
</span><span id=__span-0-560><a id=__codelineno-0-560 name=__codelineno-0-560></a><span class=sd>    Raises:</span>
</span><span id=__span-0-561><a id=__codelineno-0-561 name=__codelineno-0-561></a><span class=sd>        RuntimeError: If fewer batches are received than expected, which</span>
</span><span id=__span-0-562><a id=__codelineno-0-562 name=__codelineno-0-562></a><span class=sd>            may indicate data loading issues in distributed training.</span>
</span><span id=__span-0-563><a id=__codelineno-0-563 name=__codelineno-0-563></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-564><a id=__codelineno-0-564 name=__codelineno-0-564></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span> <span class=o>&lt;=</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-565><a id=__codelineno-0-565 name=__codelineno-0-565></a>        <span class=k>return</span>
</span><span id=__span-0-566><a id=__codelineno-0-566 name=__codelineno-0-566></a>
</span><span id=__span-0-567><a id=__codelineno-0-567 name=__codelineno-0-567></a>    <span class=bp>self</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span><span id=__span-0-568><a id=__codelineno-0-568 name=__codelineno-0-568></a>
</span><span id=__span-0-569><a id=__codelineno-0-569 name=__codelineno-0-569></a>    <span class=c1># Validation Epoch Start</span>
</span><span id=__span-0-570><a id=__codelineno-0-570 name=__codelineno-0-570></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_validation_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span><span id=__span-0-571><a id=__codelineno-0-571 name=__codelineno-0-571></a>
</span><span id=__span-0-572><a id=__codelineno-0-572 name=__codelineno-0-572></a>    <span class=c1># Validation Epoch Loop</span>
</span><span id=__span-0-573><a id=__codelineno-0-573 name=__codelineno-0-573></a>    <span class=n>batch_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-574><a id=__codelineno-0-574 name=__codelineno-0-574></a>    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>val_dataloader</span><span class=p>:</span>
</span><span id=__span-0-575><a id=__codelineno-0-575 name=__codelineno-0-575></a>        <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span><span class=p>:</span>
</span><span id=__span-0-576><a id=__codelineno-0-576 name=__codelineno-0-576></a>            <span class=k>break</span>
</span><span id=__span-0-577><a id=__codelineno-0-577 name=__codelineno-0-577></a>
</span><span id=__span-0-578><a id=__codelineno-0-578 name=__codelineno-0-578></a>        <span class=c1># Move batch to device, non-blocking</span>
</span><span id=__span-0-579><a id=__codelineno-0-579 name=__codelineno-0-579></a>        <span class=n>batch</span> <span class=o>=</span> <span class=n>apply_to_collection</span><span class=p>(</span>
</span><span id=__span-0-580><a id=__codelineno-0-580 name=__codelineno-0-580></a>            <span class=n>cast</span><span class=p>(</span><span class=n>Batch</span><span class=p>,</span> <span class=n>batch</span><span class=p>),</span>
</span><span id=__span-0-581><a id=__codelineno-0-581 name=__codelineno-0-581></a>            <span class=n>function</span><span class=o>=</span><span class=k>lambda</span> <span class=n>t</span><span class=p>:</span> <span class=n>t</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span><span id=__span-0-582><a id=__codelineno-0-582 name=__codelineno-0-582></a>            <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-583><a id=__codelineno-0-583 name=__codelineno-0-583></a>        <span class=p>)</span>
</span><span id=__span-0-584><a id=__codelineno-0-584 name=__codelineno-0-584></a>
</span><span id=__span-0-585><a id=__codelineno-0-585 name=__codelineno-0-585></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>pre_validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-586><a id=__codelineno-0-586 name=__codelineno-0-586></a>
</span><span id=__span-0-587><a id=__codelineno-0-587 name=__codelineno-0-587></a>        <span class=k>with</span> <span class=n>stacked_context</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>validation_context</span><span class=p>(</span><span class=bp>self</span><span class=p>)):</span>
</span><span id=__span-0-588><a id=__codelineno-0-588 name=__codelineno-0-588></a>            <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>validation_step</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-589><a id=__codelineno-0-589 name=__codelineno-0-589></a>
</span><span id=__span-0-590><a id=__codelineno-0-590 name=__codelineno-0-590></a>        <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>)</span>
</span><span id=__span-0-591><a id=__codelineno-0-591 name=__codelineno-0-591></a>        <span class=n>batch_idx</span> <span class=o>+=</span> <span class=mi>1</span>
</span><span id=__span-0-592><a id=__codelineno-0-592 name=__codelineno-0-592></a>
</span><span id=__span-0-593><a id=__codelineno-0-593 name=__codelineno-0-593></a>    <span class=k>if</span> <span class=p>(</span><span class=n>batch_idx</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span><span class=p>:</span>
</span><span id=__span-0-594><a id=__codelineno-0-594 name=__codelineno-0-594></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
</span><span id=__span-0-595><a id=__codelineno-0-595 name=__codelineno-0-595></a>            <span class=sa>f</span><span class=s2>&quot;Worker </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>world_mesh</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span><span class=w> </span><span class=k>if</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>world_mesh</span><span class=w> </span><span class=ow>is</span><span class=w> </span><span class=ow>not</span><span class=w> </span><span class=kc>None</span><span class=w> </span><span class=k>else</span><span class=w> </span><span class=s1>&#39;unknown&#39;</span><span class=si>}</span><span class=s2> received fewer validation batches than expected. &quot;</span>
</span><span id=__span-0-596><a id=__codelineno-0-596 name=__codelineno-0-596></a>            <span class=sa>f</span><span class=s2>&quot;Expected </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span><span class=si>}</span><span class=s2> batches, received </span><span class=si>{</span><span class=n>batch_idx</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=mi>1</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-0-597><a id=__codelineno-0-597 name=__codelineno-0-597></a>        <span class=p>)</span>
</span><span id=__span-0-598><a id=__codelineno-0-598 name=__codelineno-0-598></a>
</span><span id=__span-0-599><a id=__codelineno-0-599 name=__codelineno-0-599></a>    <span class=c1># Validation Epoch End</span>
</span><span id=__span-0-600><a id=__codelineno-0-600 name=__codelineno-0-600></a>    <span class=bp>self</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>post_validation_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h4 id=dream_trainer.trainer.BaseTrainer.perform_sanity_validation_steps class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <span class="doc doc-object-name doc-function-name">perform_sanity_validation_steps</span> <a href=#dream_trainer.trainer.BaseTrainer.perform_sanity_validation_steps class=headerlink title="Permanent link">&para;</a></h4> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>perform_sanity_validation_steps</span><span class=p>()</span>
</span></code></pre></div> <div class="doc doc-contents "> <p>Perform sanity validation steps before training begins.</p> <p>This method runs a limited number of validation steps at the start of training to ensure the validation pipeline is working correctly. It temporarily overrides the number of validation steps with the configured number of sanity validation steps.</p> <p>Sanity validation is only performed on the first epoch (epoch 0) and is skipped when resuming training from a checkpoint.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <details class=quote> <summary>Source code in <code>src/dream_trainer/trainer/base.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-602>602</a></span>
<span class=normal><a href=#__codelineno-0-603>603</a></span>
<span class=normal><a href=#__codelineno-0-604>604</a></span>
<span class=normal><a href=#__codelineno-0-605>605</a></span>
<span class=normal><a href=#__codelineno-0-606>606</a></span>
<span class=normal><a href=#__codelineno-0-607>607</a></span>
<span class=normal><a href=#__codelineno-0-608>608</a></span>
<span class=normal><a href=#__codelineno-0-609>609</a></span>
<span class=normal><a href=#__codelineno-0-610>610</a></span>
<span class=normal><a href=#__codelineno-0-611>611</a></span>
<span class=normal><a href=#__codelineno-0-612>612</a></span>
<span class=normal><a href=#__codelineno-0-613>613</a></span>
<span class=normal><a href=#__codelineno-0-614>614</a></span>
<span class=normal><a href=#__codelineno-0-615>615</a></span>
<span class=normal><a href=#__codelineno-0-616>616</a></span>
<span class=normal><a href=#__codelineno-0-617>617</a></span>
<span class=normal><a href=#__codelineno-0-618>618</a></span>
<span class=normal><a href=#__codelineno-0-619>619</a></span>
<span class=normal><a href=#__codelineno-0-620>620</a></span>
<span class=normal><a href=#__codelineno-0-621>621</a></span>
<span class=normal><a href=#__codelineno-0-622>622</a></span>
<span class=normal><a href=#__codelineno-0-623>623</a></span>
<span class=normal><a href=#__codelineno-0-624>624</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-602><a id=__codelineno-0-602 name=__codelineno-0-602></a><span class=k>def</span><span class=w> </span><span class=nf>perform_sanity_validation_steps</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-603><a id=__codelineno-0-603 name=__codelineno-0-603></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-604><a id=__codelineno-0-604 name=__codelineno-0-604></a><span class=sd>    Perform sanity validation steps before training begins.</span>
</span><span id=__span-0-605><a id=__codelineno-0-605 name=__codelineno-0-605></a>
</span><span id=__span-0-606><a id=__codelineno-0-606 name=__codelineno-0-606></a><span class=sd>    This method runs a limited number of validation steps at the start</span>
</span><span id=__span-0-607><a id=__codelineno-0-607 name=__codelineno-0-607></a><span class=sd>    of training to ensure the validation pipeline is working correctly.</span>
</span><span id=__span-0-608><a id=__codelineno-0-608 name=__codelineno-0-608></a><span class=sd>    It temporarily overrides the number of validation steps with the</span>
</span><span id=__span-0-609><a id=__codelineno-0-609 name=__codelineno-0-609></a><span class=sd>    configured number of sanity validation steps.</span>
</span><span id=__span-0-610><a id=__codelineno-0-610 name=__codelineno-0-610></a>
</span><span id=__span-0-611><a id=__codelineno-0-611 name=__codelineno-0-611></a><span class=sd>    Sanity validation is only performed on the first epoch (epoch 0)</span>
</span><span id=__span-0-612><a id=__codelineno-0-612 name=__codelineno-0-612></a><span class=sd>    and is skipped when resuming training from a checkpoint.</span>
</span><span id=__span-0-613><a id=__codelineno-0-613 name=__codelineno-0-613></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-614><a id=__codelineno-0-614 name=__codelineno-0-614></a>    <span class=c1># Don&#39;t perform sanity validation on resumption</span>
</span><span id=__span-0-615><a id=__codelineno-0-615 name=__codelineno-0-615></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_epoch</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-616><a id=__codelineno-0-616 name=__codelineno-0-616></a>        <span class=k>return</span>
</span><span id=__span-0-617><a id=__codelineno-0-617 name=__codelineno-0-617></a>
</span><span id=__span-0-618><a id=__codelineno-0-618 name=__codelineno-0-618></a>    <span class=c1># Store num val steps &amp; temporarily override to num sanity val steps</span>
</span><span id=__span-0-619><a id=__codelineno-0-619 name=__codelineno-0-619></a>    <span class=n>num_val_steps</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span>
</span><span id=__span-0-620><a id=__codelineno-0-620 name=__codelineno-0-620></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_num_sanity_val_steps</span>
</span><span id=__span-0-621><a id=__codelineno-0-621 name=__codelineno-0-621></a>
</span><span id=__span-0-622><a id=__codelineno-0-622 name=__codelineno-0-622></a>    <span class=c1># Call validation epoch normally &amp; restore num val steps</span>
</span><span id=__span-0-623><a id=__codelineno-0-623 name=__codelineno-0-623></a>    <span class=bp>self</span><span class=o>.</span><span class=n>perform_validation_epoch</span><span class=p>()</span>
</span><span id=__span-0-624><a id=__codelineno-0-624 name=__codelineno-0-624></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_num_val_steps</span> <span class=o>=</span> <span class=n>num_val_steps</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div><h2 id=configuration>Configuration<a class=headerlink href=#configuration title="Permanent link">&para;</a></h2> <div class="doc doc-object doc-class"> <h2 id=dream_trainer.trainer.BaseTrainerConfig class="doc doc-heading"> <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <span class="doc doc-object-name doc-class-name">BaseTrainerConfig</span> <span class="doc doc-labels"> <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small> </span> <a href=#dream_trainer.trainer.BaseTrainerConfig class=headerlink title="Permanent link">&para;</a></h2> <div class="language-python doc-signature highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nf>BaseTrainerConfig</span><span class=p>(</span><span class=o>*</span><span class=p>,</span> <span class=n>seed</span><span class=p>:</span> <span class=nb>int</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=mi>42</span><span class=p>,</span> <span class=n>project</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>group</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>experiment</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>device_parameters</span><span class=p>:</span> <span class=n>DeviceParameters</span><span class=p>,</span> <span class=n>training_parameters</span><span class=p>:</span> <span class=n>TrainingParameters</span><span class=p>,</span> <span class=n>callbacks</span><span class=p>:</span> <span class=n>CallbackCollection</span> <span class=o>=</span> <span class=n>cast</span><span class=p>(</span><span class=s1>&#39;CallbackCollection&#39;</span><span class=p>,</span> <span class=kc>None</span><span class=p>))</span>
</span></code></pre></div> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title='<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">AbstractTrainerConfig</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span> (<code>dream_trainer.trainer.abstract.AbstractTrainerConfig</code>)' href=../abstract/#dream_trainer.trainer.AbstractTrainerConfig>AbstractTrainerConfig</a></code></p> <p>Configuration class for BaseTrainer.</p> <p>This dataclass holds all configuration parameters needed to initialize and run a BaseTrainer instance.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <table> <thead> <tr> <th><span class=doc-section-title>ATTRIBUTE</span></th> <th><span>DESCRIPTION</span></th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code><span title=dream_trainer.trainer.BaseTrainerConfig.training_parameters>training_parameters</span></code></td> <td class=doc-attribute-details> <div class=doc-md-description> <p>Configuration for training hyperparameters including epochs, batch size, gradient accumulation, validation frequency, etc.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-attribute-annotation> <b>TYPE:</b> <code><a class="autorefs autorefs-internal" title='<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">TrainingParameters</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span> (<code>dream_trainer.configs.trainer.TrainingParameters</code>)' href=../../configuration/parameters/#dream_trainer.configs.TrainingParameters>TrainingParameters</a></code> </span> </p> </td> </tr> <tr class=doc-section-item> <td><code><span title=dream_trainer.trainer.BaseTrainerConfig.callbacks>callbacks</span></code></td> <td class=doc-attribute-details> <div class=doc-md-description> <p>Collection of callbacks to execute during training lifecycle. If None, an empty CallbackCollection will be created.</p> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> </div> <p> <span class=doc-attribute-annotation> <b>TYPE:</b> <code><a class="autorefs autorefs-internal" title='<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">CallbackCollection</span> (<code>dream_trainer.callbacks.CallbackCollection</code>)' href=../../callbacks/base/#dream_trainer.callbacks.CallbackCollection>CallbackCollection</a></code> </span> </p> </td> </tr> </tbody> </table> <div class="doc doc-children"> </div> </div> </div><h2 id=usage-example>Usage Example<a class=headerlink href=#usage-example title="Permanent link">&para;</a></h2> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.trainer</span><span class=w> </span><span class=kn>import</span> <span class=n>BaseTrainer</span><span class=p>,</span> <span class=n>BaseTrainerConfig</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.configs</span><span class=w> </span><span class=kn>import</span> <span class=n>TrainingParameters</span><span class=p>,</span> <span class=n>DeviceParameters</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=kn>from</span><span class=w> </span><span class=nn>dream_trainer.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>CallbackCollection</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=k>class</span><span class=w> </span><span class=nc>MyTrainer</span><span class=p>(</span><span class=n>BaseTrainer</span><span class=p>):</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>    <span class=k>def</span><span class=w> </span><span class=nf>configure</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>        <span class=c1># Define models</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>        <span class=c1># Setup dataloaders</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_val_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>val_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>    <span class=k>def</span><span class=w> </span><span class=nf>setup</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>        <span class=c1># Move model to device</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>world</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a>    <span class=k>def</span><span class=w> </span><span class=nf>named_models</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;main&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>}</span>
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a>    <span class=k>def</span><span class=w> </span><span class=nf>named_optimizers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;adam&quot;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>}</span>
</span><span id=__span-0-24><a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a>
</span><span id=__span-0-25><a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a>    <span class=k>def</span><span class=w> </span><span class=nf>named_schedulers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-26><a id=__codelineno-0-26 name=__codelineno-0-26 href=#__codelineno-0-26></a>        <span class=k>return</span> <span class=kc>None</span>
</span><span id=__span-0-27><a id=__codelineno-0-27 name=__codelineno-0-27 href=#__codelineno-0-27></a>
</span><span id=__span-0-28><a id=__codelineno-0-28 name=__codelineno-0-28 href=#__codelineno-0-28></a>    <span class=k>def</span><span class=w> </span><span class=nf>get_module</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>fqn</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span><span id=__span-0-29><a id=__codelineno-0-29 name=__codelineno-0-29 href=#__codelineno-0-29></a>        <span class=k>return</span> <span class=nb>getattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=n>fqn</span><span class=p>)</span>
</span><span id=__span-0-30><a id=__codelineno-0-30 name=__codelineno-0-30 href=#__codelineno-0-30></a>
</span><span id=__span-0-31><a id=__codelineno-0-31 name=__codelineno-0-31 href=#__codelineno-0-31></a>    <span class=nd>@property</span>
</span><span id=__span-0-32><a id=__codelineno-0-32 name=__codelineno-0-32 href=#__codelineno-0-32></a>    <span class=k>def</span><span class=w> </span><span class=nf>train_dataloader</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-33><a id=__codelineno-0-33 name=__codelineno-0-33 href=#__codelineno-0-33></a>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_train_dataloader</span>
</span><span id=__span-0-34><a id=__codelineno-0-34 name=__codelineno-0-34 href=#__codelineno-0-34></a>
</span><span id=__span-0-35><a id=__codelineno-0-35 name=__codelineno-0-35 href=#__codelineno-0-35></a>    <span class=nd>@property</span>
</span><span id=__span-0-36><a id=__codelineno-0-36 name=__codelineno-0-36 href=#__codelineno-0-36></a>    <span class=k>def</span><span class=w> </span><span class=nf>val_dataloader</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-37><a id=__codelineno-0-37 name=__codelineno-0-37 href=#__codelineno-0-37></a>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_val_dataloader</span>
</span><span id=__span-0-38><a id=__codelineno-0-38 name=__codelineno-0-38 href=#__codelineno-0-38></a>
</span><span id=__span-0-39><a id=__codelineno-0-39 name=__codelineno-0-39 href=#__codelineno-0-39></a>    <span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-0-40><a id=__codelineno-0-40 name=__codelineno-0-40 href=#__codelineno-0-40></a>        <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input&quot;</span><span class=p>],</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;target&quot;</span><span class=p>]</span>
</span><span id=__span-0-41><a id=__codelineno-0-41 name=__codelineno-0-41 href=#__codelineno-0-41></a>        <span class=n>y_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-42><a id=__codelineno-0-42 name=__codelineno-0-42 href=#__codelineno-0-42></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>y_hat</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span><span id=__span-0-43><a id=__codelineno-0-43 name=__codelineno-0-43 href=#__codelineno-0-43></a>
</span><span id=__span-0-44><a id=__codelineno-0-44 name=__codelineno-0-44 href=#__codelineno-0-44></a>        <span class=c1># Use no_gradient_sync for gradient accumulation</span>
</span><span id=__span-0-45><a id=__codelineno-0-45 name=__codelineno-0-45 href=#__codelineno-0-45></a>        <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_gradient_sync</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-0-46><a id=__codelineno-0-46 name=__codelineno-0-46 href=#__codelineno-0-46></a>            <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-0-47><a id=__codelineno-0-47 name=__codelineno-0-47 href=#__codelineno-0-47></a>
</span><span id=__span-0-48><a id=__codelineno-0-48 name=__codelineno-0-48 href=#__codelineno-0-48></a>        <span class=c1># Step optimizer when not accumulating</span>
</span><span id=__span-0-49><a id=__codelineno-0-49 name=__codelineno-0-49 href=#__codelineno-0-49></a>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-0-50><a id=__codelineno-0-50 name=__codelineno-0-50 href=#__codelineno-0-50></a>            <span class=bp>self</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>)</span>
</span><span id=__span-0-51><a id=__codelineno-0-51 name=__codelineno-0-51 href=#__codelineno-0-51></a>
</span><span id=__span-0-52><a id=__codelineno-0-52 name=__codelineno-0-52 href=#__codelineno-0-52></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span><span id=__span-0-53><a id=__codelineno-0-53 name=__codelineno-0-53 href=#__codelineno-0-53></a>
</span><span id=__span-0-54><a id=__codelineno-0-54 name=__codelineno-0-54 href=#__codelineno-0-54></a>    <span class=k>def</span><span class=w> </span><span class=nf>validation_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-0-55><a id=__codelineno-0-55 name=__codelineno-0-55 href=#__codelineno-0-55></a>        <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input&quot;</span><span class=p>],</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;target&quot;</span><span class=p>]</span>
</span><span id=__span-0-56><a id=__codelineno-0-56 name=__codelineno-0-56 href=#__codelineno-0-56></a>        <span class=n>y_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-57><a id=__codelineno-0-57 name=__codelineno-0-57 href=#__codelineno-0-57></a>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>y_hat</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span><span id=__span-0-58><a id=__codelineno-0-58 name=__codelineno-0-58 href=#__codelineno-0-58></a>        <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;val_loss&quot;</span><span class=p>:</span> <span class=n>loss</span><span class=p>}</span>
</span><span id=__span-0-59><a id=__codelineno-0-59 name=__codelineno-0-59 href=#__codelineno-0-59></a>
</span><span id=__span-0-60><a id=__codelineno-0-60 name=__codelineno-0-60 href=#__codelineno-0-60></a><span class=c1># Create and run trainer</span>
</span><span id=__span-0-61><a id=__codelineno-0-61 name=__codelineno-0-61 href=#__codelineno-0-61></a><span class=n>config</span> <span class=o>=</span> <span class=n>BaseTrainerConfig</span><span class=p>(</span>
</span><span id=__span-0-62><a id=__codelineno-0-62 name=__codelineno-0-62 href=#__codelineno-0-62></a>    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
</span><span id=__span-0-63><a id=__codelineno-0-63 name=__codelineno-0-63 href=#__codelineno-0-63></a>    <span class=n>project</span><span class=o>=</span><span class=s2>&quot;my_project&quot;</span><span class=p>,</span>
</span><span id=__span-0-64><a id=__codelineno-0-64 name=__codelineno-0-64 href=#__codelineno-0-64></a>    <span class=n>group</span><span class=o>=</span><span class=s2>&quot;experiments&quot;</span><span class=p>,</span>
</span><span id=__span-0-65><a id=__codelineno-0-65 name=__codelineno-0-65 href=#__codelineno-0-65></a>    <span class=n>experiment</span><span class=o>=</span><span class=s2>&quot;baseline&quot;</span><span class=p>,</span>
</span><span id=__span-0-66><a id=__codelineno-0-66 name=__codelineno-0-66 href=#__codelineno-0-66></a>    <span class=n>device_parameters</span><span class=o>=</span><span class=n>DeviceParameters</span><span class=p>(),</span>
</span><span id=__span-0-67><a id=__codelineno-0-67 name=__codelineno-0-67 href=#__codelineno-0-67></a>    <span class=n>training_parameters</span><span class=o>=</span><span class=n>TrainingParameters</span><span class=p>(</span>
</span><span id=__span-0-68><a id=__codelineno-0-68 name=__codelineno-0-68 href=#__codelineno-0-68></a>        <span class=n>num_epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span><span id=__span-0-69><a id=__codelineno-0-69 name=__codelineno-0-69 href=#__codelineno-0-69></a>        <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span><span id=__span-0-70><a id=__codelineno-0-70 name=__codelineno-0-70 href=#__codelineno-0-70></a>        <span class=n>gradient_clip_val</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>
</span><span id=__span-0-71><a id=__codelineno-0-71 name=__codelineno-0-71 href=#__codelineno-0-71></a>        <span class=n>val_check_interval</span><span class=o>=</span><span class=mi>100</span>
</span><span id=__span-0-72><a id=__codelineno-0-72 name=__codelineno-0-72 href=#__codelineno-0-72></a>    <span class=p>),</span>
</span><span id=__span-0-73><a id=__codelineno-0-73 name=__codelineno-0-73 href=#__codelineno-0-73></a>    <span class=n>callbacks</span><span class=o>=</span><span class=n>CallbackCollection</span><span class=p>()</span>
</span><span id=__span-0-74><a id=__codelineno-0-74 name=__codelineno-0-74 href=#__codelineno-0-74></a><span class=p>)</span>
</span><span id=__span-0-75><a id=__codelineno-0-75 name=__codelineno-0-75 href=#__codelineno-0-75></a>
</span><span id=__span-0-76><a id=__codelineno-0-76 name=__codelineno-0-76 href=#__codelineno-0-76></a><span class=n>trainer</span> <span class=o>=</span> <span class=n>MyTrainer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-0-77><a id=__codelineno-0-77 name=__codelineno-0-77 href=#__codelineno-0-77></a><span class=n>trainer</span><span class=o>.</span><span class=n>configure</span><span class=p>()</span>
</span><span id=__span-0-78><a id=__codelineno-0-78 name=__codelineno-0-78 href=#__codelineno-0-78></a><span class=n>trainer</span><span class=o>.</span><span class=n>setup</span><span class=p>()</span>
</span><span id=__span-0-79><a id=__codelineno-0-79 name=__codelineno-0-79 href=#__codelineno-0-79></a><span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span></code></pre></div> <h2 id=key-features>Key Features<a class=headerlink href=#key-features title="Permanent link">&para;</a></h2> <h3 id=gradient-accumulation>Gradient Accumulation<a class=headerlink href=#gradient-accumulation title="Permanent link">&para;</a></h3> <p>BaseTrainer handles gradient accumulation automatically:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># Configure in TrainingParameters</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>training_params</span> <span class=o>=</span> <span class=n>TrainingParameters</span><span class=p>(</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># Accumulate over 4 batches</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>    <span class=n>gradient_clip_val</span><span class=o>=</span><span class=mf>1.0</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=p>)</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=c1># In training_step, use no_gradient_sync</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>    <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>    <span class=c1># Automatically handles gradient sync</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>    <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_gradient_sync</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a>        <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>    <span class=c1># Only step when not accumulating</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a>    <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>)</span>
</span></code></pre></div> <h3 id=mixed-precision-training>Mixed Precision Training<a class=headerlink href=#mixed-precision-training title="Permanent link">&para;</a></h3> <p>BaseTrainer integrates with PyTorch's autocast:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=k>def</span><span class=w> </span><span class=nf>training_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>):</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>    <span class=c1># Forward pass is automatically in autocast context</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>    <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&quot;input&quot;</span><span class=p>])</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>    <span class=c1># Loss computation with loss parallelism</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>    <span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_parallel</span><span class=p>():</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&quot;target&quot;</span><span class=p>])</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>    <span class=c1># Backward automatically handles mixed precision</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>    <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></code></pre></div> <h3 id=validation>Validation<a class=headerlink href=#validation title="Permanent link">&para;</a></h3> <p>Validation runs automatically based on configuration:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=n>training_params</span> <span class=o>=</span> <span class=n>TrainingParameters</span><span class=p>(</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    <span class=n>val_check_interval</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>  <span class=c1># Validate every 100 steps</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=n>limit_val_batches</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>    <span class=c1># Use only 50 validation batches</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>    <span class=n>num_sanity_val_steps</span><span class=o>=</span><span class=mi>2</span>   <span class=c1># Run 2 sanity check steps</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=p>)</span>
</span></code></pre></div> <h3 id=callbacks>Callbacks<a class=headerlink href=#callbacks title="Permanent link">&para;</a></h3> <p>BaseTrainer provides extensive callback hooks:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Callbacks are called at these points:</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=c1># - pre/post_train_epoch</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># - pre/post_train_step</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=c1># - pre/post_validation_epoch</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=c1># - pre/post_validation_step</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=c1># - pre/post_optimizer_step</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a><span class=c1># - pre/post_optimizer_zero_grad</span>
</span></code></pre></div> <h2 id=advanced-usage>Advanced Usage<a class=headerlink href=#advanced-usage title="Permanent link">&para;</a></h2> <h3 id=custom-gradient-clipping>Custom Gradient Clipping<a class=headerlink href=#custom-gradient-clipping title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=k>def</span><span class=w> </span><span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>):</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>    <span class=c1># Compute gradient norm</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    <span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>]</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>    <span class=n>total_norm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>total_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>)</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>    <span class=c1># Custom clipping logic</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>    <span class=k>if</span> <span class=n>total_norm</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_parameters</span><span class=o>.</span><span class=n>gradient_clip_val</span><span class=p>:</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>clip_gradient_norm</span><span class=p>(</span><span class=n>parameters</span><span class=p>,</span> <span class=n>total_norm</span><span class=p>)</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>    <span class=c1># Continue with optimizer step</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></code></pre></div> <h3 id=distributed-training>Distributed Training<a class=headerlink href=#distributed-training title="Permanent link">&para;</a></h3> <p>BaseTrainer automatically handles distributed synchronization:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># No gradient sync during accumulation</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=k>with</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_gradient_sync</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>):</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    <span class=bp>self</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a><span class=c1># Automatic gradient sync on last accumulation step</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_accumulating_gradients</span><span class=p>:</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>    <span class=c1># Gradients are synchronized here</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>    <span class=bp>self</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=p>)</span>
</span></code></pre></div> <h2 id=see-also>See Also<a class=headerlink href=#see-also title="Permanent link">&para;</a></h2> <ul> <li><a href=../abstract/ >AbstractTrainer</a> - Base interface</li> <li><a href=../dream/ >DreamTrainer</a> - Full-featured trainer</li> <li><a href=../../configuration/training/ >TrainingParameters</a> - Training configuration</li> <li><a href=../../callbacks/base/ >CallbackCollection</a> - Callback system </li> </ul> <p>*[API]: Application Programming Interface *[CPU]: Central Processing Unit *[GPU]: Graphics Processing Unit *[TPU]: Tensor Processing Unit *[CUDA]: Compute Unified Device Architecture *[PyTorch]: Python-based open source machine learning framework *[DTensor]: Distributed Tensor *[FSDP]: Fully Sharded Data Parallel *[FSDP2]: Fully Sharded Data Parallel version 2 *[DDP]: Distributed Data Parallel *[HSDP]: Hybrid Sharded Data Parallel *[TP]: Tensor Parallel *[PP]: Pipeline Parallel *[NCCL]: NVIDIA Collective Communication Library *[MPI]: Message Passing Interface *[ML]: Machine Learning *[AI]: Artificial Intelligence *[OOM]: Out of Memory *[RDMA]: Remote Direct Memory Access *[JIT]: Just-In-Time compilation *[QoS]: Quality of Service *[NVMe]: Non-Volatile Memory Express *[JSON]: JavaScript Object Notation *[YAML]: YAML Ain't Markup Language *[CSV]: Comma-Separated Values *[SDK]: Software Development Kit *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[SSH]: Secure Shell *[URL]: Uniform Resource Locator *[RAM]: Random Access Memory *[VRAM]: Video Random Access Memory *[FP8]: 8-bit Floating Point *[FP16]: 16-bit Floating Point (Half Precision) *[FP32]: 32-bit Floating Point (Single Precision) *[BF16]: Brain Floating Point 16 *[AMP]: Automatic Mixed Precision *[TF]: TensorFlow *[HF]: HuggingFace *[LLM]: Large Language Model *[NLP]: Natural Language Processing *[CV]: Computer Vision *[RL]: Reinforcement Learning *[GAN]: Generative Adversarial Network *[CNN]: Convolutional Neural Network *[RNN]: Recurrent Neural Network *[LSTM]: Long Short-Term Memory *[GRU]: Gated Recurrent Unit *[SGD]: Stochastic Gradient Descent *[Adam]: Adaptive Moment Estimation optimizer *[AdamW]: Adam with decoupled Weight decay *[LR]: Learning Rate *[BS]: Batch Size *[WD]: Weight Decay *[KV]: Key-Value *[MHA]: Multi-Head Attention *[FFN]: Feed-Forward Network *[MSE]: Mean Squared Error *[MAE]: Mean Absolute Error *[BCE]: Binary Cross-Entropy *[CE]: Cross-Entropy *[BERT]: Bidirectional Encoder Representations from Transformers *[GPT]: Generative Pre-trained Transformer *[CLIP]: Contrastive Language-Image Pre-training *[ONNX]: Open Neural Network Exchange *[TorchScript]: PyTorch's graph representation format *[W&amp;B]: Weights &amp; Biases *[TB]: TensorBoard *[MLOps]: Machine Learning Operations *[CI/CD]: Continuous Integration/Continuous Deployment *[REST]: Representational State Transfer *[gRPC]: Google Remote Procedure Call *[IPC]: Inter-Process Communication *[P2P]: Peer-to-Peer *[AllReduce]: Collective communication operation that combines values from all processes *[AllGather]: Collective communication operation that g </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="July 16, 2025 02:20:00 UTC"><span class=timeago datetime=2025-07-16T02:20:00+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="July 16, 2025 02:20:00 UTC">2025-07-16</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../abstract/ class="md-footer__link md-footer__link--prev" aria-label="Previous: AbstractTrainer"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> AbstractTrainer </div> </div> </a> <a href=../dream/ class="md-footer__link md-footer__link--next" aria-label="Next: DreamTrainer"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> DreamTrainer </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dream3d/dream-trainer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://discord.gg/dream-trainer target=_blank rel=noopener title=discord.gg class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> <a href=https://twitter.com/dream3d_ai target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.prune", "navigation.indexes", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tooltips", "toc.follow", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.56ea9cef.min.js></script> <script src=../../../js/timeago.min.js></script> <script src=../../../js/timeago_mkdocs_material.js></script> <script src=../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>